{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method in RL\n",
    "The cross-entropy method falls into the model-free and policy-based category\n",
    "of methods.\n",
    "\n",
    "The term \"model-free\" means that the method doesn't build a model of the\n",
    "environment or reward.\n",
    "\n",
    "Policy-based methods are directly approximating the policy of the agent, that is, what actions the agent should carry out at every step. Policy is usually represented by probability distribution over the available actions.\n",
    "\n",
    "As our cross-entropy method is policy-based, our nonlinear function (neural network) produces policy, which basically says for every observation which action the agent should take.\n",
    "\n",
    "During the agent's lifetime, its experience is present as *episodes*. Every episode is a sequence of observations that the agent has got from the environment, actions it has issued, and rewards for these actions. Imagine that our agent has played several such episodes. For every episode, we can calculate the total reward that the agent has claimed. This total reward shows how good this episode was for the agent.\n",
    "\n",
    "Due to randomness in the environment and the way that the agent selects actions to take, some episodes will be better than others. The core of the cross-entropy method is to throw away bad\n",
    "episodes and train on better ones.\n",
    "\n",
    "### Algorithm\n",
    "the steps of the method are as follows:\n",
    "\n",
    "1. Play ```N``` number of episodes using our current model and environment.\n",
    "2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/greg/standard_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tf.keras.backend.clear_session() # Reset TF notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole\n",
    "The first example will use the standard Cartpole problem."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> 33e4f5eaf610b4db07f0b6a865191f5677a34714
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparams.\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Net\n",
    "The neural network takes in an observation and returns an action. In this case, we have a discrete number of potential actions, so the network returns a \"probability\" for each action, which a higher probability indicating that the action is more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset() # Reset environment and obtain first observation.\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v)) # Get action probabilities from observation.\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) # Action is an integer.\n",
    "        next_obs, reward, is_done, _ = env.step(action) # Perform an action and move to the next state.\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "        \n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.686, reward_mean=22.2, reward_bound=21.0\n",
      "1: loss=0.652, reward_mean=15.9, reward_bound=15.5\n",
      "2: loss=0.672, reward_mean=18.8, reward_bound=20.0\n",
      "3: loss=0.677, reward_mean=18.2, reward_bound=21.5\n",
      "4: loss=0.658, reward_mean=21.4, reward_bound=31.0\n",
      "5: loss=0.682, reward_mean=19.0, reward_bound=20.0\n",
      "6: loss=0.669, reward_mean=27.4, reward_bound=31.5\n",
      "7: loss=0.666, reward_mean=25.2, reward_bound=29.0\n",
      "8: loss=0.660, reward_mean=26.2, reward_bound=27.5\n",
      "9: loss=0.656, reward_mean=40.9, reward_bound=42.0\n",
      "10: loss=0.647, reward_mean=34.4, reward_bound=35.0\n",
      "11: loss=0.664, reward_mean=33.9, reward_bound=39.0\n",
      "12: loss=0.639, reward_mean=33.1, reward_bound=31.0\n",
      "13: loss=0.639, reward_mean=36.8, reward_bound=41.0\n",
      "14: loss=0.655, reward_mean=40.1, reward_bound=48.5\n",
      "15: loss=0.636, reward_mean=48.6, reward_bound=55.0\n",
      "16: loss=0.612, reward_mean=46.7, reward_bound=55.0\n",
      "17: loss=0.623, reward_mean=51.9, reward_bound=57.0\n",
      "18: loss=0.602, reward_mean=49.5, reward_bound=58.0\n",
      "19: loss=0.621, reward_mean=52.4, reward_bound=60.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-62dc050710bb>\", line 10, in <module>\n",
      "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
      "  File \"<ipython-input-5-3983138dd5b1>\", line 9, in iterate_batches\n",
      "    act_probs_v = sm(net(obs_v))\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-3-8bdaef70c462>\", line 11, in forward\n",
      "    return self.net(x)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 92, in forward\n",
      "    input = module(input)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 547, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 1369, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Greg\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs_size = env.observation_space.shape[0] # Number of possible observations.\n",
    "n_actions = env.action_space.n # Number of possible agent actions.\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    \n",
    "    # Train neural net.\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Solution\n",
<<<<<<< HEAD
    "This has slow training...possibly because TensorFlow has to build a new graph each time we fit a Keras model..."
=======
    "This has slow convergence...possibly because TensorFlow has to build a new graph each time we fit a Keras model..."
>>>>>>> 33e4f5eaf610b4db07f0b6a865191f5677a34714
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_mlp():\n",
    "    \n",
    "    inputs = keras.Input(shape=(obs_size,))\n",
    "    dense = layers.Dense(256, activation='relu')(inputs)\n",
    "    outputs = layers.Dense(n_actions, activation='softmax')(dense)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches_keras(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset() # Reset environment and obtain first observation.\n",
    "    while True:\n",
    "        obs_v = np.array([obs])\n",
    "        act_probs = net.predict(obs_v).reshape(-1)\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action) # Perform an action and move to the next state.\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "        \n",
    "def filter_batch_keras(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    return train_obs, train_act, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs_size = env.observation_space.shape[0] # Number of possible observations.\n",
    "n_actions = env.action_space.n # Number of possible agent actions.\n",
    "net = keras_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: reward_mean=19.1, reward_bound=19.5\n",
      "1: reward_mean=22.4, reward_bound=22.5\n",
      "2: reward_mean=30.4, reward_bound=40.0\n",
      "3: reward_mean=39.6, reward_bound=48.5\n",
      "4: reward_mean=36.3, reward_bound=33.5\n",
      "5: reward_mean=31.1, reward_bound=40.0\n",
      "6: reward_mean=35.4, reward_bound=41.0\n",
      "7: reward_mean=35.9, reward_bound=52.0\n",
      "8: reward_mean=43.6, reward_bound=47.5\n",
      "9: reward_mean=49.9, reward_bound=56.5\n",
      "10: reward_mean=53.0, reward_bound=66.0\n",
      "11: reward_mean=50.3, reward_bound=65.5\n",
      "12: reward_mean=65.3, reward_bound=80.5\n",
      "13: reward_mean=93.8, reward_bound=111.0\n",
      "14: reward_mean=74.8, reward_bound=77.5\n",
      "15: reward_mean=76.2, reward_bound=87.0\n",
      "16: reward_mean=111.7, reward_bound=123.5\n",
      "17: reward_mean=128.3, reward_bound=156.5\n",
      "18: reward_mean=97.5, reward_bound=127.0\n",
      "19: reward_mean=136.5, reward_bound=152.5\n",
      "20: reward_mean=90.6, reward_bound=125.0\n",
      "21: reward_mean=131.9, reward_bound=165.5\n",
      "22: reward_mean=159.6, reward_bound=200.0\n",
      "23: reward_mean=151.1, reward_bound=200.0\n"
     ]
    }
   ],
   "source": [
    "for iter_no, batch in enumerate(iterate_batches_keras(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch_keras(batch, PERCENTILE)\n",
    "    \n",
    "    obs_v, acts_v_binary = np.array(obs_v), to_categorical(acts_v)\n",
    "    net.fit(obs_v, acts_v_binary, batch_size = BATCH_SIZE, verbose = 0)\n",
    "    \n",
    "    print(\"%d: reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "        iter_no, reward_m, reward_b))\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "We now move onto Frozen Lake, a more difficult problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "        obs = next_obs\n",
    "        \n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "        \n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.384, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=1.385, reward_mean=0.0, reward_bound=0.0\n",
      "2: loss=1.378, reward_mean=0.0, reward_bound=0.0\n",
      "3: loss=1.378, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=1.368, reward_mean=0.0, reward_bound=0.0\n",
      "5: loss=1.374, reward_mean=0.0, reward_bound=0.0\n",
      "6: loss=1.364, reward_mean=0.0, reward_bound=0.0\n",
      "7: loss=1.366, reward_mean=0.0, reward_bound=0.0\n",
      "8: loss=1.351, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=1.354, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=1.370, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=1.345, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=1.333, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=1.328, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=1.326, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=1.306, reward_mean=0.0, reward_bound=0.0\n",
      "16: loss=1.280, reward_mean=0.1, reward_bound=0.0\n",
      "17: loss=1.271, reward_mean=0.0, reward_bound=0.0\n",
      "18: loss=1.270, reward_mean=0.0, reward_bound=0.0\n",
      "19: loss=1.254, reward_mean=0.0, reward_bound=0.0\n",
      "20: loss=1.210, reward_mean=0.0, reward_bound=0.0\n",
      "21: loss=1.235, reward_mean=0.0, reward_bound=0.0\n",
      "22: loss=1.211, reward_mean=0.0, reward_bound=0.0\n",
      "23: loss=1.214, reward_mean=0.0, reward_bound=0.0\n",
      "24: loss=1.184, reward_mean=0.0, reward_bound=0.0\n",
      "25: loss=1.188, reward_mean=0.0, reward_bound=0.0\n",
      "26: loss=1.199, reward_mean=0.0, reward_bound=0.0\n",
      "27: loss=1.202, reward_mean=0.0, reward_bound=0.0\n",
      "28: loss=1.181, reward_mean=0.0, reward_bound=0.0\n",
      "29: loss=1.176, reward_mean=0.0, reward_bound=0.0\n",
      "30: loss=1.218, reward_mean=0.0, reward_bound=0.0\n",
      "31: loss=1.222, reward_mean=0.0, reward_bound=0.0\n",
      "32: loss=1.232, reward_mean=0.0, reward_bound=0.0\n",
      "33: loss=1.241, reward_mean=0.0, reward_bound=0.0\n",
      "34: loss=1.211, reward_mean=0.0, reward_bound=0.0\n",
      "35: loss=1.223, reward_mean=0.0, reward_bound=0.0\n",
      "36: loss=1.221, reward_mean=0.0, reward_bound=0.0\n",
      "37: loss=1.225, reward_mean=0.0, reward_bound=0.0\n",
      "38: loss=1.239, reward_mean=0.0, reward_bound=0.0\n",
      "39: loss=1.236, reward_mean=0.0, reward_bound=0.0\n",
      "40: loss=1.223, reward_mean=0.0, reward_bound=0.0\n",
      "41: loss=1.231, reward_mean=0.0, reward_bound=0.0\n",
      "42: loss=1.225, reward_mean=0.0, reward_bound=0.0\n",
      "43: loss=1.250, reward_mean=0.0, reward_bound=0.0\n",
      "44: loss=1.227, reward_mean=0.0, reward_bound=0.0\n",
      "45: loss=1.233, reward_mean=0.0, reward_bound=0.0\n",
      "46: loss=1.212, reward_mean=0.0, reward_bound=0.0\n",
      "47: loss=1.215, reward_mean=0.0, reward_bound=0.0\n",
      "48: loss=1.217, reward_mean=0.0, reward_bound=0.0\n",
      "49: loss=1.231, reward_mean=0.0, reward_bound=0.0\n",
      "50: loss=1.224, reward_mean=0.0, reward_bound=0.0\n",
      "51: loss=1.257, reward_mean=0.0, reward_bound=0.0\n",
      "52: loss=1.219, reward_mean=0.0, reward_bound=0.0\n",
      "53: loss=1.227, reward_mean=0.0, reward_bound=0.0\n",
      "54: loss=1.222, reward_mean=0.0, reward_bound=0.0\n",
      "55: loss=1.231, reward_mean=0.0, reward_bound=0.0\n",
      "56: loss=1.254, reward_mean=0.0, reward_bound=0.0\n",
      "57: loss=1.246, reward_mean=0.0, reward_bound=0.0\n",
      "58: loss=1.220, reward_mean=0.0, reward_bound=0.0\n",
      "59: loss=1.252, reward_mean=0.0, reward_bound=0.0\n",
      "60: loss=1.248, reward_mean=0.0, reward_bound=0.0\n",
      "61: loss=1.243, reward_mean=0.0, reward_bound=0.0\n",
      "62: loss=1.258, reward_mean=0.1, reward_bound=0.0\n",
      "63: loss=1.252, reward_mean=0.0, reward_bound=0.0\n",
      "64: loss=1.240, reward_mean=0.0, reward_bound=0.0\n",
      "65: loss=1.243, reward_mean=0.0, reward_bound=0.0\n",
      "66: loss=1.268, reward_mean=0.0, reward_bound=0.0\n",
      "67: loss=1.251, reward_mean=0.0, reward_bound=0.0\n",
      "68: loss=1.261, reward_mean=0.0, reward_bound=0.0\n",
      "69: loss=1.233, reward_mean=0.0, reward_bound=0.0\n",
      "70: loss=1.244, reward_mean=0.0, reward_bound=0.0\n",
      "71: loss=1.252, reward_mean=0.0, reward_bound=0.0\n",
      "72: loss=1.257, reward_mean=0.0, reward_bound=0.0\n",
      "73: loss=1.205, reward_mean=0.0, reward_bound=0.0\n",
      "74: loss=1.223, reward_mean=0.0, reward_bound=0.0\n",
      "75: loss=1.236, reward_mean=0.0, reward_bound=0.0\n",
      "76: loss=1.202, reward_mean=0.0, reward_bound=0.0\n",
      "77: loss=1.232, reward_mean=0.0, reward_bound=0.0\n",
      "78: loss=1.227, reward_mean=0.0, reward_bound=0.0\n",
      "79: loss=1.206, reward_mean=0.0, reward_bound=0.0\n",
      "80: loss=1.235, reward_mean=0.0, reward_bound=0.0\n",
      "81: loss=1.230, reward_mean=0.0, reward_bound=0.0\n",
      "82: loss=1.240, reward_mean=0.0, reward_bound=0.0\n",
      "83: loss=1.238, reward_mean=0.0, reward_bound=0.0\n",
      "84: loss=1.231, reward_mean=0.0, reward_bound=0.0\n",
      "85: loss=1.251, reward_mean=0.0, reward_bound=0.0\n",
      "86: loss=1.244, reward_mean=0.0, reward_bound=0.0\n",
      "87: loss=1.213, reward_mean=0.0, reward_bound=0.0\n",
      "88: loss=1.224, reward_mean=0.0, reward_bound=0.0\n",
      "89: loss=1.261, reward_mean=0.0, reward_bound=0.0\n",
      "90: loss=1.194, reward_mean=0.0, reward_bound=0.0\n",
      "91: loss=1.223, reward_mean=0.0, reward_bound=0.0\n",
      "92: loss=1.214, reward_mean=0.0, reward_bound=0.0\n",
      "93: loss=1.223, reward_mean=0.0, reward_bound=0.0\n",
      "94: loss=1.167, reward_mean=0.0, reward_bound=0.0\n",
      "95: loss=1.192, reward_mean=0.0, reward_bound=0.0\n",
      "96: loss=1.168, reward_mean=0.0, reward_bound=0.0\n",
      "97: loss=1.174, reward_mean=0.0, reward_bound=0.0\n",
      "98: loss=1.154, reward_mean=0.0, reward_bound=0.0\n",
      "99: loss=1.162, reward_mean=0.0, reward_bound=0.0\n",
      "100: loss=1.143, reward_mean=0.0, reward_bound=0.0\n",
      "101: loss=1.151, reward_mean=0.0, reward_bound=0.0\n",
      "102: loss=1.133, reward_mean=0.0, reward_bound=0.0\n",
      "103: loss=1.100, reward_mean=0.0, reward_bound=0.0\n",
      "104: loss=1.159, reward_mean=0.0, reward_bound=0.0\n",
      "105: loss=1.093, reward_mean=0.0, reward_bound=0.0\n",
      "106: loss=1.091, reward_mean=0.0, reward_bound=0.0\n",
      "107: loss=1.115, reward_mean=0.0, reward_bound=0.0\n",
      "108: loss=1.075, reward_mean=0.0, reward_bound=0.0\n",
      "109: loss=1.129, reward_mean=0.0, reward_bound=0.0\n",
      "110: loss=1.089, reward_mean=0.0, reward_bound=0.0\n",
      "111: loss=1.164, reward_mean=0.0, reward_bound=0.0\n",
      "112: loss=1.119, reward_mean=0.0, reward_bound=0.0\n",
      "113: loss=1.115, reward_mean=0.0, reward_bound=0.0\n",
      "114: loss=1.124, reward_mean=0.0, reward_bound=0.0\n",
      "115: loss=1.133, reward_mean=0.0, reward_bound=0.0\n",
      "116: loss=1.098, reward_mean=0.0, reward_bound=0.0\n",
      "117: loss=1.138, reward_mean=0.0, reward_bound=0.0\n",
      "118: loss=1.097, reward_mean=0.0, reward_bound=0.0\n",
      "119: loss=1.091, reward_mean=0.0, reward_bound=0.0\n",
      "120: loss=1.088, reward_mean=0.0, reward_bound=0.0\n",
      "121: loss=1.090, reward_mean=0.0, reward_bound=0.0\n",
      "122: loss=1.074, reward_mean=0.0, reward_bound=0.0\n",
      "123: loss=1.088, reward_mean=0.0, reward_bound=0.0\n",
      "124: loss=1.045, reward_mean=0.0, reward_bound=0.0\n",
      "125: loss=1.073, reward_mean=0.0, reward_bound=0.0\n",
      "126: loss=1.052, reward_mean=0.0, reward_bound=0.0\n",
      "127: loss=1.035, reward_mean=0.0, reward_bound=0.0\n",
      "128: loss=1.060, reward_mean=0.0, reward_bound=0.0\n",
      "129: loss=1.017, reward_mean=0.0, reward_bound=0.0\n",
      "130: loss=1.067, reward_mean=0.0, reward_bound=0.0\n",
      "131: loss=1.023, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-224c20acd1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mobs_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-9a78c6dfa1ef>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/standard_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ba1f26b1d587>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/standard_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/standard_env/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/standard_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    \n",
    "    if reward_m > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above method is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.395, reward_mean=0.020, reward_bound=0.000, batch=2\n",
      "1: loss=1.387, reward_mean=0.000, reward_bound=0.000, batch=2\n",
      "2: loss=1.383, reward_mean=0.030, reward_bound=0.000, batch=5\n",
      "3: loss=1.372, reward_mean=0.010, reward_bound=0.000, batch=6\n",
      "4: loss=1.368, reward_mean=0.040, reward_bound=0.000, batch=10\n",
      "5: loss=1.359, reward_mean=0.030, reward_bound=0.000, batch=13\n",
      "6: loss=1.359, reward_mean=0.020, reward_bound=0.000, batch=15\n",
      "7: loss=1.358, reward_mean=0.030, reward_bound=0.000, batch=18\n",
      "8: loss=1.356, reward_mean=0.030, reward_bound=0.000, batch=21\n",
      "9: loss=1.355, reward_mean=0.020, reward_bound=0.000, batch=23\n",
      "10: loss=1.350, reward_mean=0.050, reward_bound=0.000, batch=28\n",
      "11: loss=1.356, reward_mean=0.030, reward_bound=0.000, batch=31\n",
      "12: loss=1.353, reward_mean=0.010, reward_bound=0.000, batch=32\n",
      "13: loss=1.353, reward_mean=0.020, reward_bound=0.000, batch=34\n",
      "14: loss=1.351, reward_mean=0.010, reward_bound=0.000, batch=35\n",
      "15: loss=1.351, reward_mean=0.020, reward_bound=0.000, batch=37\n",
      "16: loss=1.347, reward_mean=0.020, reward_bound=0.000, batch=39\n",
      "17: loss=1.346, reward_mean=0.010, reward_bound=0.000, batch=40\n",
      "18: loss=1.345, reward_mean=0.000, reward_bound=0.000, batch=40\n",
      "19: loss=1.344, reward_mean=0.000, reward_bound=0.000, batch=40\n",
      "20: loss=1.343, reward_mean=0.000, reward_bound=0.000, batch=40\n",
      "21: loss=1.344, reward_mean=0.040, reward_bound=0.000, batch=44\n",
      "22: loss=1.343, reward_mean=0.030, reward_bound=0.000, batch=47\n",
      "23: loss=1.339, reward_mean=0.030, reward_bound=0.000, batch=50\n",
      "24: loss=1.339, reward_mean=0.030, reward_bound=0.000, batch=53\n",
      "25: loss=1.335, reward_mean=0.030, reward_bound=0.000, batch=56\n",
      "26: loss=1.335, reward_mean=0.010, reward_bound=0.000, batch=57\n",
      "27: loss=1.335, reward_mean=0.020, reward_bound=0.000, batch=59\n",
      "28: loss=1.334, reward_mean=0.050, reward_bound=0.000, batch=64\n",
      "29: loss=1.331, reward_mean=0.020, reward_bound=0.000, batch=66\n",
      "30: loss=1.330, reward_mean=0.010, reward_bound=0.000, batch=67\n",
      "31: loss=1.328, reward_mean=0.030, reward_bound=0.000, batch=70\n",
      "32: loss=1.328, reward_mean=0.000, reward_bound=0.000, batch=70\n",
      "33: loss=1.325, reward_mean=0.030, reward_bound=0.000, batch=73\n",
      "34: loss=1.325, reward_mean=0.000, reward_bound=0.000, batch=73\n",
      "35: loss=1.325, reward_mean=0.020, reward_bound=0.000, batch=75\n",
      "36: loss=1.327, reward_mean=0.010, reward_bound=0.000, batch=76\n",
      "37: loss=1.326, reward_mean=0.000, reward_bound=0.000, batch=76\n",
      "38: loss=1.324, reward_mean=0.030, reward_bound=0.000, batch=79\n",
      "39: loss=1.324, reward_mean=0.030, reward_bound=0.000, batch=82\n",
      "40: loss=1.322, reward_mean=0.020, reward_bound=0.000, batch=84\n",
      "41: loss=1.323, reward_mean=0.020, reward_bound=0.000, batch=86\n",
      "42: loss=1.322, reward_mean=0.010, reward_bound=0.000, batch=87\n",
      "43: loss=1.322, reward_mean=0.010, reward_bound=0.000, batch=88\n",
      "44: loss=1.319, reward_mean=0.020, reward_bound=0.000, batch=90\n",
      "45: loss=1.318, reward_mean=0.010, reward_bound=0.000, batch=91\n",
      "46: loss=1.316, reward_mean=0.010, reward_bound=0.000, batch=92\n",
      "47: loss=1.318, reward_mean=0.040, reward_bound=0.000, batch=96\n",
      "48: loss=1.317, reward_mean=0.010, reward_bound=0.000, batch=97\n",
      "49: loss=1.313, reward_mean=0.040, reward_bound=0.000, batch=101\n",
      "50: loss=1.313, reward_mean=0.010, reward_bound=0.000, batch=102\n",
      "51: loss=1.313, reward_mean=0.020, reward_bound=0.000, batch=104\n",
      "52: loss=1.309, reward_mean=0.030, reward_bound=0.000, batch=107\n",
      "53: loss=1.306, reward_mean=0.030, reward_bound=0.000, batch=110\n",
      "54: loss=1.305, reward_mean=0.040, reward_bound=0.000, batch=114\n",
      "55: loss=1.305, reward_mean=0.020, reward_bound=0.000, batch=116\n",
      "56: loss=1.303, reward_mean=0.020, reward_bound=0.000, batch=118\n",
      "57: loss=1.302, reward_mean=0.030, reward_bound=0.000, batch=121\n",
      "58: loss=1.304, reward_mean=0.020, reward_bound=0.000, batch=123\n",
      "59: loss=1.303, reward_mean=0.050, reward_bound=0.000, batch=128\n",
      "60: loss=1.303, reward_mean=0.020, reward_bound=0.000, batch=130\n",
      "61: loss=1.303, reward_mean=0.020, reward_bound=0.000, batch=132\n",
      "62: loss=1.302, reward_mean=0.020, reward_bound=0.000, batch=134\n",
      "63: loss=1.303, reward_mean=0.050, reward_bound=0.000, batch=139\n",
      "64: loss=1.305, reward_mean=0.050, reward_bound=0.000, batch=144\n",
      "65: loss=1.304, reward_mean=0.010, reward_bound=0.000, batch=145\n",
      "66: loss=1.303, reward_mean=0.050, reward_bound=0.000, batch=150\n",
      "67: loss=1.302, reward_mean=0.010, reward_bound=0.000, batch=151\n",
      "68: loss=1.301, reward_mean=0.030, reward_bound=0.000, batch=154\n",
      "69: loss=1.299, reward_mean=0.040, reward_bound=0.000, batch=158\n",
      "70: loss=1.297, reward_mean=0.040, reward_bound=0.000, batch=162\n",
      "71: loss=1.297, reward_mean=0.000, reward_bound=0.000, batch=162\n",
      "72: loss=1.296, reward_mean=0.020, reward_bound=0.000, batch=164\n",
      "73: loss=1.294, reward_mean=0.040, reward_bound=0.000, batch=168\n",
      "74: loss=1.292, reward_mean=0.040, reward_bound=0.000, batch=172\n",
      "75: loss=1.291, reward_mean=0.020, reward_bound=0.000, batch=174\n",
      "76: loss=1.290, reward_mean=0.000, reward_bound=0.000, batch=174\n",
      "77: loss=1.290, reward_mean=0.010, reward_bound=0.000, batch=175\n",
      "78: loss=1.288, reward_mean=0.030, reward_bound=0.000, batch=178\n",
      "79: loss=1.287, reward_mean=0.020, reward_bound=0.000, batch=180\n",
      "80: loss=1.287, reward_mean=0.000, reward_bound=0.000, batch=180\n",
      "81: loss=1.285, reward_mean=0.060, reward_bound=0.000, batch=186\n",
      "82: loss=1.284, reward_mean=0.020, reward_bound=0.000, batch=188\n",
      "83: loss=1.284, reward_mean=0.030, reward_bound=0.000, batch=191\n",
      "84: loss=1.282, reward_mean=0.030, reward_bound=0.000, batch=194\n",
      "85: loss=1.283, reward_mean=0.030, reward_bound=0.000, batch=197\n",
      "86: loss=1.283, reward_mean=0.010, reward_bound=0.000, batch=198\n",
      "87: loss=1.283, reward_mean=0.000, reward_bound=0.000, batch=198\n",
      "88: loss=1.282, reward_mean=0.060, reward_bound=0.000, batch=204\n",
      "89: loss=1.282, reward_mean=0.030, reward_bound=0.000, batch=207\n",
      "90: loss=1.281, reward_mean=0.010, reward_bound=0.000, batch=208\n",
      "91: loss=1.280, reward_mean=0.030, reward_bound=0.000, batch=211\n",
      "92: loss=1.280, reward_mean=0.050, reward_bound=0.000, batch=216\n",
      "93: loss=1.279, reward_mean=0.050, reward_bound=0.007, batch=221\n",
      "94: loss=1.278, reward_mean=0.010, reward_bound=0.000, batch=222\n",
      "95: loss=1.277, reward_mean=0.050, reward_bound=0.033, batch=225\n",
      "96: loss=1.272, reward_mean=0.050, reward_bound=0.066, batch=227\n",
      "97: loss=1.270, reward_mean=0.030, reward_bound=0.085, batch=229\n",
      "98: loss=1.269, reward_mean=0.020, reward_bound=0.093, batch=230\n",
      "99: loss=1.267, reward_mean=0.040, reward_bound=0.098, batch=230\n",
      "100: loss=1.266, reward_mean=0.020, reward_bound=0.109, batch=229\n",
      "101: loss=1.262, reward_mean=0.020, reward_bound=0.122, batch=225\n",
      "102: loss=1.262, reward_mean=0.020, reward_bound=0.027, batch=227\n",
      "103: loss=1.261, reward_mean=0.040, reward_bound=0.147, batch=229\n",
      "104: loss=1.259, reward_mean=0.020, reward_bound=0.103, batch=230\n",
      "105: loss=1.259, reward_mean=0.010, reward_bound=0.105, batch=231\n",
      "106: loss=1.252, reward_mean=0.050, reward_bound=0.150, batch=228\n",
      "107: loss=1.249, reward_mean=0.030, reward_bound=0.167, batch=222\n",
      "108: loss=1.248, reward_mean=0.030, reward_bound=0.041, batch=225\n",
      "109: loss=1.247, reward_mean=0.030, reward_bound=0.098, batch=227\n",
      "110: loss=1.241, reward_mean=0.060, reward_bound=0.185, batch=222\n",
      "111: loss=1.241, reward_mean=0.050, reward_bound=0.206, batch=227\n",
      "112: loss=1.240, reward_mean=0.000, reward_bound=0.000, batch=227\n",
      "113: loss=1.239, reward_mean=0.040, reward_bound=0.202, batch=229\n",
      "114: loss=1.233, reward_mean=0.040, reward_bound=0.206, batch=221\n",
      "115: loss=1.232, reward_mean=0.020, reward_bound=0.000, batch=223\n",
      "116: loss=1.231, reward_mean=0.070, reward_bound=0.198, batch=226\n",
      "117: loss=1.230, reward_mean=0.020, reward_bound=0.103, batch=228\n",
      "118: loss=1.215, reward_mean=0.050, reward_bound=0.229, batch=214\n",
      "119: loss=1.214, reward_mean=0.020, reward_bound=0.000, batch=216\n",
      "120: loss=1.212, reward_mean=0.060, reward_bound=0.122, batch=220\n",
      "121: loss=1.211, reward_mean=0.010, reward_bound=0.000, batch=221\n",
      "122: loss=1.211, reward_mean=0.030, reward_bound=0.000, batch=224\n",
      "123: loss=1.208, reward_mean=0.060, reward_bound=0.247, batch=227\n",
      "124: loss=1.207, reward_mean=0.030, reward_bound=0.245, batch=229\n",
      "125: loss=1.205, reward_mean=0.030, reward_bound=0.254, batch=211\n",
      "126: loss=1.204, reward_mean=0.040, reward_bound=0.000, batch=215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127: loss=1.202, reward_mean=0.040, reward_bound=0.000, batch=219\n",
      "128: loss=1.201, reward_mean=0.030, reward_bound=0.000, batch=222\n",
      "129: loss=1.200, reward_mean=0.030, reward_bound=0.036, batch=225\n",
      "130: loss=1.199, reward_mean=0.020, reward_bound=0.024, batch=227\n",
      "131: loss=1.197, reward_mean=0.050, reward_bound=0.147, batch=229\n",
      "132: loss=1.197, reward_mean=0.050, reward_bound=0.185, batch=229\n",
      "133: loss=1.197, reward_mean=0.040, reward_bound=0.206, batch=229\n",
      "134: loss=1.196, reward_mean=0.010, reward_bound=0.092, batch=230\n",
      "135: loss=1.195, reward_mean=0.010, reward_bound=0.160, batch=231\n",
      "136: loss=1.196, reward_mean=0.030, reward_bound=0.254, batch=229\n",
      "137: loss=1.181, reward_mean=0.050, reward_bound=0.282, batch=202\n",
      "138: loss=1.183, reward_mean=0.030, reward_bound=0.000, batch=205\n",
      "139: loss=1.181, reward_mean=0.020, reward_bound=0.000, batch=207\n",
      "140: loss=1.181, reward_mean=0.040, reward_bound=0.000, batch=211\n",
      "141: loss=1.177, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "142: loss=1.174, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "143: loss=1.173, reward_mean=0.030, reward_bound=0.000, batch=224\n",
      "144: loss=1.170, reward_mean=0.040, reward_bound=0.185, batch=226\n",
      "145: loss=1.171, reward_mean=0.030, reward_bound=0.124, batch=228\n",
      "146: loss=1.169, reward_mean=0.030, reward_bound=0.229, batch=228\n",
      "147: loss=1.170, reward_mean=0.060, reward_bound=0.282, batch=227\n",
      "148: loss=1.170, reward_mean=0.020, reward_bound=0.251, batch=229\n",
      "149: loss=1.169, reward_mean=0.010, reward_bound=0.126, batch=230\n",
      "150: loss=1.154, reward_mean=0.070, reward_bound=0.314, batch=192\n",
      "151: loss=1.156, reward_mean=0.030, reward_bound=0.000, batch=195\n",
      "152: loss=1.156, reward_mean=0.040, reward_bound=0.000, batch=199\n",
      "153: loss=1.157, reward_mean=0.050, reward_bound=0.000, batch=204\n",
      "154: loss=1.157, reward_mean=0.040, reward_bound=0.000, batch=208\n",
      "155: loss=1.155, reward_mean=0.050, reward_bound=0.000, batch=213\n",
      "156: loss=1.152, reward_mean=0.080, reward_bound=0.125, batch=219\n",
      "157: loss=1.150, reward_mean=0.030, reward_bound=0.000, batch=222\n",
      "158: loss=1.150, reward_mean=0.050, reward_bound=0.135, batch=224\n",
      "159: loss=1.148, reward_mean=0.050, reward_bound=0.147, batch=227\n",
      "160: loss=1.144, reward_mean=0.060, reward_bound=0.206, batch=227\n",
      "161: loss=1.141, reward_mean=0.060, reward_bound=0.249, batch=229\n",
      "162: loss=1.140, reward_mean=0.050, reward_bound=0.282, batch=226\n",
      "163: loss=1.139, reward_mean=0.020, reward_bound=0.157, batch=228\n",
      "164: loss=1.140, reward_mean=0.050, reward_bound=0.314, batch=223\n",
      "165: loss=1.138, reward_mean=0.080, reward_bound=0.220, batch=226\n",
      "166: loss=1.138, reward_mean=0.040, reward_bound=0.229, batch=227\n",
      "167: loss=1.138, reward_mean=0.000, reward_bound=0.000, batch=227\n",
      "168: loss=1.136, reward_mean=0.070, reward_bound=0.342, batch=229\n",
      "169: loss=1.136, reward_mean=0.010, reward_bound=0.044, batch=230\n",
      "170: loss=1.133, reward_mean=0.050, reward_bound=0.349, batch=190\n",
      "171: loss=1.135, reward_mean=0.050, reward_bound=0.000, batch=195\n",
      "172: loss=1.137, reward_mean=0.020, reward_bound=0.000, batch=197\n",
      "173: loss=1.131, reward_mean=0.060, reward_bound=0.000, batch=203\n",
      "174: loss=1.133, reward_mean=0.040, reward_bound=0.000, batch=207\n",
      "175: loss=1.130, reward_mean=0.040, reward_bound=0.000, batch=211\n",
      "176: loss=1.125, reward_mean=0.080, reward_bound=0.109, batch=216\n",
      "177: loss=1.125, reward_mean=0.010, reward_bound=0.000, batch=217\n",
      "178: loss=1.128, reward_mean=0.050, reward_bound=0.057, batch=222\n",
      "179: loss=1.124, reward_mean=0.060, reward_bound=0.185, batch=223\n",
      "180: loss=1.126, reward_mean=0.070, reward_bound=0.206, batch=223\n",
      "181: loss=1.125, reward_mean=0.060, reward_bound=0.219, batch=226\n",
      "182: loss=1.125, reward_mean=0.070, reward_bound=0.254, batch=227\n",
      "183: loss=1.125, reward_mean=0.040, reward_bound=0.282, batch=227\n",
      "184: loss=1.123, reward_mean=0.040, reward_bound=0.237, batch=229\n",
      "185: loss=1.125, reward_mean=0.030, reward_bound=0.254, batch=229\n",
      "186: loss=1.127, reward_mean=0.030, reward_bound=0.314, batch=223\n",
      "187: loss=1.128, reward_mean=0.070, reward_bound=0.271, batch=226\n",
      "188: loss=1.126, reward_mean=0.050, reward_bound=0.282, batch=227\n",
      "189: loss=1.126, reward_mean=0.020, reward_bound=0.097, batch=229\n",
      "190: loss=1.123, reward_mean=0.050, reward_bound=0.314, batch=229\n",
      "191: loss=1.122, reward_mean=0.010, reward_bound=0.102, batch=230\n",
      "192: loss=1.122, reward_mean=0.050, reward_bound=0.274, batch=231\n",
      "193: loss=1.123, reward_mean=0.040, reward_bound=0.314, batch=231\n",
      "194: loss=1.125, reward_mean=0.050, reward_bound=0.349, batch=219\n",
      "195: loss=1.125, reward_mean=0.030, reward_bound=0.000, batch=222\n",
      "196: loss=1.125, reward_mean=0.060, reward_bound=0.174, batch=225\n",
      "197: loss=1.123, reward_mean=0.020, reward_bound=0.046, batch=227\n",
      "198: loss=1.123, reward_mean=0.080, reward_bound=0.349, batch=227\n",
      "199: loss=1.122, reward_mean=0.060, reward_bound=0.342, batch=229\n",
      "200: loss=1.123, reward_mean=0.040, reward_bound=0.364, batch=230\n",
      "201: loss=1.109, reward_mean=0.030, reward_bound=0.387, batch=167\n",
      "202: loss=1.119, reward_mean=0.070, reward_bound=0.000, batch=174\n",
      "203: loss=1.115, reward_mean=0.080, reward_bound=0.000, batch=182\n",
      "204: loss=1.113, reward_mean=0.030, reward_bound=0.000, batch=185\n",
      "205: loss=1.114, reward_mean=0.060, reward_bound=0.000, batch=191\n",
      "206: loss=1.112, reward_mean=0.070, reward_bound=0.000, batch=198\n",
      "207: loss=1.111, reward_mean=0.010, reward_bound=0.000, batch=199\n",
      "208: loss=1.110, reward_mean=0.050, reward_bound=0.000, batch=204\n",
      "209: loss=1.109, reward_mean=0.040, reward_bound=0.000, batch=208\n",
      "210: loss=1.105, reward_mean=0.080, reward_bound=0.081, batch=215\n",
      "211: loss=1.106, reward_mean=0.060, reward_bound=0.089, batch=221\n",
      "212: loss=1.103, reward_mean=0.100, reward_bound=0.122, batch=224\n",
      "213: loss=1.100, reward_mean=0.070, reward_bound=0.185, batch=222\n",
      "214: loss=1.101, reward_mean=0.020, reward_bound=0.000, batch=224\n",
      "215: loss=1.099, reward_mean=0.050, reward_bound=0.206, batch=223\n",
      "216: loss=1.095, reward_mean=0.100, reward_bound=0.254, batch=224\n",
      "217: loss=1.094, reward_mean=0.030, reward_bound=0.254, batch=227\n",
      "218: loss=1.093, reward_mean=0.080, reward_bound=0.308, batch=229\n",
      "219: loss=1.095, reward_mean=0.090, reward_bound=0.314, batch=226\n",
      "220: loss=1.090, reward_mean=0.070, reward_bound=0.349, batch=209\n",
      "221: loss=1.088, reward_mean=0.100, reward_bound=0.254, batch=215\n",
      "222: loss=1.089, reward_mean=0.040, reward_bound=0.000, batch=219\n",
      "223: loss=1.091, reward_mean=0.050, reward_bound=0.141, batch=223\n",
      "224: loss=1.090, reward_mean=0.070, reward_bound=0.229, batch=223\n",
      "225: loss=1.084, reward_mean=0.080, reward_bound=0.254, batch=225\n",
      "226: loss=1.085, reward_mean=0.060, reward_bound=0.289, batch=227\n",
      "227: loss=1.084, reward_mean=0.040, reward_bound=0.254, batch=228\n",
      "228: loss=1.084, reward_mean=0.050, reward_bound=0.314, batch=228\n",
      "229: loss=1.084, reward_mean=0.040, reward_bound=0.349, batch=225\n",
      "230: loss=1.085, reward_mean=0.050, reward_bound=0.387, batch=217\n",
      "231: loss=1.088, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "232: loss=1.086, reward_mean=0.050, reward_bound=0.167, batch=224\n",
      "233: loss=1.085, reward_mean=0.010, reward_bound=0.000, batch=225\n",
      "234: loss=1.083, reward_mean=0.050, reward_bound=0.289, batch=227\n",
      "235: loss=1.083, reward_mean=0.040, reward_bound=0.249, batch=229\n",
      "236: loss=1.083, reward_mean=0.030, reward_bound=0.254, batch=229\n",
      "237: loss=1.082, reward_mean=0.040, reward_bound=0.328, batch=230\n",
      "238: loss=1.083, reward_mean=0.030, reward_bound=0.349, batch=228\n",
      "239: loss=1.083, reward_mean=0.050, reward_bound=0.387, batch=228\n",
      "240: loss=1.082, reward_mean=0.050, reward_bound=0.353, batch=229\n",
      "241: loss=1.082, reward_mean=0.060, reward_bound=0.387, batch=229\n",
      "242: loss=1.064, reward_mean=0.080, reward_bound=0.430, batch=128\n",
      "243: loss=1.065, reward_mean=0.050, reward_bound=0.000, batch=133\n",
      "244: loss=1.061, reward_mean=0.040, reward_bound=0.000, batch=137\n",
      "245: loss=1.066, reward_mean=0.050, reward_bound=0.000, batch=142\n",
      "246: loss=1.062, reward_mean=0.050, reward_bound=0.000, batch=147\n",
      "247: loss=1.060, reward_mean=0.040, reward_bound=0.000, batch=151\n",
      "248: loss=1.056, reward_mean=0.030, reward_bound=0.000, batch=154\n",
      "249: loss=1.055, reward_mean=0.070, reward_bound=0.000, batch=161\n",
      "250: loss=1.057, reward_mean=0.060, reward_bound=0.000, batch=167\n",
      "251: loss=1.057, reward_mean=0.000, reward_bound=0.000, batch=167\n",
      "252: loss=1.056, reward_mean=0.090, reward_bound=0.000, batch=176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253: loss=1.060, reward_mean=0.030, reward_bound=0.000, batch=179\n",
      "254: loss=1.061, reward_mean=0.020, reward_bound=0.000, batch=181\n",
      "255: loss=1.060, reward_mean=0.040, reward_bound=0.000, batch=185\n",
      "256: loss=1.061, reward_mean=0.030, reward_bound=0.000, batch=188\n",
      "257: loss=1.057, reward_mean=0.040, reward_bound=0.000, batch=192\n",
      "258: loss=1.055, reward_mean=0.090, reward_bound=0.000, batch=201\n",
      "259: loss=1.056, reward_mean=0.040, reward_bound=0.000, batch=205\n",
      "260: loss=1.054, reward_mean=0.050, reward_bound=0.000, batch=210\n",
      "261: loss=1.055, reward_mean=0.070, reward_bound=0.014, batch=217\n",
      "262: loss=1.053, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "263: loss=1.048, reward_mean=0.080, reward_bound=0.080, batch=224\n",
      "264: loss=1.048, reward_mean=0.010, reward_bound=0.000, batch=225\n",
      "265: loss=1.046, reward_mean=0.070, reward_bound=0.150, batch=224\n",
      "266: loss=1.045, reward_mean=0.050, reward_bound=0.165, batch=227\n",
      "267: loss=1.045, reward_mean=0.110, reward_bound=0.185, batch=227\n",
      "268: loss=1.046, reward_mean=0.050, reward_bound=0.206, batch=227\n",
      "269: loss=1.046, reward_mean=0.030, reward_bound=0.216, batch=229\n",
      "270: loss=1.043, reward_mean=0.080, reward_bound=0.229, batch=227\n",
      "271: loss=1.038, reward_mean=0.080, reward_bound=0.254, batch=227\n",
      "272: loss=1.032, reward_mean=0.110, reward_bound=0.282, batch=220\n",
      "273: loss=1.032, reward_mean=0.060, reward_bound=0.254, batch=223\n",
      "274: loss=1.034, reward_mean=0.080, reward_bound=0.314, batch=210\n",
      "275: loss=1.035, reward_mean=0.100, reward_bound=0.206, batch=218\n",
      "276: loss=1.034, reward_mean=0.040, reward_bound=0.017, batch=222\n",
      "277: loss=1.034, reward_mean=0.050, reward_bound=0.206, batch=226\n",
      "278: loss=1.029, reward_mean=0.060, reward_bound=0.206, batch=227\n",
      "279: loss=1.028, reward_mean=0.040, reward_bound=0.224, batch=229\n",
      "280: loss=1.025, reward_mean=0.120, reward_bound=0.349, batch=210\n",
      "281: loss=1.024, reward_mean=0.060, reward_bound=0.000, batch=216\n",
      "282: loss=1.025, reward_mean=0.050, reward_bound=0.083, batch=221\n",
      "283: loss=1.024, reward_mean=0.040, reward_bound=0.072, batch=224\n",
      "284: loss=1.024, reward_mean=0.060, reward_bound=0.206, batch=226\n",
      "285: loss=1.024, reward_mean=0.080, reward_bound=0.331, batch=228\n",
      "286: loss=1.026, reward_mean=0.120, reward_bound=0.349, batch=228\n",
      "287: loss=1.030, reward_mean=0.060, reward_bound=0.387, batch=211\n",
      "288: loss=1.028, reward_mean=0.080, reward_bound=0.206, batch=217\n",
      "289: loss=1.025, reward_mean=0.060, reward_bound=0.097, batch=222\n",
      "290: loss=1.025, reward_mean=0.050, reward_bound=0.163, batch=225\n",
      "291: loss=1.027, reward_mean=0.040, reward_bound=0.234, batch=227\n",
      "292: loss=1.026, reward_mean=0.040, reward_bound=0.277, batch=229\n",
      "293: loss=1.023, reward_mean=0.070, reward_bound=0.295, batch=230\n",
      "294: loss=1.024, reward_mean=0.090, reward_bound=0.349, batch=225\n",
      "295: loss=1.022, reward_mean=0.060, reward_bound=0.234, batch=227\n",
      "296: loss=1.021, reward_mean=0.090, reward_bound=0.380, batch=229\n",
      "297: loss=1.024, reward_mean=0.070, reward_bound=0.387, batch=225\n",
      "298: loss=1.023, reward_mean=0.020, reward_bound=0.070, batch=227\n",
      "299: loss=1.021, reward_mean=0.020, reward_bound=0.203, batch=229\n",
      "300: loss=1.022, reward_mean=0.100, reward_bound=0.387, batch=229\n",
      "301: loss=1.022, reward_mean=0.070, reward_bound=0.381, batch=230\n",
      "302: loss=1.030, reward_mean=0.060, reward_bound=0.430, batch=192\n",
      "303: loss=1.030, reward_mean=0.090, reward_bound=0.000, batch=201\n",
      "304: loss=1.032, reward_mean=0.090, reward_bound=0.000, batch=210\n",
      "305: loss=1.031, reward_mean=0.060, reward_bound=0.000, batch=216\n",
      "306: loss=1.027, reward_mean=0.070, reward_bound=0.136, batch=221\n",
      "307: loss=1.028, reward_mean=0.070, reward_bound=0.167, batch=224\n",
      "308: loss=1.026, reward_mean=0.130, reward_bound=0.229, batch=224\n",
      "309: loss=1.026, reward_mean=0.040, reward_bound=0.244, batch=227\n",
      "310: loss=1.026, reward_mean=0.070, reward_bound=0.254, batch=228\n",
      "311: loss=1.027, reward_mean=0.080, reward_bound=0.282, batch=227\n",
      "312: loss=1.026, reward_mean=0.020, reward_bound=0.251, batch=229\n",
      "313: loss=1.025, reward_mean=0.060, reward_bound=0.314, batch=228\n",
      "314: loss=1.028, reward_mean=0.050, reward_bound=0.349, batch=224\n",
      "315: loss=1.027, reward_mean=0.060, reward_bound=0.305, batch=227\n",
      "316: loss=1.025, reward_mean=0.030, reward_bound=0.210, batch=229\n",
      "317: loss=1.024, reward_mean=0.020, reward_bound=0.229, batch=229\n",
      "318: loss=1.027, reward_mean=0.080, reward_bound=0.314, batch=229\n",
      "319: loss=1.027, reward_mean=0.040, reward_bound=0.309, batch=230\n",
      "320: loss=1.029, reward_mean=0.060, reward_bound=0.338, batch=231\n",
      "321: loss=1.026, reward_mean=0.060, reward_bound=0.349, batch=229\n",
      "322: loss=1.025, reward_mean=0.050, reward_bound=0.387, batch=221\n",
      "323: loss=1.026, reward_mean=0.090, reward_bound=0.430, batch=213\n",
      "324: loss=1.023, reward_mean=0.100, reward_bound=0.282, batch=217\n",
      "325: loss=1.022, reward_mean=0.080, reward_bound=0.263, batch=222\n",
      "326: loss=1.022, reward_mean=0.050, reward_bound=0.263, batch=225\n",
      "327: loss=1.026, reward_mean=0.040, reward_bound=0.289, batch=227\n",
      "328: loss=1.021, reward_mean=0.080, reward_bound=0.349, batch=226\n",
      "329: loss=1.020, reward_mean=0.050, reward_bound=0.368, batch=228\n",
      "330: loss=1.020, reward_mean=0.030, reward_bound=0.353, batch=229\n",
      "331: loss=1.021, reward_mean=0.070, reward_bound=0.387, batch=227\n",
      "332: loss=1.020, reward_mean=0.070, reward_bound=0.414, batch=229\n",
      "333: loss=1.020, reward_mean=0.080, reward_bound=0.405, batch=230\n",
      "334: loss=1.022, reward_mean=0.040, reward_bound=0.430, batch=225\n",
      "335: loss=1.025, reward_mean=0.100, reward_bound=0.478, batch=86\n",
      "336: loss=1.024, reward_mean=0.080, reward_bound=0.000, batch=94\n",
      "337: loss=1.022, reward_mean=0.070, reward_bound=0.000, batch=101\n",
      "338: loss=1.024, reward_mean=0.100, reward_bound=0.000, batch=111\n",
      "339: loss=1.033, reward_mean=0.040, reward_bound=0.000, batch=115\n",
      "340: loss=1.031, reward_mean=0.060, reward_bound=0.000, batch=121\n",
      "341: loss=1.023, reward_mean=0.070, reward_bound=0.000, batch=128\n",
      "342: loss=1.018, reward_mean=0.040, reward_bound=0.000, batch=132\n",
      "343: loss=1.020, reward_mean=0.050, reward_bound=0.000, batch=137\n",
      "344: loss=1.020, reward_mean=0.100, reward_bound=0.000, batch=147\n",
      "345: loss=1.022, reward_mean=0.030, reward_bound=0.000, batch=150\n",
      "346: loss=1.021, reward_mean=0.060, reward_bound=0.000, batch=156\n",
      "347: loss=1.019, reward_mean=0.020, reward_bound=0.000, batch=158\n",
      "348: loss=1.018, reward_mean=0.050, reward_bound=0.000, batch=163\n",
      "349: loss=1.018, reward_mean=0.090, reward_bound=0.000, batch=172\n",
      "350: loss=1.017, reward_mean=0.020, reward_bound=0.000, batch=174\n",
      "351: loss=1.017, reward_mean=0.080, reward_bound=0.000, batch=182\n",
      "352: loss=1.017, reward_mean=0.020, reward_bound=0.000, batch=184\n",
      "353: loss=1.014, reward_mean=0.070, reward_bound=0.000, batch=191\n",
      "354: loss=1.014, reward_mean=0.000, reward_bound=0.000, batch=191\n",
      "355: loss=1.013, reward_mean=0.090, reward_bound=0.000, batch=200\n",
      "356: loss=1.012, reward_mean=0.050, reward_bound=0.000, batch=205\n",
      "357: loss=1.009, reward_mean=0.110, reward_bound=0.068, batch=213\n",
      "358: loss=1.008, reward_mean=0.060, reward_bound=0.048, batch=219\n",
      "359: loss=1.012, reward_mean=0.050, reward_bound=0.080, batch=222\n",
      "360: loss=1.007, reward_mean=0.080, reward_bound=0.109, batch=224\n",
      "361: loss=1.012, reward_mean=0.100, reward_bound=0.150, batch=225\n",
      "362: loss=1.010, reward_mean=0.040, reward_bound=0.167, batch=222\n",
      "363: loss=1.011, reward_mean=0.110, reward_bound=0.185, batch=220\n",
      "364: loss=1.013, reward_mean=0.050, reward_bound=0.206, batch=225\n",
      "365: loss=1.012, reward_mean=0.040, reward_bound=0.206, batch=224\n",
      "366: loss=1.013, reward_mean=0.050, reward_bound=0.229, batch=217\n",
      "367: loss=1.012, reward_mean=0.050, reward_bound=0.133, batch=222\n",
      "368: loss=1.013, reward_mean=0.050, reward_bound=0.185, batch=225\n",
      "369: loss=1.011, reward_mean=0.070, reward_bound=0.229, batch=224\n",
      "370: loss=1.008, reward_mean=0.110, reward_bound=0.254, batch=213\n",
      "371: loss=1.007, reward_mean=0.110, reward_bound=0.261, batch=219\n",
      "372: loss=1.008, reward_mean=0.060, reward_bound=0.225, batch=223\n",
      "373: loss=1.008, reward_mean=0.080, reward_bound=0.282, batch=212\n",
      "374: loss=1.013, reward_mean=0.070, reward_bound=0.211, batch=218\n",
      "375: loss=1.012, reward_mean=0.070, reward_bound=0.286, batch=222\n",
      "376: loss=1.014, reward_mean=0.050, reward_bound=0.213, batch=225\n",
      "377: loss=1.011, reward_mean=0.080, reward_bound=0.314, batch=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378: loss=1.007, reward_mean=0.040, reward_bound=0.000, batch=204\n",
      "379: loss=1.007, reward_mean=0.010, reward_bound=0.000, batch=205\n",
      "380: loss=1.006, reward_mean=0.050, reward_bound=0.000, batch=210\n",
      "381: loss=1.005, reward_mean=0.030, reward_bound=0.000, batch=213\n",
      "382: loss=1.003, reward_mean=0.050, reward_bound=0.000, batch=218\n",
      "383: loss=1.003, reward_mean=0.050, reward_bound=0.053, batch=222\n",
      "384: loss=1.001, reward_mean=0.040, reward_bound=0.090, batch=225\n",
      "385: loss=1.002, reward_mean=0.040, reward_bound=0.167, batch=226\n",
      "386: loss=1.005, reward_mean=0.030, reward_bound=0.196, batch=228\n",
      "387: loss=1.005, reward_mean=0.070, reward_bound=0.254, batch=227\n",
      "388: loss=1.008, reward_mean=0.070, reward_bound=0.282, batch=225\n",
      "389: loss=1.008, reward_mean=0.060, reward_bound=0.314, batch=222\n",
      "390: loss=1.007, reward_mean=0.010, reward_bound=0.000, batch=223\n",
      "391: loss=1.006, reward_mean=0.060, reward_bound=0.244, batch=226\n",
      "392: loss=1.014, reward_mean=0.110, reward_bound=0.349, batch=193\n",
      "393: loss=1.014, reward_mean=0.060, reward_bound=0.000, batch=199\n",
      "394: loss=1.015, reward_mean=0.060, reward_bound=0.000, batch=205\n",
      "395: loss=1.010, reward_mean=0.060, reward_bound=0.000, batch=211\n",
      "396: loss=1.010, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "397: loss=1.010, reward_mean=0.050, reward_bound=0.052, batch=222\n",
      "398: loss=1.006, reward_mean=0.070, reward_bound=0.167, batch=224\n",
      "399: loss=1.004, reward_mean=0.040, reward_bound=0.226, batch=227\n",
      "400: loss=1.007, reward_mean=0.060, reward_bound=0.229, batch=228\n",
      "401: loss=1.006, reward_mean=0.020, reward_bound=0.254, batch=226\n",
      "402: loss=1.004, reward_mean=0.030, reward_bound=0.158, batch=228\n",
      "403: loss=1.006, reward_mean=0.060, reward_bound=0.234, batch=229\n",
      "404: loss=1.005, reward_mean=0.070, reward_bound=0.314, batch=226\n",
      "405: loss=1.003, reward_mean=0.080, reward_bound=0.349, batch=224\n",
      "406: loss=1.003, reward_mean=0.080, reward_bound=0.282, batch=226\n",
      "407: loss=1.003, reward_mean=0.090, reward_bound=0.314, batch=227\n",
      "408: loss=1.002, reward_mean=0.030, reward_bound=0.332, batch=229\n",
      "409: loss=0.986, reward_mean=0.080, reward_bound=0.387, batch=190\n",
      "410: loss=0.987, reward_mean=0.090, reward_bound=0.000, batch=199\n",
      "411: loss=0.985, reward_mean=0.020, reward_bound=0.000, batch=201\n",
      "412: loss=0.985, reward_mean=0.010, reward_bound=0.000, batch=202\n",
      "413: loss=0.984, reward_mean=0.080, reward_bound=0.000, batch=210\n",
      "414: loss=0.982, reward_mean=0.040, reward_bound=0.000, batch=214\n",
      "415: loss=0.981, reward_mean=0.100, reward_bound=0.162, batch=220\n",
      "416: loss=0.985, reward_mean=0.040, reward_bound=0.045, batch=224\n",
      "417: loss=0.989, reward_mean=0.030, reward_bound=0.012, batch=227\n",
      "418: loss=0.981, reward_mean=0.080, reward_bound=0.185, batch=227\n",
      "419: loss=0.980, reward_mean=0.040, reward_bound=0.249, batch=229\n",
      "420: loss=0.976, reward_mean=0.050, reward_bound=0.254, batch=228\n",
      "421: loss=0.976, reward_mean=0.020, reward_bound=0.195, batch=229\n",
      "422: loss=0.976, reward_mean=0.050, reward_bound=0.282, batch=229\n",
      "423: loss=0.974, reward_mean=0.020, reward_bound=0.265, batch=230\n",
      "424: loss=0.977, reward_mean=0.050, reward_bound=0.314, batch=224\n",
      "425: loss=0.974, reward_mean=0.060, reward_bound=0.275, batch=227\n",
      "426: loss=0.972, reward_mean=0.050, reward_bound=0.277, batch=229\n",
      "427: loss=0.974, reward_mean=0.050, reward_bound=0.295, batch=230\n",
      "428: loss=0.974, reward_mean=0.070, reward_bound=0.349, batch=225\n",
      "429: loss=0.974, reward_mean=0.040, reward_bound=0.321, batch=227\n",
      "430: loss=0.973, reward_mean=0.050, reward_bound=0.380, batch=229\n",
      "431: loss=0.974, reward_mean=0.060, reward_bound=0.364, batch=230\n",
      "432: loss=0.974, reward_mean=0.030, reward_bound=0.333, batch=231\n",
      "433: loss=0.979, reward_mean=0.090, reward_bound=0.387, batch=216\n",
      "434: loss=0.978, reward_mean=0.100, reward_bound=0.351, batch=221\n",
      "435: loss=0.980, reward_mean=0.060, reward_bound=0.349, batch=224\n",
      "436: loss=0.980, reward_mean=0.080, reward_bound=0.384, batch=227\n",
      "437: loss=0.979, reward_mean=0.080, reward_bound=0.380, batch=229\n",
      "438: loss=0.978, reward_mean=0.040, reward_bound=0.387, batch=227\n",
      "439: loss=0.980, reward_mean=0.020, reward_bound=0.148, batch=229\n",
      "440: loss=0.978, reward_mean=0.090, reward_bound=0.405, batch=230\n",
      "441: loss=0.978, reward_mean=0.040, reward_bound=0.314, batch=230\n",
      "442: loss=0.978, reward_mean=0.060, reward_bound=0.418, batch=231\n",
      "443: loss=0.985, reward_mean=0.070, reward_bound=0.430, batch=169\n",
      "444: loss=0.976, reward_mean=0.060, reward_bound=0.000, batch=175\n",
      "445: loss=0.980, reward_mean=0.030, reward_bound=0.000, batch=178\n",
      "446: loss=0.978, reward_mean=0.050, reward_bound=0.000, batch=183\n",
      "447: loss=0.976, reward_mean=0.050, reward_bound=0.000, batch=188\n",
      "448: loss=0.974, reward_mean=0.020, reward_bound=0.000, batch=190\n",
      "449: loss=0.971, reward_mean=0.030, reward_bound=0.000, batch=193\n",
      "450: loss=0.970, reward_mean=0.020, reward_bound=0.000, batch=195\n",
      "451: loss=0.969, reward_mean=0.030, reward_bound=0.000, batch=198\n",
      "452: loss=0.978, reward_mean=0.130, reward_bound=0.124, batch=208\n",
      "453: loss=0.981, reward_mean=0.040, reward_bound=0.000, batch=212\n",
      "454: loss=0.980, reward_mean=0.070, reward_bound=0.150, batch=214\n",
      "455: loss=0.981, reward_mean=0.050, reward_bound=0.000, batch=219\n",
      "456: loss=0.980, reward_mean=0.030, reward_bound=0.000, batch=222\n",
      "457: loss=0.978, reward_mean=0.080, reward_bound=0.167, batch=224\n",
      "458: loss=0.981, reward_mean=0.060, reward_bound=0.185, batch=226\n",
      "459: loss=0.980, reward_mean=0.030, reward_bound=0.164, batch=228\n",
      "460: loss=0.979, reward_mean=0.040, reward_bound=0.171, batch=229\n",
      "461: loss=0.977, reward_mean=0.060, reward_bound=0.206, batch=228\n",
      "462: loss=0.978, reward_mean=0.030, reward_bound=0.229, batch=228\n",
      "463: loss=0.974, reward_mean=0.060, reward_bound=0.254, batch=224\n",
      "464: loss=0.973, reward_mean=0.060, reward_bound=0.282, batch=222\n",
      "465: loss=0.972, reward_mean=0.050, reward_bound=0.172, batch=225\n",
      "466: loss=0.972, reward_mean=0.030, reward_bound=0.194, batch=227\n",
      "467: loss=0.972, reward_mean=0.020, reward_bound=0.183, batch=229\n",
      "468: loss=0.973, reward_mean=0.030, reward_bound=0.203, batch=230\n",
      "469: loss=0.970, reward_mean=0.070, reward_bound=0.282, batch=230\n",
      "470: loss=0.971, reward_mean=0.020, reward_bound=0.314, batch=227\n",
      "471: loss=0.970, reward_mean=0.040, reward_bound=0.316, batch=229\n",
      "472: loss=0.972, reward_mean=0.080, reward_bound=0.349, batch=224\n",
      "473: loss=0.972, reward_mean=0.040, reward_bound=0.314, batch=226\n",
      "474: loss=0.969, reward_mean=0.060, reward_bound=0.387, batch=214\n",
      "475: loss=0.970, reward_mean=0.090, reward_bound=0.147, batch=220\n",
      "476: loss=0.966, reward_mean=0.080, reward_bound=0.206, batch=226\n",
      "477: loss=0.962, reward_mean=0.050, reward_bound=0.206, batch=227\n",
      "478: loss=0.961, reward_mean=0.070, reward_bound=0.277, batch=229\n",
      "479: loss=0.960, reward_mean=0.020, reward_bound=0.155, batch=230\n",
      "480: loss=0.961, reward_mean=0.020, reward_bound=0.222, batch=231\n",
      "481: loss=0.966, reward_mean=0.110, reward_bound=0.314, batch=227\n",
      "482: loss=0.966, reward_mean=0.060, reward_bound=0.325, batch=229\n",
      "483: loss=0.967, reward_mean=0.040, reward_bound=0.278, batch=230\n",
      "484: loss=0.966, reward_mean=0.040, reward_bound=0.259, batch=231\n",
      "485: loss=0.967, reward_mean=0.040, reward_bound=0.282, batch=231\n",
      "486: loss=0.968, reward_mean=0.040, reward_bound=0.314, batch=231\n",
      "487: loss=0.965, reward_mean=0.060, reward_bound=0.349, batch=227\n",
      "488: loss=0.964, reward_mean=0.060, reward_bound=0.277, batch=229\n",
      "489: loss=0.962, reward_mean=0.060, reward_bound=0.387, batch=226\n",
      "490: loss=0.961, reward_mean=0.050, reward_bound=0.298, batch=228\n",
      "491: loss=0.960, reward_mean=0.060, reward_bound=0.314, batch=228\n",
      "492: loss=0.971, reward_mean=0.030, reward_bound=0.430, batch=206\n",
      "493: loss=0.968, reward_mean=0.050, reward_bound=0.000, batch=211\n",
      "494: loss=0.971, reward_mean=0.050, reward_bound=0.000, batch=216\n",
      "495: loss=0.967, reward_mean=0.040, reward_bound=0.000, batch=220\n",
      "496: loss=0.966, reward_mean=0.080, reward_bound=0.216, batch=224\n",
      "497: loss=0.966, reward_mean=0.090, reward_bound=0.282, batch=224\n",
      "498: loss=0.965, reward_mean=0.060, reward_bound=0.311, batch=227\n",
      "499: loss=0.966, reward_mean=0.110, reward_bound=0.314, batch=227\n",
      "500: loss=0.963, reward_mean=0.050, reward_bound=0.349, batch=225\n",
      "501: loss=0.966, reward_mean=0.070, reward_bound=0.387, batch=220\n",
      "502: loss=0.967, reward_mean=0.010, reward_bound=0.000, batch=221\n",
      "503: loss=0.966, reward_mean=0.070, reward_bound=0.314, batch=224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504: loss=0.964, reward_mean=0.100, reward_bound=0.377, batch=227\n",
      "505: loss=0.964, reward_mean=0.050, reward_bound=0.267, batch=229\n",
      "506: loss=0.963, reward_mean=0.090, reward_bound=0.295, batch=230\n",
      "507: loss=0.966, reward_mean=0.030, reward_bound=0.365, batch=231\n",
      "508: loss=0.965, reward_mean=0.030, reward_bound=0.387, batch=226\n",
      "509: loss=0.965, reward_mean=0.010, reward_bound=0.000, batch=227\n",
      "510: loss=0.963, reward_mean=0.090, reward_bound=0.422, batch=229\n",
      "511: loss=0.969, reward_mean=0.080, reward_bound=0.430, batch=222\n",
      "512: loss=0.970, reward_mean=0.050, reward_bound=0.229, batch=223\n",
      "513: loss=0.971, reward_mean=0.030, reward_bound=0.169, batch=226\n",
      "514: loss=0.968, reward_mean=0.050, reward_bound=0.351, batch=228\n",
      "515: loss=0.966, reward_mean=0.050, reward_bound=0.387, batch=228\n",
      "516: loss=0.965, reward_mean=0.090, reward_bound=0.353, batch=229\n",
      "517: loss=0.965, reward_mean=0.040, reward_bound=0.324, batch=230\n",
      "518: loss=0.965, reward_mean=0.080, reward_bound=0.418, batch=231\n",
      "519: loss=0.965, reward_mean=0.060, reward_bound=0.430, batch=228\n",
      "520: loss=0.965, reward_mean=0.040, reward_bound=0.430, batch=228\n",
      "521: loss=0.967, reward_mean=0.060, reward_bound=0.478, batch=230\n",
      "522: loss=0.984, reward_mean=0.050, reward_bound=0.478, batch=150\n",
      "523: loss=0.984, reward_mean=0.010, reward_bound=0.000, batch=151\n",
      "524: loss=0.978, reward_mean=0.040, reward_bound=0.000, batch=155\n",
      "525: loss=0.977, reward_mean=0.080, reward_bound=0.000, batch=163\n",
      "526: loss=0.974, reward_mean=0.060, reward_bound=0.000, batch=169\n",
      "527: loss=0.972, reward_mean=0.050, reward_bound=0.000, batch=174\n",
      "528: loss=0.980, reward_mean=0.070, reward_bound=0.000, batch=181\n",
      "529: loss=0.979, reward_mean=0.020, reward_bound=0.000, batch=183\n",
      "530: loss=0.976, reward_mean=0.030, reward_bound=0.000, batch=186\n",
      "531: loss=0.975, reward_mean=0.060, reward_bound=0.000, batch=192\n",
      "532: loss=0.973, reward_mean=0.060, reward_bound=0.000, batch=198\n",
      "533: loss=0.973, reward_mean=0.070, reward_bound=0.000, batch=205\n",
      "534: loss=0.975, reward_mean=0.080, reward_bound=0.002, batch=213\n",
      "535: loss=0.977, reward_mean=0.060, reward_bound=0.007, batch=219\n",
      "536: loss=0.980, reward_mean=0.050, reward_bound=0.018, batch=223\n",
      "537: loss=0.981, reward_mean=0.090, reward_bound=0.105, batch=226\n",
      "538: loss=0.978, reward_mean=0.070, reward_bound=0.151, batch=228\n",
      "539: loss=0.974, reward_mean=0.100, reward_bound=0.206, batch=227\n",
      "540: loss=0.975, reward_mean=0.020, reward_bound=0.120, batch=229\n",
      "541: loss=0.979, reward_mean=0.070, reward_bound=0.229, batch=224\n",
      "542: loss=0.977, reward_mean=0.120, reward_bound=0.280, batch=227\n",
      "543: loss=0.973, reward_mean=0.040, reward_bound=0.282, batch=224\n",
      "544: loss=0.979, reward_mean=0.090, reward_bound=0.314, batch=214\n",
      "545: loss=0.977, reward_mean=0.030, reward_bound=0.000, batch=217\n",
      "546: loss=0.980, reward_mean=0.060, reward_bound=0.160, batch=222\n",
      "547: loss=0.983, reward_mean=0.070, reward_bound=0.236, batch=225\n",
      "548: loss=0.983, reward_mean=0.090, reward_bound=0.282, batch=226\n",
      "549: loss=0.982, reward_mean=0.040, reward_bound=0.244, batch=228\n",
      "550: loss=0.982, reward_mean=0.060, reward_bound=0.289, batch=229\n",
      "551: loss=0.981, reward_mean=0.060, reward_bound=0.292, batch=230\n",
      "552: loss=0.971, reward_mean=0.060, reward_bound=0.349, batch=218\n",
      "553: loss=0.972, reward_mean=0.040, reward_bound=0.035, batch=222\n",
      "554: loss=0.970, reward_mean=0.080, reward_bound=0.265, batch=225\n",
      "555: loss=0.972, reward_mean=0.060, reward_bound=0.387, batch=211\n",
      "556: loss=0.975, reward_mean=0.080, reward_bound=0.122, batch=217\n",
      "557: loss=0.979, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "558: loss=0.973, reward_mean=0.080, reward_bound=0.254, batch=224\n",
      "559: loss=0.975, reward_mean=0.030, reward_bound=0.098, batch=227\n",
      "560: loss=0.971, reward_mean=0.060, reward_bound=0.282, batch=228\n",
      "561: loss=0.970, reward_mean=0.030, reward_bound=0.289, batch=229\n",
      "562: loss=0.973, reward_mean=0.040, reward_bound=0.349, batch=227\n",
      "563: loss=0.972, reward_mean=0.030, reward_bound=0.216, batch=229\n",
      "564: loss=0.973, reward_mean=0.080, reward_bound=0.387, batch=224\n",
      "565: loss=0.972, reward_mean=0.110, reward_bound=0.426, batch=227\n",
      "566: loss=0.975, reward_mean=0.080, reward_bound=0.430, batch=210\n",
      "567: loss=0.974, reward_mean=0.070, reward_bound=0.105, batch=217\n",
      "568: loss=0.970, reward_mean=0.080, reward_bound=0.147, batch=222\n",
      "569: loss=0.971, reward_mean=0.080, reward_bound=0.185, batch=224\n",
      "570: loss=0.969, reward_mean=0.110, reward_bound=0.282, batch=224\n",
      "571: loss=0.968, reward_mean=0.060, reward_bound=0.345, batch=227\n",
      "572: loss=0.966, reward_mean=0.070, reward_bound=0.349, batch=227\n",
      "573: loss=0.966, reward_mean=0.080, reward_bound=0.380, batch=229\n",
      "574: loss=0.971, reward_mean=0.060, reward_bound=0.387, batch=226\n",
      "575: loss=0.972, reward_mean=0.060, reward_bound=0.430, batch=221\n",
      "576: loss=0.971, reward_mean=0.080, reward_bound=0.282, batch=224\n",
      "577: loss=0.970, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "578: loss=0.969, reward_mean=0.040, reward_bound=0.271, batch=228\n",
      "579: loss=0.971, reward_mean=0.070, reward_bound=0.353, batch=229\n",
      "580: loss=0.972, reward_mean=0.060, reward_bound=0.364, batch=230\n",
      "581: loss=0.973, reward_mean=0.050, reward_bound=0.418, batch=231\n",
      "582: loss=0.975, reward_mean=0.040, reward_bound=0.430, batch=229\n",
      "583: loss=0.976, reward_mean=0.100, reward_bound=0.405, batch=230\n",
      "584: loss=0.977, reward_mean=0.040, reward_bound=0.378, batch=231\n",
      "585: loss=0.976, reward_mean=0.030, reward_bound=0.430, batch=230\n",
      "586: loss=0.975, reward_mean=0.080, reward_bound=0.464, batch=231\n",
      "587: loss=0.976, reward_mean=0.060, reward_bound=0.478, batch=194\n",
      "588: loss=0.979, reward_mean=0.100, reward_bound=0.000, batch=204\n",
      "589: loss=0.976, reward_mean=0.060, reward_bound=0.000, batch=210\n",
      "590: loss=0.977, reward_mean=0.040, reward_bound=0.000, batch=214\n",
      "591: loss=0.980, reward_mean=0.050, reward_bound=0.000, batch=219\n",
      "592: loss=0.976, reward_mean=0.060, reward_bound=0.103, batch=223\n",
      "593: loss=0.976, reward_mean=0.010, reward_bound=0.000, batch=224\n",
      "594: loss=0.975, reward_mean=0.070, reward_bound=0.135, batch=226\n",
      "595: loss=0.975, reward_mean=0.040, reward_bound=0.168, batch=228\n",
      "596: loss=0.970, reward_mean=0.140, reward_bound=0.254, batch=228\n",
      "597: loss=0.969, reward_mean=0.050, reward_bound=0.282, batch=227\n",
      "598: loss=0.971, reward_mean=0.030, reward_bound=0.229, batch=228\n",
      "599: loss=0.971, reward_mean=0.100, reward_bound=0.314, batch=223\n",
      "600: loss=0.969, reward_mean=0.030, reward_bound=0.153, batch=226\n",
      "601: loss=0.970, reward_mean=0.060, reward_bound=0.241, batch=228\n",
      "602: loss=0.977, reward_mean=0.090, reward_bound=0.349, batch=221\n",
      "603: loss=0.977, reward_mean=0.060, reward_bound=0.206, batch=224\n",
      "604: loss=0.982, reward_mean=0.050, reward_bound=0.223, batch=227\n",
      "605: loss=0.977, reward_mean=0.080, reward_bound=0.249, batch=229\n",
      "606: loss=0.979, reward_mean=0.090, reward_bound=0.314, batch=228\n",
      "607: loss=0.972, reward_mean=0.080, reward_bound=0.387, batch=216\n",
      "608: loss=0.968, reward_mean=0.050, reward_bound=0.114, batch=221\n",
      "609: loss=0.969, reward_mean=0.050, reward_bound=0.229, batch=224\n",
      "610: loss=0.967, reward_mean=0.030, reward_bound=0.206, batch=227\n",
      "611: loss=0.967, reward_mean=0.040, reward_bound=0.249, batch=229\n",
      "612: loss=0.969, reward_mean=0.060, reward_bound=0.278, batch=230\n",
      "613: loss=0.968, reward_mean=0.080, reward_bound=0.338, batch=231\n",
      "614: loss=0.974, reward_mean=0.050, reward_bound=0.349, batch=230\n",
      "615: loss=0.973, reward_mean=0.070, reward_bound=0.376, batch=231\n",
      "616: loss=0.970, reward_mean=0.040, reward_bound=0.387, batch=225\n",
      "617: loss=0.970, reward_mean=0.060, reward_bound=0.396, batch=227\n",
      "618: loss=0.973, reward_mean=0.060, reward_bound=0.430, batch=218\n",
      "619: loss=0.976, reward_mean=0.050, reward_bound=0.187, batch=222\n",
      "620: loss=0.973, reward_mean=0.060, reward_bound=0.254, batch=225\n",
      "621: loss=0.973, reward_mean=0.050, reward_bound=0.289, batch=227\n",
      "622: loss=0.971, reward_mean=0.070, reward_bound=0.342, batch=229\n",
      "623: loss=0.970, reward_mean=0.040, reward_bound=0.349, batch=224\n",
      "624: loss=0.970, reward_mean=0.030, reward_bound=0.185, batch=227\n",
      "625: loss=0.973, reward_mean=0.100, reward_bound=0.342, batch=229\n",
      "626: loss=0.972, reward_mean=0.080, reward_bound=0.364, batch=230\n",
      "627: loss=0.971, reward_mean=0.080, reward_bound=0.430, batch=226\n",
      "628: loss=0.969, reward_mean=0.040, reward_bound=0.318, batch=228\n",
      "629: loss=0.969, reward_mean=0.050, reward_bound=0.272, batch=229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630: loss=0.970, reward_mean=0.060, reward_bound=0.430, batch=228\n",
      "631: loss=0.969, reward_mean=0.070, reward_bound=0.478, batch=230\n",
      "632: loss=0.969, reward_mean=0.050, reward_bound=0.430, batch=230\n",
      "633: loss=0.969, reward_mean=0.050, reward_bound=0.439, batch=231\n",
      "634: loss=0.969, reward_mean=0.050, reward_bound=0.430, batch=231\n",
      "635: loss=0.971, reward_mean=0.040, reward_bound=0.478, batch=214\n",
      "636: loss=0.972, reward_mean=0.030, reward_bound=0.000, batch=217\n",
      "637: loss=0.975, reward_mean=0.060, reward_bound=0.237, batch=222\n",
      "638: loss=0.972, reward_mean=0.050, reward_bound=0.254, batch=224\n",
      "639: loss=0.974, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "640: loss=0.969, reward_mean=0.050, reward_bound=0.314, batch=224\n",
      "641: loss=0.971, reward_mean=0.090, reward_bound=0.349, batch=224\n",
      "642: loss=0.973, reward_mean=0.070, reward_bound=0.374, batch=227\n",
      "643: loss=0.971, reward_mean=0.050, reward_bound=0.342, batch=229\n",
      "644: loss=0.973, reward_mean=0.070, reward_bound=0.364, batch=230\n",
      "645: loss=0.972, reward_mean=0.030, reward_bound=0.387, batch=230\n",
      "646: loss=0.972, reward_mean=0.040, reward_bound=0.406, batch=231\n",
      "647: loss=0.971, reward_mean=0.040, reward_bound=0.430, batch=225\n",
      "648: loss=0.970, reward_mean=0.100, reward_bound=0.396, batch=227\n",
      "649: loss=0.969, reward_mean=0.070, reward_bound=0.387, batch=228\n",
      "650: loss=0.968, reward_mean=0.060, reward_bound=0.392, batch=229\n",
      "651: loss=0.968, reward_mean=0.050, reward_bound=0.349, batch=229\n",
      "652: loss=0.968, reward_mean=0.050, reward_bound=0.405, batch=230\n",
      "653: loss=0.969, reward_mean=0.060, reward_bound=0.406, batch=231\n",
      "654: loss=0.971, reward_mean=0.070, reward_bound=0.430, batch=229\n",
      "655: loss=0.970, reward_mean=0.060, reward_bound=0.450, batch=230\n",
      "656: loss=0.970, reward_mean=0.060, reward_bound=0.430, batch=230\n",
      "657: loss=0.970, reward_mean=0.050, reward_bound=0.451, batch=231\n",
      "658: loss=0.970, reward_mean=0.010, reward_bound=0.254, batch=231\n",
      "659: loss=0.970, reward_mean=0.090, reward_bound=0.314, batch=231\n",
      "660: loss=0.968, reward_mean=0.060, reward_bound=0.478, batch=223\n",
      "661: loss=0.969, reward_mean=0.040, reward_bound=0.113, batch=226\n",
      "662: loss=0.966, reward_mean=0.030, reward_bound=0.195, batch=228\n",
      "663: loss=0.967, reward_mean=0.070, reward_bound=0.293, batch=229\n",
      "664: loss=0.966, reward_mean=0.040, reward_bound=0.364, batch=230\n",
      "665: loss=0.967, reward_mean=0.060, reward_bound=0.418, batch=231\n",
      "666: loss=0.968, reward_mean=0.050, reward_bound=0.430, batch=230\n",
      "667: loss=0.967, reward_mean=0.070, reward_bound=0.478, batch=228\n",
      "668: loss=0.968, reward_mean=0.010, reward_bound=0.039, batch=229\n",
      "669: loss=0.967, reward_mean=0.070, reward_bound=0.430, batch=229\n",
      "670: loss=0.968, reward_mean=0.090, reward_bound=0.500, batch=230\n",
      "671: loss=0.967, reward_mean=0.080, reward_bound=0.356, batch=231\n",
      "672: loss=0.968, reward_mean=0.060, reward_bound=0.387, batch=230\n",
      "673: loss=0.967, reward_mean=0.050, reward_bound=0.418, batch=231\n",
      "675: loss=0.994, reward_mean=0.090, reward_bound=0.000, batch=9\n",
      "676: loss=0.976, reward_mean=0.050, reward_bound=0.000, batch=14\n",
      "677: loss=0.964, reward_mean=0.090, reward_bound=0.000, batch=23\n",
      "678: loss=0.963, reward_mean=0.060, reward_bound=0.000, batch=29\n",
      "679: loss=0.956, reward_mean=0.010, reward_bound=0.000, batch=30\n",
      "680: loss=0.949, reward_mean=0.070, reward_bound=0.000, batch=37\n",
      "681: loss=0.948, reward_mean=0.060, reward_bound=0.000, batch=43\n",
      "682: loss=0.944, reward_mean=0.010, reward_bound=0.000, batch=44\n",
      "683: loss=0.948, reward_mean=0.060, reward_bound=0.000, batch=50\n",
      "684: loss=0.956, reward_mean=0.080, reward_bound=0.000, batch=58\n",
      "685: loss=0.961, reward_mean=0.080, reward_bound=0.000, batch=66\n",
      "686: loss=0.958, reward_mean=0.050, reward_bound=0.000, batch=71\n",
      "687: loss=0.955, reward_mean=0.050, reward_bound=0.000, batch=76\n",
      "688: loss=0.956, reward_mean=0.040, reward_bound=0.000, batch=80\n",
      "689: loss=0.953, reward_mean=0.050, reward_bound=0.000, batch=85\n",
      "690: loss=0.950, reward_mean=0.110, reward_bound=0.000, batch=96\n",
      "691: loss=0.950, reward_mean=0.060, reward_bound=0.000, batch=102\n",
      "692: loss=0.945, reward_mean=0.070, reward_bound=0.000, batch=109\n",
      "693: loss=0.941, reward_mean=0.060, reward_bound=0.000, batch=115\n",
      "694: loss=0.941, reward_mean=0.020, reward_bound=0.000, batch=117\n",
      "695: loss=0.942, reward_mean=0.030, reward_bound=0.000, batch=120\n",
      "696: loss=0.940, reward_mean=0.080, reward_bound=0.000, batch=128\n",
      "697: loss=0.943, reward_mean=0.080, reward_bound=0.000, batch=136\n",
      "698: loss=0.938, reward_mean=0.090, reward_bound=0.000, batch=145\n",
      "699: loss=0.935, reward_mean=0.060, reward_bound=0.000, batch=151\n",
      "700: loss=0.929, reward_mean=0.080, reward_bound=0.000, batch=159\n",
      "701: loss=0.929, reward_mean=0.060, reward_bound=0.000, batch=165\n",
      "702: loss=0.930, reward_mean=0.070, reward_bound=0.000, batch=172\n",
      "703: loss=0.929, reward_mean=0.110, reward_bound=0.000, batch=183\n",
      "704: loss=0.928, reward_mean=0.030, reward_bound=0.000, batch=186\n",
      "705: loss=0.929, reward_mean=0.030, reward_bound=0.000, batch=189\n",
      "706: loss=0.928, reward_mean=0.080, reward_bound=0.000, batch=197\n",
      "707: loss=0.929, reward_mean=0.120, reward_bound=0.015, batch=208\n",
      "708: loss=0.929, reward_mean=0.070, reward_bound=0.002, batch=215\n",
      "709: loss=0.928, reward_mean=0.080, reward_bound=0.042, batch=219\n",
      "710: loss=0.928, reward_mean=0.050, reward_bound=0.072, batch=221\n",
      "711: loss=0.926, reward_mean=0.060, reward_bound=0.089, batch=223\n",
      "712: loss=0.928, reward_mean=0.040, reward_bound=0.098, batch=222\n",
      "713: loss=0.934, reward_mean=0.080, reward_bound=0.109, batch=224\n",
      "714: loss=0.936, reward_mean=0.080, reward_bound=0.122, batch=226\n",
      "715: loss=0.938, reward_mean=0.050, reward_bound=0.135, batch=218\n",
      "716: loss=0.936, reward_mean=0.080, reward_bound=0.150, batch=216\n",
      "717: loss=0.935, reward_mean=0.080, reward_bound=0.167, batch=213\n",
      "718: loss=0.930, reward_mean=0.090, reward_bound=0.185, batch=211\n",
      "719: loss=0.929, reward_mean=0.070, reward_bound=0.185, batch=217\n",
      "720: loss=0.928, reward_mean=0.020, reward_bound=0.000, batch=219\n",
      "721: loss=0.930, reward_mean=0.090, reward_bound=0.206, batch=219\n",
      "722: loss=0.931, reward_mean=0.070, reward_bound=0.229, batch=213\n",
      "723: loss=0.933, reward_mean=0.110, reward_bound=0.254, batch=203\n",
      "724: loss=0.934, reward_mean=0.100, reward_bound=0.160, batch=212\n",
      "725: loss=0.935, reward_mean=0.080, reward_bound=0.167, batch=217\n",
      "726: loss=0.937, reward_mean=0.080, reward_bound=0.206, batch=221\n",
      "727: loss=0.934, reward_mean=0.070, reward_bound=0.229, batch=224\n",
      "728: loss=0.930, reward_mean=0.080, reward_bound=0.282, batch=195\n",
      "729: loss=0.927, reward_mean=0.050, reward_bound=0.000, batch=200\n",
      "730: loss=0.924, reward_mean=0.070, reward_bound=0.000, batch=207\n",
      "731: loss=0.928, reward_mean=0.060, reward_bound=0.000, batch=213\n",
      "732: loss=0.928, reward_mean=0.080, reward_bound=0.125, batch=219\n",
      "733: loss=0.929, reward_mean=0.060, reward_bound=0.141, batch=223\n",
      "734: loss=0.926, reward_mean=0.060, reward_bound=0.198, batch=226\n",
      "735: loss=0.924, reward_mean=0.070, reward_bound=0.217, batch=228\n",
      "736: loss=0.928, reward_mean=0.050, reward_bound=0.231, batch=229\n",
      "737: loss=0.927, reward_mean=0.030, reward_bound=0.254, batch=226\n",
      "738: loss=0.928, reward_mean=0.070, reward_bound=0.241, batch=228\n",
      "739: loss=0.925, reward_mean=0.060, reward_bound=0.282, batch=223\n",
      "740: loss=0.923, reward_mean=0.030, reward_bound=0.039, batch=226\n",
      "741: loss=0.923, reward_mean=0.070, reward_bound=0.282, batch=227\n",
      "742: loss=0.927, reward_mean=0.070, reward_bound=0.314, batch=200\n",
      "743: loss=0.928, reward_mean=0.050, reward_bound=0.000, batch=205\n",
      "744: loss=0.926, reward_mean=0.100, reward_bound=0.161, batch=213\n",
      "745: loss=0.925, reward_mean=0.050, reward_bound=0.000, batch=218\n",
      "746: loss=0.922, reward_mean=0.050, reward_bound=0.152, batch=222\n",
      "747: loss=0.922, reward_mean=0.090, reward_bound=0.191, batch=225\n",
      "748: loss=0.926, reward_mean=0.080, reward_bound=0.254, batch=224\n",
      "749: loss=0.928, reward_mean=0.060, reward_bound=0.206, batch=226\n",
      "750: loss=0.927, reward_mean=0.090, reward_bound=0.282, batch=227\n",
      "751: loss=0.926, reward_mean=0.060, reward_bound=0.308, batch=229\n",
      "752: loss=0.926, reward_mean=0.030, reward_bound=0.295, batch=230\n",
      "753: loss=0.926, reward_mean=0.100, reward_bound=0.314, batch=229\n",
      "754: loss=0.926, reward_mean=0.030, reward_bound=0.328, batch=230\n",
      "755: loss=0.914, reward_mean=0.070, reward_bound=0.349, batch=181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756: loss=0.918, reward_mean=0.120, reward_bound=0.000, batch=193\n",
      "757: loss=0.914, reward_mean=0.080, reward_bound=0.000, batch=201\n",
      "758: loss=0.915, reward_mean=0.060, reward_bound=0.000, batch=207\n",
      "759: loss=0.918, reward_mean=0.070, reward_bound=0.000, batch=214\n",
      "760: loss=0.916, reward_mean=0.070, reward_bound=0.066, batch=220\n",
      "761: loss=0.917, reward_mean=0.050, reward_bound=0.077, batch=224\n",
      "762: loss=0.910, reward_mean=0.080, reward_bound=0.147, batch=227\n",
      "763: loss=0.911, reward_mean=0.050, reward_bound=0.167, batch=228\n",
      "764: loss=0.913, reward_mean=0.070, reward_bound=0.185, batch=228\n",
      "765: loss=0.914, reward_mean=0.040, reward_bound=0.206, batch=226\n",
      "766: loss=0.910, reward_mean=0.070, reward_bound=0.229, batch=224\n",
      "767: loss=0.915, reward_mean=0.110, reward_bound=0.282, batch=223\n",
      "768: loss=0.916, reward_mean=0.040, reward_bound=0.280, batch=226\n",
      "769: loss=0.915, reward_mean=0.060, reward_bound=0.314, batch=221\n",
      "770: loss=0.918, reward_mean=0.100, reward_bound=0.349, batch=212\n",
      "771: loss=0.922, reward_mean=0.140, reward_bound=0.324, batch=218\n",
      "772: loss=0.921, reward_mean=0.070, reward_bound=0.317, batch=222\n",
      "773: loss=0.922, reward_mean=0.070, reward_bound=0.324, batch=225\n",
      "774: loss=0.919, reward_mean=0.030, reward_bound=0.289, batch=227\n",
      "775: loss=0.920, reward_mean=0.060, reward_bound=0.342, batch=229\n",
      "776: loss=0.917, reward_mean=0.060, reward_bound=0.349, batch=225\n",
      "777: loss=0.917, reward_mean=0.060, reward_bound=0.356, batch=227\n",
      "778: loss=0.917, reward_mean=0.080, reward_bound=0.380, batch=229\n",
      "779: loss=0.915, reward_mean=0.040, reward_bound=0.387, batch=161\n",
      "780: loss=0.918, reward_mean=0.050, reward_bound=0.000, batch=166\n",
      "781: loss=0.924, reward_mean=0.090, reward_bound=0.000, batch=175\n",
      "782: loss=0.914, reward_mean=0.100, reward_bound=0.000, batch=185\n",
      "783: loss=0.912, reward_mean=0.040, reward_bound=0.000, batch=189\n",
      "784: loss=0.911, reward_mean=0.030, reward_bound=0.000, batch=192\n",
      "785: loss=0.912, reward_mean=0.090, reward_bound=0.000, batch=201\n",
      "786: loss=0.911, reward_mean=0.060, reward_bound=0.000, batch=207\n",
      "787: loss=0.916, reward_mean=0.070, reward_bound=0.000, batch=214\n",
      "788: loss=0.916, reward_mean=0.080, reward_bound=0.077, batch=220\n",
      "789: loss=0.915, reward_mean=0.040, reward_bound=0.056, batch=224\n",
      "790: loss=0.919, reward_mean=0.050, reward_bound=0.119, batch=227\n",
      "791: loss=0.919, reward_mean=0.040, reward_bound=0.132, batch=229\n",
      "792: loss=0.912, reward_mean=0.090, reward_bound=0.206, batch=225\n",
      "793: loss=0.910, reward_mean=0.100, reward_bound=0.229, batch=226\n",
      "794: loss=0.910, reward_mean=0.060, reward_bound=0.241, batch=228\n",
      "795: loss=0.915, reward_mean=0.050, reward_bound=0.254, batch=226\n",
      "796: loss=0.916, reward_mean=0.080, reward_bound=0.282, batch=223\n",
      "797: loss=0.917, reward_mean=0.140, reward_bound=0.314, batch=221\n",
      "798: loss=0.914, reward_mean=0.040, reward_bound=0.122, batch=224\n",
      "799: loss=0.913, reward_mean=0.100, reward_bound=0.349, batch=218\n",
      "800: loss=0.913, reward_mean=0.060, reward_bound=0.195, batch=222\n",
      "801: loss=0.910, reward_mean=0.090, reward_bound=0.292, batch=225\n",
      "802: loss=0.908, reward_mean=0.110, reward_bound=0.321, batch=227\n",
      "803: loss=0.908, reward_mean=0.060, reward_bound=0.342, batch=229\n",
      "804: loss=0.908, reward_mean=0.070, reward_bound=0.364, batch=230\n",
      "805: loss=0.907, reward_mean=0.050, reward_bound=0.376, batch=231\n",
      "806: loss=0.906, reward_mean=0.040, reward_bound=0.387, batch=210\n",
      "807: loss=0.905, reward_mean=0.090, reward_bound=0.247, batch=217\n",
      "808: loss=0.906, reward_mean=0.030, reward_bound=0.000, batch=220\n",
      "809: loss=0.905, reward_mean=0.050, reward_bound=0.247, batch=224\n",
      "810: loss=0.907, reward_mean=0.040, reward_bound=0.238, batch=227\n",
      "811: loss=0.905, reward_mean=0.090, reward_bound=0.277, batch=229\n",
      "812: loss=0.902, reward_mean=0.130, reward_bound=0.314, batch=228\n",
      "813: loss=0.903, reward_mean=0.110, reward_bound=0.353, batch=229\n",
      "814: loss=0.902, reward_mean=0.070, reward_bound=0.364, batch=230\n",
      "815: loss=0.903, reward_mean=0.150, reward_bound=0.387, batch=228\n",
      "816: loss=0.905, reward_mean=0.030, reward_bound=0.430, batch=129\n",
      "817: loss=0.912, reward_mean=0.070, reward_bound=0.000, batch=136\n",
      "818: loss=0.909, reward_mean=0.050, reward_bound=0.000, batch=141\n",
      "819: loss=0.913, reward_mean=0.110, reward_bound=0.000, batch=152\n",
      "820: loss=0.909, reward_mean=0.070, reward_bound=0.000, batch=159\n",
      "821: loss=0.904, reward_mean=0.060, reward_bound=0.000, batch=165\n",
      "822: loss=0.902, reward_mean=0.050, reward_bound=0.000, batch=170\n",
      "823: loss=0.904, reward_mean=0.030, reward_bound=0.000, batch=173\n",
      "824: loss=0.900, reward_mean=0.060, reward_bound=0.000, batch=179\n",
      "825: loss=0.898, reward_mean=0.050, reward_bound=0.000, batch=184\n",
      "826: loss=0.899, reward_mean=0.050, reward_bound=0.000, batch=189\n",
      "827: loss=0.896, reward_mean=0.050, reward_bound=0.000, batch=194\n",
      "828: loss=0.900, reward_mean=0.070, reward_bound=0.000, batch=201\n",
      "829: loss=0.899, reward_mean=0.050, reward_bound=0.000, batch=206\n",
      "830: loss=0.898, reward_mean=0.050, reward_bound=0.000, batch=211\n",
      "831: loss=0.893, reward_mean=0.100, reward_bound=0.072, batch=215\n",
      "832: loss=0.895, reward_mean=0.060, reward_bound=0.080, batch=218\n",
      "833: loss=0.896, reward_mean=0.030, reward_bound=0.000, batch=221\n",
      "834: loss=0.895, reward_mean=0.010, reward_bound=0.000, batch=222\n",
      "835: loss=0.894, reward_mean=0.060, reward_bound=0.082, batch=225\n",
      "836: loss=0.894, reward_mean=0.040, reward_bound=0.101, batch=227\n",
      "837: loss=0.892, reward_mean=0.040, reward_bound=0.109, batch=230\n",
      "838: loss=0.895, reward_mean=0.090, reward_bound=0.150, batch=227\n",
      "839: loss=0.899, reward_mean=0.090, reward_bound=0.167, batch=226\n",
      "840: loss=0.900, reward_mean=0.090, reward_bound=0.206, batch=225\n",
      "841: loss=0.900, reward_mean=0.100, reward_bound=0.229, batch=225\n",
      "842: loss=0.900, reward_mean=0.030, reward_bound=0.171, batch=227\n",
      "843: loss=0.900, reward_mean=0.050, reward_bound=0.254, batch=220\n",
      "844: loss=0.903, reward_mean=0.110, reward_bound=0.282, batch=211\n",
      "845: loss=0.901, reward_mean=0.050, reward_bound=0.000, batch=216\n",
      "846: loss=0.900, reward_mean=0.050, reward_bound=0.127, batch=221\n",
      "847: loss=0.897, reward_mean=0.060, reward_bound=0.254, batch=224\n",
      "848: loss=0.899, reward_mean=0.040, reward_bound=0.275, batch=227\n",
      "849: loss=0.894, reward_mean=0.060, reward_bound=0.314, batch=211\n",
      "850: loss=0.896, reward_mean=0.080, reward_bound=0.167, batch=216\n",
      "851: loss=0.896, reward_mean=0.100, reward_bound=0.314, batch=219\n",
      "852: loss=0.899, reward_mean=0.070, reward_bound=0.278, batch=223\n",
      "853: loss=0.900, reward_mean=0.070, reward_bound=0.314, batch=224\n",
      "854: loss=0.900, reward_mean=0.090, reward_bound=0.345, batch=227\n",
      "855: loss=0.900, reward_mean=0.080, reward_bound=0.349, batch=211\n",
      "856: loss=0.896, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "857: loss=0.895, reward_mean=0.100, reward_bound=0.224, batch=222\n",
      "858: loss=0.900, reward_mean=0.050, reward_bound=0.263, batch=225\n",
      "859: loss=0.904, reward_mean=0.060, reward_bound=0.314, batch=225\n",
      "860: loss=0.902, reward_mean=0.060, reward_bound=0.349, batch=225\n",
      "861: loss=0.905, reward_mean=0.100, reward_bound=0.387, batch=197\n",
      "862: loss=0.908, reward_mean=0.050, reward_bound=0.000, batch=202\n",
      "863: loss=0.904, reward_mean=0.080, reward_bound=0.000, batch=210\n",
      "864: loss=0.906, reward_mean=0.090, reward_bound=0.093, batch=217\n",
      "865: loss=0.905, reward_mean=0.050, reward_bound=0.064, batch=222\n",
      "866: loss=0.907, reward_mean=0.100, reward_bound=0.229, batch=224\n",
      "867: loss=0.905, reward_mean=0.100, reward_bound=0.254, batch=224\n",
      "868: loss=0.903, reward_mean=0.060, reward_bound=0.282, batch=226\n",
      "869: loss=0.899, reward_mean=0.100, reward_bound=0.314, batch=227\n",
      "870: loss=0.900, reward_mean=0.050, reward_bound=0.349, batch=224\n",
      "871: loss=0.899, reward_mean=0.110, reward_bound=0.342, batch=227\n",
      "872: loss=0.897, reward_mean=0.030, reward_bound=0.253, batch=229\n",
      "873: loss=0.899, reward_mean=0.090, reward_bound=0.387, batch=226\n",
      "874: loss=0.898, reward_mean=0.020, reward_bound=0.141, batch=228\n",
      "875: loss=0.897, reward_mean=0.080, reward_bound=0.353, batch=229\n",
      "876: loss=0.898, reward_mean=0.050, reward_bound=0.387, batch=228\n",
      "877: loss=0.904, reward_mean=0.050, reward_bound=0.430, batch=186\n",
      "878: loss=0.910, reward_mean=0.080, reward_bound=0.000, batch=194\n",
      "879: loss=0.913, reward_mean=0.080, reward_bound=0.000, batch=202\n",
      "880: loss=0.913, reward_mean=0.050, reward_bound=0.000, batch=207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881: loss=0.905, reward_mean=0.080, reward_bound=0.052, batch=215\n",
      "882: loss=0.910, reward_mean=0.070, reward_bound=0.124, batch=220\n",
      "883: loss=0.902, reward_mean=0.060, reward_bound=0.157, batch=224\n",
      "884: loss=0.904, reward_mean=0.080, reward_bound=0.204, batch=227\n",
      "885: loss=0.900, reward_mean=0.120, reward_bound=0.229, batch=227\n",
      "886: loss=0.899, reward_mean=0.040, reward_bound=0.254, batch=225\n",
      "887: loss=0.897, reward_mean=0.060, reward_bound=0.234, batch=227\n",
      "888: loss=0.900, reward_mean=0.080, reward_bound=0.254, batch=228\n",
      "889: loss=0.900, reward_mean=0.050, reward_bound=0.282, batch=222\n",
      "890: loss=0.899, reward_mean=0.100, reward_bound=0.349, batch=223\n",
      "891: loss=0.897, reward_mean=0.040, reward_bound=0.159, batch=226\n",
      "892: loss=0.900, reward_mean=0.060, reward_bound=0.316, batch=228\n",
      "893: loss=0.905, reward_mean=0.060, reward_bound=0.387, batch=215\n",
      "894: loss=0.901, reward_mean=0.070, reward_bound=0.127, batch=220\n",
      "895: loss=0.903, reward_mean=0.050, reward_bound=0.205, batch=224\n",
      "896: loss=0.902, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "897: loss=0.904, reward_mean=0.060, reward_bound=0.268, batch=228\n",
      "898: loss=0.903, reward_mean=0.050, reward_bound=0.286, batch=229\n",
      "899: loss=0.902, reward_mean=0.090, reward_bound=0.364, batch=230\n",
      "900: loss=0.901, reward_mean=0.090, reward_bound=0.376, batch=231\n",
      "901: loss=0.903, reward_mean=0.070, reward_bound=0.387, batch=228\n",
      "902: loss=0.903, reward_mean=0.030, reward_bound=0.392, batch=229\n",
      "903: loss=0.901, reward_mean=0.100, reward_bound=0.430, batch=214\n",
      "904: loss=0.902, reward_mean=0.110, reward_bound=0.282, batch=219\n",
      "905: loss=0.902, reward_mean=0.020, reward_bound=0.000, batch=221\n",
      "906: loss=0.902, reward_mean=0.080, reward_bound=0.314, batch=223\n",
      "907: loss=0.900, reward_mean=0.070, reward_bound=0.349, batch=225\n",
      "908: loss=0.901, reward_mean=0.050, reward_bound=0.387, batch=225\n",
      "909: loss=0.900, reward_mean=0.080, reward_bound=0.430, batch=221\n",
      "910: loss=0.898, reward_mean=0.050, reward_bound=0.282, batch=223\n",
      "911: loss=0.898, reward_mean=0.040, reward_bound=0.107, batch=226\n",
      "912: loss=0.898, reward_mean=0.070, reward_bound=0.254, batch=227\n",
      "913: loss=0.899, reward_mean=0.070, reward_bound=0.314, batch=228\n",
      "914: loss=0.899, reward_mean=0.060, reward_bound=0.260, batch=229\n",
      "915: loss=0.899, reward_mean=0.060, reward_bound=0.349, batch=229\n",
      "916: loss=0.900, reward_mean=0.070, reward_bound=0.387, batch=229\n",
      "917: loss=0.901, reward_mean=0.080, reward_bound=0.405, batch=230\n",
      "918: loss=0.901, reward_mean=0.090, reward_bound=0.430, batch=225\n",
      "919: loss=0.902, reward_mean=0.060, reward_bound=0.396, batch=227\n",
      "920: loss=0.902, reward_mean=0.040, reward_bound=0.401, batch=229\n",
      "921: loss=0.902, reward_mean=0.110, reward_bound=0.430, batch=229\n",
      "922: loss=0.902, reward_mean=0.080, reward_bound=0.450, batch=230\n",
      "923: loss=0.916, reward_mean=0.080, reward_bound=0.478, batch=108\n",
      "924: loss=0.904, reward_mean=0.070, reward_bound=0.000, batch=115\n",
      "925: loss=0.906, reward_mean=0.100, reward_bound=0.000, batch=125\n",
      "926: loss=0.905, reward_mean=0.030, reward_bound=0.000, batch=128\n",
      "927: loss=0.904, reward_mean=0.050, reward_bound=0.000, batch=133\n",
      "928: loss=0.895, reward_mean=0.080, reward_bound=0.000, batch=141\n",
      "929: loss=0.894, reward_mean=0.030, reward_bound=0.000, batch=144\n",
      "930: loss=0.896, reward_mean=0.060, reward_bound=0.000, batch=150\n",
      "931: loss=0.891, reward_mean=0.030, reward_bound=0.000, batch=153\n",
      "932: loss=0.890, reward_mean=0.010, reward_bound=0.000, batch=154\n",
      "933: loss=0.887, reward_mean=0.060, reward_bound=0.000, batch=160\n",
      "934: loss=0.887, reward_mean=0.080, reward_bound=0.000, batch=168\n",
      "935: loss=0.885, reward_mean=0.050, reward_bound=0.000, batch=173\n",
      "936: loss=0.883, reward_mean=0.080, reward_bound=0.000, batch=181\n",
      "937: loss=0.883, reward_mean=0.100, reward_bound=0.000, batch=191\n",
      "938: loss=0.884, reward_mean=0.080, reward_bound=0.000, batch=199\n",
      "939: loss=0.885, reward_mean=0.090, reward_bound=0.000, batch=208\n",
      "940: loss=0.884, reward_mean=0.070, reward_bound=0.006, batch=215\n",
      "941: loss=0.881, reward_mean=0.080, reward_bound=0.089, batch=221\n",
      "942: loss=0.886, reward_mean=0.090, reward_bound=0.098, batch=223\n",
      "943: loss=0.885, reward_mean=0.080, reward_bound=0.109, batch=224\n",
      "944: loss=0.882, reward_mean=0.080, reward_bound=0.134, batch=227\n",
      "945: loss=0.881, reward_mean=0.110, reward_bound=0.150, batch=225\n",
      "946: loss=0.883, reward_mean=0.040, reward_bound=0.167, batch=222\n",
      "947: loss=0.878, reward_mean=0.080, reward_bound=0.185, batch=221\n",
      "948: loss=0.878, reward_mean=0.080, reward_bound=0.206, batch=218\n",
      "949: loss=0.877, reward_mean=0.060, reward_bound=0.206, batch=221\n",
      "950: loss=0.874, reward_mean=0.100, reward_bound=0.229, batch=224\n",
      "951: loss=0.880, reward_mean=0.070, reward_bound=0.254, batch=211\n",
      "952: loss=0.882, reward_mean=0.080, reward_bound=0.072, batch=217\n",
      "953: loss=0.882, reward_mean=0.080, reward_bound=0.198, batch=222\n",
      "954: loss=0.880, reward_mean=0.060, reward_bound=0.229, batch=225\n",
      "955: loss=0.881, reward_mean=0.100, reward_bound=0.282, batch=218\n",
      "956: loss=0.880, reward_mean=0.040, reward_bound=0.017, batch=222\n",
      "957: loss=0.879, reward_mean=0.100, reward_bound=0.292, batch=225\n",
      "958: loss=0.880, reward_mean=0.100, reward_bound=0.314, batch=214\n",
      "959: loss=0.877, reward_mean=0.070, reward_bound=0.080, batch=220\n",
      "960: loss=0.878, reward_mean=0.030, reward_bound=0.000, batch=223\n",
      "961: loss=0.874, reward_mean=0.110, reward_bound=0.254, batch=224\n",
      "962: loss=0.871, reward_mean=0.070, reward_bound=0.314, batch=225\n",
      "963: loss=0.874, reward_mean=0.090, reward_bound=0.349, batch=197\n",
      "964: loss=0.870, reward_mean=0.090, reward_bound=0.000, batch=206\n",
      "965: loss=0.868, reward_mean=0.100, reward_bound=0.110, batch=214\n",
      "966: loss=0.866, reward_mean=0.080, reward_bound=0.135, batch=218\n",
      "967: loss=0.868, reward_mean=0.050, reward_bound=0.138, batch=222\n",
      "968: loss=0.870, reward_mean=0.060, reward_bound=0.167, batch=224\n",
      "969: loss=0.875, reward_mean=0.130, reward_bound=0.204, batch=227\n",
      "970: loss=0.870, reward_mean=0.110, reward_bound=0.254, batch=226\n",
      "971: loss=0.867, reward_mean=0.120, reward_bound=0.282, batch=227\n",
      "972: loss=0.866, reward_mean=0.030, reward_bound=0.284, batch=229\n",
      "973: loss=0.874, reward_mean=0.080, reward_bound=0.314, batch=224\n",
      "974: loss=0.873, reward_mean=0.080, reward_bound=0.282, batch=226\n",
      "975: loss=0.876, reward_mean=0.050, reward_bound=0.277, batch=228\n",
      "976: loss=0.873, reward_mean=0.130, reward_bound=0.349, batch=217\n",
      "977: loss=0.875, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "978: loss=0.873, reward_mean=0.090, reward_bound=0.229, batch=224\n",
      "979: loss=0.871, reward_mean=0.080, reward_bound=0.252, batch=227\n",
      "980: loss=0.874, reward_mean=0.070, reward_bound=0.254, batch=228\n",
      "981: loss=0.873, reward_mean=0.010, reward_bound=0.028, batch=229\n",
      "982: loss=0.874, reward_mean=0.050, reward_bound=0.295, batch=230\n",
      "983: loss=0.872, reward_mean=0.090, reward_bound=0.314, batch=229\n",
      "984: loss=0.869, reward_mean=0.100, reward_bound=0.387, batch=197\n",
      "985: loss=0.870, reward_mean=0.050, reward_bound=0.000, batch=202\n",
      "986: loss=0.873, reward_mean=0.060, reward_bound=0.000, batch=208\n",
      "987: loss=0.874, reward_mean=0.090, reward_bound=0.140, batch=215\n",
      "988: loss=0.872, reward_mean=0.090, reward_bound=0.185, batch=218\n",
      "989: loss=0.870, reward_mean=0.050, reward_bound=0.187, batch=222\n",
      "990: loss=0.871, reward_mean=0.080, reward_bound=0.263, batch=225\n",
      "991: loss=0.873, reward_mean=0.080, reward_bound=0.282, batch=226\n",
      "992: loss=0.874, reward_mean=0.070, reward_bound=0.284, batch=228\n",
      "993: loss=0.868, reward_mean=0.050, reward_bound=0.314, batch=225\n",
      "994: loss=0.871, reward_mean=0.070, reward_bound=0.349, batch=226\n",
      "995: loss=0.870, reward_mean=0.080, reward_bound=0.368, batch=228\n",
      "996: loss=0.869, reward_mean=0.050, reward_bound=0.353, batch=229\n",
      "997: loss=0.864, reward_mean=0.060, reward_bound=0.387, batch=221\n",
      "998: loss=0.864, reward_mean=0.110, reward_bound=0.314, batch=224\n",
      "999: loss=0.867, reward_mean=0.090, reward_bound=0.345, batch=227\n",
      "1000: loss=0.864, reward_mean=0.070, reward_bound=0.349, batch=227\n",
      "1001: loss=0.865, reward_mean=0.060, reward_bound=0.387, batch=226\n",
      "1002: loss=0.863, reward_mean=0.060, reward_bound=0.390, batch=228\n",
      "1003: loss=0.877, reward_mean=0.080, reward_bound=0.430, batch=178\n",
      "1004: loss=0.879, reward_mean=0.040, reward_bound=0.000, batch=182\n",
      "1005: loss=0.879, reward_mean=0.030, reward_bound=0.000, batch=185\n",
      "1006: loss=0.875, reward_mean=0.040, reward_bound=0.000, batch=189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007: loss=0.873, reward_mean=0.060, reward_bound=0.000, batch=195\n",
      "1008: loss=0.872, reward_mean=0.020, reward_bound=0.000, batch=197\n",
      "1009: loss=0.866, reward_mean=0.050, reward_bound=0.000, batch=202\n",
      "1010: loss=0.869, reward_mean=0.040, reward_bound=0.000, batch=206\n",
      "1011: loss=0.870, reward_mean=0.040, reward_bound=0.000, batch=210\n",
      "1012: loss=0.870, reward_mean=0.070, reward_bound=0.033, batch=217\n",
      "1013: loss=0.867, reward_mean=0.060, reward_bound=0.046, batch=222\n",
      "1014: loss=0.872, reward_mean=0.060, reward_bound=0.102, batch=225\n",
      "1015: loss=0.872, reward_mean=0.060, reward_bound=0.124, batch=227\n",
      "1016: loss=0.872, reward_mean=0.040, reward_bound=0.122, batch=229\n",
      "1017: loss=0.874, reward_mean=0.050, reward_bound=0.157, batch=230\n",
      "1018: loss=0.875, reward_mean=0.060, reward_bound=0.206, batch=233\n",
      "1019: loss=0.873, reward_mean=0.060, reward_bound=0.206, batch=232\n",
      "1020: loss=0.880, reward_mean=0.130, reward_bound=0.282, batch=223\n",
      "1021: loss=0.877, reward_mean=0.070, reward_bound=0.314, batch=223\n",
      "1022: loss=0.879, reward_mean=0.040, reward_bound=0.261, batch=226\n",
      "1023: loss=0.877, reward_mean=0.040, reward_bound=0.256, batch=228\n",
      "1024: loss=0.878, reward_mean=0.030, reward_bound=0.289, batch=229\n",
      "1025: loss=0.878, reward_mean=0.070, reward_bound=0.349, batch=222\n",
      "1026: loss=0.877, reward_mean=0.120, reward_bound=0.387, batch=213\n",
      "1027: loss=0.878, reward_mean=0.090, reward_bound=0.219, batch=219\n",
      "1028: loss=0.875, reward_mean=0.070, reward_bound=0.175, batch=223\n",
      "1029: loss=0.875, reward_mean=0.040, reward_bound=0.229, batch=225\n",
      "1030: loss=0.868, reward_mean=0.090, reward_bound=0.282, batch=226\n",
      "1031: loss=0.869, reward_mean=0.040, reward_bound=0.314, batch=226\n",
      "1032: loss=0.868, reward_mean=0.040, reward_bound=0.282, batch=227\n",
      "1033: loss=0.873, reward_mean=0.110, reward_bound=0.349, batch=227\n",
      "1034: loss=0.875, reward_mean=0.050, reward_bound=0.325, batch=229\n",
      "1035: loss=0.872, reward_mean=0.060, reward_bound=0.349, batch=229\n",
      "1036: loss=0.874, reward_mean=0.030, reward_bound=0.387, batch=227\n",
      "1037: loss=0.873, reward_mean=0.020, reward_bound=0.344, batch=229\n",
      "1038: loss=0.875, reward_mean=0.050, reward_bound=0.430, batch=211\n",
      "1039: loss=0.874, reward_mean=0.050, reward_bound=0.000, batch=216\n",
      "1040: loss=0.872, reward_mean=0.130, reward_bound=0.331, batch=221\n",
      "1041: loss=0.873, reward_mean=0.040, reward_bound=0.135, batch=224\n",
      "1042: loss=0.868, reward_mean=0.090, reward_bound=0.345, batch=227\n",
      "1043: loss=0.868, reward_mean=0.060, reward_bound=0.349, batch=227\n",
      "1044: loss=0.872, reward_mean=0.060, reward_bound=0.387, batch=223\n",
      "1045: loss=0.871, reward_mean=0.080, reward_bound=0.307, batch=226\n",
      "1046: loss=0.870, reward_mean=0.050, reward_bound=0.331, batch=228\n",
      "1047: loss=0.870, reward_mean=0.060, reward_bound=0.353, batch=229\n",
      "1048: loss=0.870, reward_mean=0.030, reward_bound=0.314, batch=229\n",
      "1049: loss=0.869, reward_mean=0.100, reward_bound=0.405, batch=230\n",
      "1050: loss=0.871, reward_mean=0.070, reward_bound=0.430, batch=225\n",
      "1051: loss=0.870, reward_mean=0.100, reward_bound=0.430, batch=226\n",
      "1052: loss=0.872, reward_mean=0.030, reward_bound=0.084, batch=228\n",
      "1053: loss=0.871, reward_mean=0.050, reward_bound=0.435, batch=229\n",
      "1054: loss=0.870, reward_mean=0.060, reward_bound=0.478, batch=231\n",
      "1055: loss=0.870, reward_mean=0.090, reward_bound=0.430, batch=231\n",
      "1056: loss=0.870, reward_mean=0.070, reward_bound=0.430, batch=231\n",
      "1057: loss=0.895, reward_mean=0.080, reward_bound=0.478, batch=152\n",
      "1058: loss=0.883, reward_mean=0.100, reward_bound=0.000, batch=162\n",
      "1059: loss=0.884, reward_mean=0.100, reward_bound=0.000, batch=172\n",
      "1060: loss=0.883, reward_mean=0.070, reward_bound=0.000, batch=179\n",
      "1061: loss=0.883, reward_mean=0.070, reward_bound=0.000, batch=186\n",
      "1062: loss=0.881, reward_mean=0.040, reward_bound=0.000, batch=190\n",
      "1063: loss=0.880, reward_mean=0.060, reward_bound=0.000, batch=196\n",
      "1064: loss=0.876, reward_mean=0.060, reward_bound=0.000, batch=202\n",
      "1065: loss=0.874, reward_mean=0.050, reward_bound=0.000, batch=207\n",
      "1066: loss=0.867, reward_mean=0.090, reward_bound=0.019, batch=215\n",
      "1067: loss=0.866, reward_mean=0.040, reward_bound=0.000, batch=219\n",
      "1068: loss=0.863, reward_mean=0.020, reward_bound=0.000, batch=221\n",
      "1069: loss=0.870, reward_mean=0.090, reward_bound=0.098, batch=224\n",
      "1070: loss=0.871, reward_mean=0.070, reward_bound=0.149, batch=227\n",
      "1071: loss=0.871, reward_mean=0.020, reward_bound=0.120, batch=229\n",
      "1072: loss=0.875, reward_mean=0.060, reward_bound=0.174, batch=230\n",
      "1073: loss=0.877, reward_mean=0.100, reward_bound=0.200, batch=231\n",
      "1074: loss=0.873, reward_mean=0.060, reward_bound=0.206, batch=228\n",
      "1075: loss=0.877, reward_mean=0.110, reward_bound=0.229, batch=228\n",
      "1076: loss=0.880, reward_mean=0.060, reward_bound=0.254, batch=226\n",
      "1077: loss=0.880, reward_mean=0.070, reward_bound=0.268, batch=228\n",
      "1078: loss=0.875, reward_mean=0.130, reward_bound=0.282, batch=225\n",
      "1079: loss=0.873, reward_mean=0.040, reward_bound=0.314, batch=213\n",
      "1080: loss=0.872, reward_mean=0.050, reward_bound=0.000, batch=218\n",
      "1081: loss=0.873, reward_mean=0.070, reward_bound=0.190, batch=222\n",
      "1082: loss=0.866, reward_mean=0.100, reward_bound=0.282, batch=221\n",
      "1083: loss=0.869, reward_mean=0.040, reward_bound=0.135, batch=224\n",
      "1084: loss=0.873, reward_mean=0.030, reward_bound=0.015, batch=227\n",
      "1085: loss=0.869, reward_mean=0.120, reward_bound=0.314, batch=227\n",
      "1086: loss=0.868, reward_mean=0.050, reward_bound=0.342, batch=229\n",
      "1087: loss=0.870, reward_mean=0.080, reward_bound=0.349, batch=218\n",
      "1088: loss=0.869, reward_mean=0.070, reward_bound=0.234, batch=222\n",
      "1089: loss=0.868, reward_mean=0.060, reward_bound=0.292, batch=225\n",
      "1090: loss=0.867, reward_mean=0.050, reward_bound=0.289, batch=227\n",
      "1091: loss=0.866, reward_mean=0.090, reward_bound=0.349, batch=228\n",
      "1092: loss=0.870, reward_mean=0.050, reward_bound=0.387, batch=207\n",
      "1093: loss=0.871, reward_mean=0.050, reward_bound=0.000, batch=212\n",
      "1094: loss=0.870, reward_mean=0.040, reward_bound=0.000, batch=216\n",
      "1095: loss=0.873, reward_mean=0.070, reward_bound=0.153, batch=221\n",
      "1096: loss=0.872, reward_mean=0.050, reward_bound=0.150, batch=224\n",
      "1097: loss=0.874, reward_mean=0.010, reward_bound=0.000, batch=225\n",
      "1098: loss=0.872, reward_mean=0.060, reward_bound=0.229, batch=225\n",
      "1099: loss=0.874, reward_mean=0.060, reward_bound=0.254, batch=225\n",
      "1100: loss=0.875, reward_mean=0.090, reward_bound=0.282, batch=225\n",
      "1101: loss=0.874, reward_mean=0.050, reward_bound=0.246, batch=227\n",
      "1102: loss=0.873, reward_mean=0.070, reward_bound=0.314, batch=220\n",
      "1103: loss=0.871, reward_mean=0.050, reward_bound=0.313, batch=224\n",
      "1104: loss=0.869, reward_mean=0.030, reward_bound=0.122, batch=227\n",
      "1105: loss=0.871, reward_mean=0.060, reward_bound=0.198, batch=229\n",
      "1106: loss=0.872, reward_mean=0.040, reward_bound=0.215, batch=230\n",
      "1107: loss=0.875, reward_mean=0.040, reward_bound=0.247, batch=231\n",
      "1108: loss=0.873, reward_mean=0.110, reward_bound=0.314, batch=230\n",
      "1109: loss=0.875, reward_mean=0.060, reward_bound=0.349, batch=226\n",
      "1110: loss=0.873, reward_mean=0.080, reward_bound=0.368, batch=228\n",
      "1111: loss=0.875, reward_mean=0.040, reward_bound=0.241, batch=229\n",
      "1112: loss=0.872, reward_mean=0.050, reward_bound=0.364, batch=230\n",
      "1113: loss=0.873, reward_mean=0.040, reward_bound=0.356, batch=231\n",
      "1114: loss=0.874, reward_mean=0.050, reward_bound=0.349, batch=231\n",
      "1115: loss=0.873, reward_mean=0.060, reward_bound=0.387, batch=229\n",
      "1116: loss=0.886, reward_mean=0.130, reward_bound=0.430, batch=195\n",
      "1117: loss=0.878, reward_mean=0.080, reward_bound=0.000, batch=203\n",
      "1118: loss=0.878, reward_mean=0.030, reward_bound=0.000, batch=206\n",
      "1119: loss=0.882, reward_mean=0.040, reward_bound=0.000, batch=210\n",
      "1120: loss=0.883, reward_mean=0.050, reward_bound=0.000, batch=215\n",
      "1121: loss=0.875, reward_mean=0.110, reward_bound=0.109, batch=221\n",
      "1122: loss=0.883, reward_mean=0.090, reward_bound=0.167, batch=222\n",
      "1123: loss=0.881, reward_mean=0.050, reward_bound=0.185, batch=224\n",
      "1124: loss=0.878, reward_mean=0.070, reward_bound=0.229, batch=225\n",
      "1125: loss=0.878, reward_mean=0.040, reward_bound=0.254, batch=222\n",
      "1126: loss=0.876, reward_mean=0.080, reward_bound=0.282, batch=224\n",
      "1127: loss=0.875, reward_mean=0.050, reward_bound=0.206, batch=226\n",
      "1128: loss=0.877, reward_mean=0.020, reward_bound=0.055, batch=228\n",
      "1129: loss=0.875, reward_mean=0.070, reward_bound=0.314, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130: loss=0.875, reward_mean=0.070, reward_bound=0.314, batch=224\n",
      "1131: loss=0.872, reward_mean=0.100, reward_bound=0.349, batch=224\n",
      "1132: loss=0.870, reward_mean=0.060, reward_bound=0.282, batch=226\n",
      "1133: loss=0.871, reward_mean=0.060, reward_bound=0.331, batch=228\n",
      "1134: loss=0.880, reward_mean=0.110, reward_bound=0.387, batch=226\n",
      "1135: loss=0.878, reward_mean=0.070, reward_bound=0.430, batch=214\n",
      "1136: loss=0.878, reward_mean=0.040, reward_bound=0.000, batch=218\n",
      "1137: loss=0.875, reward_mean=0.060, reward_bound=0.128, batch=222\n",
      "1138: loss=0.879, reward_mean=0.120, reward_bound=0.229, batch=223\n",
      "1139: loss=0.876, reward_mean=0.060, reward_bound=0.207, batch=226\n",
      "1140: loss=0.874, reward_mean=0.040, reward_bound=0.153, batch=228\n",
      "1141: loss=0.877, reward_mean=0.060, reward_bound=0.254, batch=228\n",
      "1142: loss=0.873, reward_mean=0.080, reward_bound=0.282, batch=227\n",
      "1143: loss=0.875, reward_mean=0.100, reward_bound=0.349, batch=228\n",
      "1144: loss=0.874, reward_mean=0.050, reward_bound=0.387, batch=223\n",
      "1145: loss=0.873, reward_mean=0.060, reward_bound=0.387, batch=225\n",
      "1146: loss=0.870, reward_mean=0.030, reward_bound=0.246, batch=227\n",
      "1147: loss=0.869, reward_mean=0.010, reward_bound=0.000, batch=228\n",
      "1148: loss=0.875, reward_mean=0.080, reward_bound=0.353, batch=229\n",
      "1149: loss=0.876, reward_mean=0.050, reward_bound=0.343, batch=230\n",
      "1150: loss=0.876, reward_mean=0.060, reward_bound=0.376, batch=231\n",
      "1151: loss=0.874, reward_mean=0.070, reward_bound=0.430, batch=224\n",
      "1152: loss=0.874, reward_mean=0.050, reward_bound=0.247, batch=227\n",
      "1153: loss=0.874, reward_mean=0.070, reward_bound=0.308, batch=229\n",
      "1154: loss=0.874, reward_mean=0.030, reward_bound=0.263, batch=230\n",
      "1155: loss=0.874, reward_mean=0.050, reward_bound=0.338, batch=231\n",
      "1156: loss=0.874, reward_mean=0.050, reward_bound=0.349, batch=229\n",
      "1157: loss=0.874, reward_mean=0.070, reward_bound=0.387, batch=229\n",
      "1158: loss=0.874, reward_mean=0.080, reward_bound=0.405, batch=230\n",
      "1159: loss=0.876, reward_mean=0.160, reward_bound=0.430, batch=229\n",
      "1160: loss=0.877, reward_mean=0.040, reward_bound=0.424, batch=230\n",
      "1161: loss=0.888, reward_mean=0.080, reward_bound=0.478, batch=185\n",
      "1162: loss=0.886, reward_mean=0.030, reward_bound=0.000, batch=188\n",
      "1163: loss=0.888, reward_mean=0.040, reward_bound=0.000, batch=192\n",
      "1164: loss=0.885, reward_mean=0.110, reward_bound=0.000, batch=203\n",
      "1165: loss=0.880, reward_mean=0.030, reward_bound=0.000, batch=206\n",
      "1166: loss=0.878, reward_mean=0.110, reward_bound=0.135, batch=213\n",
      "1167: loss=0.877, reward_mean=0.090, reward_bound=0.167, batch=218\n",
      "1168: loss=0.874, reward_mean=0.050, reward_bound=0.098, batch=222\n",
      "1169: loss=0.871, reward_mean=0.070, reward_bound=0.206, batch=226\n",
      "1170: loss=0.876, reward_mean=0.060, reward_bound=0.217, batch=228\n",
      "1171: loss=0.873, reward_mean=0.080, reward_bound=0.231, batch=229\n",
      "1172: loss=0.873, reward_mean=0.060, reward_bound=0.265, batch=230\n",
      "1173: loss=0.875, reward_mean=0.080, reward_bound=0.282, batch=227\n",
      "1174: loss=0.874, reward_mean=0.030, reward_bound=0.292, batch=229\n",
      "1175: loss=0.879, reward_mean=0.050, reward_bound=0.314, batch=222\n",
      "1176: loss=0.873, reward_mean=0.090, reward_bound=0.349, batch=217\n",
      "1177: loss=0.876, reward_mean=0.090, reward_bound=0.342, batch=222\n",
      "1178: loss=0.873, reward_mean=0.140, reward_bound=0.360, batch=225\n",
      "1179: loss=0.873, reward_mean=0.030, reward_bound=0.289, batch=227\n",
      "1180: loss=0.874, reward_mean=0.050, reward_bound=0.254, batch=228\n",
      "1181: loss=0.874, reward_mean=0.070, reward_bound=0.257, batch=229\n",
      "1182: loss=0.873, reward_mean=0.090, reward_bound=0.282, batch=229\n",
      "1183: loss=0.873, reward_mean=0.030, reward_bound=0.295, batch=230\n",
      "1184: loss=0.875, reward_mean=0.090, reward_bound=0.376, batch=231\n",
      "1185: loss=0.880, reward_mean=0.050, reward_bound=0.387, batch=221\n",
      "1186: loss=0.878, reward_mean=0.090, reward_bound=0.314, batch=222\n",
      "1187: loss=0.879, reward_mean=0.080, reward_bound=0.387, batch=224\n",
      "1188: loss=0.881, reward_mean=0.060, reward_bound=0.252, batch=227\n",
      "1189: loss=0.879, reward_mean=0.040, reward_bound=0.240, batch=229\n",
      "1190: loss=0.878, reward_mean=0.090, reward_bound=0.343, batch=230\n",
      "1191: loss=0.877, reward_mean=0.030, reward_bound=0.347, batch=231\n",
      "1192: loss=0.878, reward_mean=0.040, reward_bound=0.387, batch=231\n",
      "1193: loss=0.884, reward_mean=0.060, reward_bound=0.430, batch=210\n",
      "1194: loss=0.884, reward_mean=0.020, reward_bound=0.000, batch=212\n",
      "1195: loss=0.883, reward_mean=0.060, reward_bound=0.033, batch=218\n",
      "1196: loss=0.877, reward_mean=0.050, reward_bound=0.109, batch=221\n",
      "1197: loss=0.875, reward_mean=0.060, reward_bound=0.206, batch=224\n",
      "1198: loss=0.874, reward_mean=0.080, reward_bound=0.280, batch=227\n",
      "1199: loss=0.880, reward_mean=0.050, reward_bound=0.282, batch=228\n",
      "1200: loss=0.885, reward_mean=0.070, reward_bound=0.349, batch=225\n",
      "1201: loss=0.885, reward_mean=0.000, reward_bound=0.000, batch=225\n",
      "1202: loss=0.884, reward_mean=0.040, reward_bound=0.296, batch=227\n",
      "1203: loss=0.883, reward_mean=0.050, reward_bound=0.342, batch=229\n",
      "1204: loss=0.885, reward_mean=0.030, reward_bound=0.364, batch=230\n",
      "1205: loss=0.881, reward_mean=0.080, reward_bound=0.387, batch=227\n",
      "1206: loss=0.880, reward_mean=0.030, reward_bound=0.098, batch=229\n",
      "1207: loss=0.881, reward_mean=0.070, reward_bound=0.387, batch=229\n",
      "1208: loss=0.880, reward_mean=0.050, reward_bound=0.360, batch=230\n",
      "1209: loss=0.880, reward_mean=0.080, reward_bound=0.430, batch=222\n",
      "1210: loss=0.878, reward_mean=0.060, reward_bound=0.236, batch=225\n",
      "1211: loss=0.878, reward_mean=0.020, reward_bound=0.051, batch=227\n",
      "1212: loss=0.880, reward_mean=0.080, reward_bound=0.342, batch=229\n",
      "1213: loss=0.884, reward_mean=0.090, reward_bound=0.349, batch=228\n",
      "1214: loss=0.885, reward_mean=0.060, reward_bound=0.392, batch=229\n",
      "1215: loss=0.885, reward_mean=0.070, reward_bound=0.430, batch=227\n",
      "1216: loss=0.885, reward_mean=0.060, reward_bound=0.349, batch=228\n",
      "1217: loss=0.886, reward_mean=0.030, reward_bound=0.211, batch=229\n",
      "1218: loss=0.884, reward_mean=0.090, reward_bound=0.278, batch=230\n",
      "1219: loss=0.884, reward_mean=0.030, reward_bound=0.254, batch=230\n",
      "1220: loss=0.886, reward_mean=0.040, reward_bound=0.338, batch=231\n",
      "1221: loss=0.885, reward_mean=0.040, reward_bound=0.349, batch=231\n",
      "1222: loss=0.885, reward_mean=0.090, reward_bound=0.387, batch=231\n",
      "1223: loss=0.885, reward_mean=0.040, reward_bound=0.387, batch=231\n",
      "1224: loss=0.884, reward_mean=0.060, reward_bound=0.430, batch=229\n",
      "1225: loss=0.883, reward_mean=0.020, reward_bound=0.155, batch=230\n",
      "1226: loss=0.885, reward_mean=0.050, reward_bound=0.356, batch=231\n",
      "1227: loss=0.884, reward_mean=0.070, reward_bound=0.387, batch=231\n",
      "1228: loss=0.884, reward_mean=0.070, reward_bound=0.430, batch=230\n",
      "1229: loss=0.883, reward_mean=0.100, reward_bound=0.464, batch=231\n",
      "1230: loss=0.887, reward_mean=0.070, reward_bound=0.478, batch=214\n",
      "1231: loss=0.888, reward_mean=0.060, reward_bound=0.135, batch=220\n",
      "1232: loss=0.889, reward_mean=0.060, reward_bound=0.121, batch=224\n",
      "1233: loss=0.888, reward_mean=0.070, reward_bound=0.150, batch=225\n",
      "1234: loss=0.888, reward_mean=0.030, reward_bound=0.103, batch=227\n",
      "1235: loss=0.887, reward_mean=0.070, reward_bound=0.229, batch=228\n",
      "1236: loss=0.888, reward_mean=0.050, reward_bound=0.282, batch=228\n",
      "1237: loss=0.884, reward_mean=0.060, reward_bound=0.349, batch=227\n",
      "1238: loss=0.886, reward_mean=0.080, reward_bound=0.387, batch=227\n",
      "1239: loss=0.883, reward_mean=0.070, reward_bound=0.277, batch=229\n",
      "1240: loss=0.884, reward_mean=0.040, reward_bound=0.213, batch=230\n",
      "1241: loss=0.887, reward_mean=0.060, reward_bound=0.430, batch=225\n",
      "1242: loss=0.888, reward_mean=0.070, reward_bound=0.387, batch=226\n",
      "1243: loss=0.888, reward_mean=0.080, reward_bound=0.314, batch=227\n",
      "1244: loss=0.888, reward_mean=0.070, reward_bound=0.349, batch=228\n",
      "1245: loss=0.888, reward_mean=0.090, reward_bound=0.387, batch=228\n",
      "1246: loss=0.886, reward_mean=0.050, reward_bound=0.430, batch=228\n",
      "1247: loss=0.885, reward_mean=0.080, reward_bound=0.430, batch=228\n",
      "1248: loss=0.885, reward_mean=0.060, reward_bound=0.297, batch=229\n",
      "1249: loss=0.885, reward_mean=0.040, reward_bound=0.405, batch=230\n",
      "1250: loss=0.885, reward_mean=0.060, reward_bound=0.418, batch=231\n",
      "1251: loss=0.885, reward_mean=0.060, reward_bound=0.314, batch=231\n",
      "1252: loss=0.887, reward_mean=0.030, reward_bound=0.430, batch=231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1253: loss=0.887, reward_mean=0.060, reward_bound=0.430, batch=231\n",
      "1254: loss=0.887, reward_mean=0.050, reward_bound=0.430, batch=231\n",
      "1255: loss=0.889, reward_mean=0.040, reward_bound=0.478, batch=225\n",
      "1256: loss=0.890, reward_mean=0.030, reward_bound=0.052, batch=227\n",
      "1257: loss=0.890, reward_mean=0.040, reward_bound=0.150, batch=228\n",
      "1258: loss=0.890, reward_mean=0.050, reward_bound=0.353, batch=229\n",
      "1259: loss=0.889, reward_mean=0.080, reward_bound=0.405, batch=230\n",
      "1260: loss=0.889, reward_mean=0.040, reward_bound=0.418, batch=231\n",
      "1261: loss=0.887, reward_mean=0.080, reward_bound=0.430, batch=228\n",
      "1262: loss=0.887, reward_mean=0.130, reward_bound=0.387, batch=228\n",
      "1263: loss=0.886, reward_mean=0.080, reward_bound=0.435, batch=229\n",
      "1264: loss=0.885, reward_mean=0.040, reward_bound=0.364, batch=230\n",
      "1265: loss=0.886, reward_mean=0.080, reward_bound=0.418, batch=231\n",
      "1266: loss=0.885, reward_mean=0.080, reward_bound=0.430, batch=230\n",
      "1267: loss=0.886, reward_mean=0.070, reward_bound=0.451, batch=231\n",
      "1268: loss=0.889, reward_mean=0.100, reward_bound=0.478, batch=227\n",
      "1269: loss=0.888, reward_mean=0.090, reward_bound=0.422, batch=229\n",
      "1270: loss=0.888, reward_mean=0.070, reward_bound=0.349, batch=229\n",
      "1271: loss=0.887, reward_mean=0.060, reward_bound=0.405, batch=230\n",
      "1272: loss=0.886, reward_mean=0.060, reward_bound=0.338, batch=231\n",
      "1273: loss=0.886, reward_mean=0.060, reward_bound=0.349, batch=231\n",
      "1274: loss=0.888, reward_mean=0.110, reward_bound=0.430, batch=231\n",
      "1275: loss=0.888, reward_mean=0.030, reward_bound=0.478, batch=231\n",
      "1276: loss=0.888, reward_mean=0.050, reward_bound=0.478, batch=231\n",
      "1277: loss=0.888, reward_mean=0.070, reward_bound=0.478, batch=231\n",
      "1278: loss=0.888, reward_mean=0.010, reward_bound=0.387, batch=231\n",
      "1279: loss=0.888, reward_mean=0.090, reward_bound=0.478, batch=231\n",
      "1280: loss=0.888, reward_mean=0.100, reward_bound=0.430, batch=231\n",
      "1281: loss=0.888, reward_mean=0.060, reward_bound=0.478, batch=231\n",
      "1282: loss=0.888, reward_mean=0.090, reward_bound=0.430, batch=231\n",
      "1283: loss=0.888, reward_mean=0.060, reward_bound=0.387, batch=231\n",
      "1285: loss=0.881, reward_mean=0.070, reward_bound=0.000, batch=7\n",
      "1286: loss=0.855, reward_mean=0.060, reward_bound=0.000, batch=13\n",
      "1287: loss=0.843, reward_mean=0.040, reward_bound=0.000, batch=17\n",
      "1288: loss=0.851, reward_mean=0.110, reward_bound=0.000, batch=28\n",
      "1289: loss=0.842, reward_mean=0.050, reward_bound=0.000, batch=33\n",
      "1290: loss=0.841, reward_mean=0.030, reward_bound=0.000, batch=36\n",
      "1291: loss=0.853, reward_mean=0.060, reward_bound=0.000, batch=42\n",
      "1292: loss=0.859, reward_mean=0.060, reward_bound=0.000, batch=48\n",
      "1293: loss=0.863, reward_mean=0.090, reward_bound=0.000, batch=57\n",
      "1294: loss=0.852, reward_mean=0.080, reward_bound=0.000, batch=65\n",
      "1295: loss=0.855, reward_mean=0.040, reward_bound=0.000, batch=69\n",
      "1296: loss=0.855, reward_mean=0.060, reward_bound=0.000, batch=75\n",
      "1297: loss=0.847, reward_mean=0.080, reward_bound=0.000, batch=83\n",
      "1298: loss=0.851, reward_mean=0.060, reward_bound=0.000, batch=89\n",
      "1299: loss=0.856, reward_mean=0.060, reward_bound=0.000, batch=95\n",
      "1300: loss=0.854, reward_mean=0.050, reward_bound=0.000, batch=100\n",
      "1301: loss=0.853, reward_mean=0.050, reward_bound=0.000, batch=105\n",
      "1302: loss=0.854, reward_mean=0.040, reward_bound=0.000, batch=109\n",
      "1303: loss=0.856, reward_mean=0.050, reward_bound=0.000, batch=114\n",
      "1304: loss=0.854, reward_mean=0.080, reward_bound=0.000, batch=122\n",
      "1305: loss=0.854, reward_mean=0.090, reward_bound=0.000, batch=131\n",
      "1306: loss=0.858, reward_mean=0.070, reward_bound=0.000, batch=138\n",
      "1307: loss=0.862, reward_mean=0.050, reward_bound=0.000, batch=143\n",
      "1308: loss=0.860, reward_mean=0.090, reward_bound=0.000, batch=152\n",
      "1309: loss=0.856, reward_mean=0.080, reward_bound=0.000, batch=160\n",
      "1310: loss=0.851, reward_mean=0.140, reward_bound=0.000, batch=174\n",
      "1311: loss=0.852, reward_mean=0.040, reward_bound=0.000, batch=178\n",
      "1312: loss=0.851, reward_mean=0.070, reward_bound=0.000, batch=185\n",
      "1313: loss=0.853, reward_mean=0.090, reward_bound=0.000, batch=194\n",
      "1314: loss=0.853, reward_mean=0.060, reward_bound=0.000, batch=200\n",
      "1315: loss=0.851, reward_mean=0.040, reward_bound=0.000, batch=204\n",
      "1316: loss=0.849, reward_mean=0.050, reward_bound=0.000, batch=209\n",
      "1317: loss=0.852, reward_mean=0.050, reward_bound=0.000, batch=214\n",
      "1318: loss=0.850, reward_mean=0.040, reward_bound=0.000, batch=218\n",
      "1319: loss=0.851, reward_mean=0.030, reward_bound=0.000, batch=221\n",
      "1320: loss=0.849, reward_mean=0.070, reward_bound=0.042, batch=224\n",
      "1321: loss=0.853, reward_mean=0.070, reward_bound=0.058, batch=227\n",
      "1322: loss=0.854, reward_mean=0.060, reward_bound=0.072, batch=228\n",
      "1323: loss=0.854, reward_mean=0.040, reward_bound=0.098, batch=228\n",
      "1324: loss=0.855, reward_mean=0.060, reward_bound=0.111, batch=229\n",
      "1325: loss=0.852, reward_mean=0.050, reward_bound=0.135, batch=227\n",
      "1326: loss=0.858, reward_mean=0.090, reward_bound=0.150, batch=225\n",
      "1327: loss=0.858, reward_mean=0.070, reward_bound=0.167, batch=226\n",
      "1328: loss=0.857, reward_mean=0.110, reward_bound=0.185, batch=224\n",
      "1329: loss=0.855, reward_mean=0.080, reward_bound=0.206, batch=223\n",
      "1330: loss=0.865, reward_mean=0.090, reward_bound=0.229, batch=209\n",
      "1331: loss=0.857, reward_mean=0.040, reward_bound=0.000, batch=213\n",
      "1332: loss=0.860, reward_mean=0.080, reward_bound=0.120, batch=219\n",
      "1333: loss=0.859, reward_mean=0.060, reward_bound=0.141, batch=223\n",
      "1334: loss=0.861, reward_mean=0.060, reward_bound=0.206, batch=225\n",
      "1335: loss=0.865, reward_mean=0.110, reward_bound=0.254, batch=211\n",
      "1336: loss=0.863, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "1337: loss=0.863, reward_mean=0.090, reward_bound=0.163, batch=222\n",
      "1338: loss=0.860, reward_mean=0.070, reward_bound=0.213, batch=225\n",
      "1339: loss=0.860, reward_mean=0.070, reward_bound=0.194, batch=227\n",
      "1340: loss=0.861, reward_mean=0.050, reward_bound=0.229, batch=227\n",
      "1341: loss=0.870, reward_mean=0.070, reward_bound=0.282, batch=198\n",
      "1342: loss=0.866, reward_mean=0.060, reward_bound=0.000, batch=204\n",
      "1343: loss=0.868, reward_mean=0.100, reward_bound=0.150, batch=212\n",
      "1344: loss=0.866, reward_mean=0.080, reward_bound=0.155, batch=218\n",
      "1345: loss=0.862, reward_mean=0.090, reward_bound=0.185, batch=221\n",
      "1346: loss=0.859, reward_mean=0.050, reward_bound=0.135, batch=224\n",
      "1347: loss=0.858, reward_mean=0.090, reward_bound=0.280, batch=227\n",
      "1348: loss=0.859, reward_mean=0.090, reward_bound=0.277, batch=229\n",
      "1349: loss=0.862, reward_mean=0.090, reward_bound=0.295, batch=230\n",
      "1350: loss=0.855, reward_mean=0.070, reward_bound=0.314, batch=191\n",
      "1351: loss=0.858, reward_mean=0.060, reward_bound=0.000, batch=197\n",
      "1352: loss=0.860, reward_mean=0.060, reward_bound=0.000, batch=203\n",
      "1353: loss=0.858, reward_mean=0.070, reward_bound=0.000, batch=210\n",
      "1354: loss=0.859, reward_mean=0.080, reward_bound=0.088, batch=217\n",
      "1355: loss=0.854, reward_mean=0.080, reward_bound=0.147, batch=222\n",
      "1356: loss=0.853, reward_mean=0.090, reward_bound=0.206, batch=227\n",
      "1357: loss=0.854, reward_mean=0.050, reward_bound=0.206, batch=227\n",
      "1358: loss=0.854, reward_mean=0.050, reward_bound=0.224, batch=229\n",
      "1359: loss=0.855, reward_mean=0.070, reward_bound=0.254, batch=227\n",
      "1360: loss=0.852, reward_mean=0.090, reward_bound=0.282, batch=225\n",
      "1361: loss=0.855, reward_mean=0.080, reward_bound=0.289, batch=227\n",
      "1362: loss=0.856, reward_mean=0.090, reward_bound=0.314, batch=223\n",
      "1363: loss=0.853, reward_mean=0.090, reward_bound=0.349, batch=168\n",
      "1364: loss=0.855, reward_mean=0.080, reward_bound=0.000, batch=176\n",
      "1365: loss=0.858, reward_mean=0.060, reward_bound=0.000, batch=182\n",
      "1366: loss=0.857, reward_mean=0.100, reward_bound=0.000, batch=192\n",
      "1367: loss=0.855, reward_mean=0.080, reward_bound=0.000, batch=200\n",
      "1368: loss=0.855, reward_mean=0.070, reward_bound=0.000, batch=207\n",
      "1369: loss=0.855, reward_mean=0.100, reward_bound=0.070, batch=215\n",
      "1370: loss=0.851, reward_mean=0.080, reward_bound=0.101, batch=220\n",
      "1371: loss=0.847, reward_mean=0.140, reward_bound=0.185, batch=220\n",
      "1372: loss=0.847, reward_mean=0.080, reward_bound=0.185, batch=223\n",
      "1373: loss=0.846, reward_mean=0.080, reward_bound=0.229, batch=223\n",
      "1374: loss=0.844, reward_mean=0.070, reward_bound=0.254, batch=224\n",
      "1375: loss=0.846, reward_mean=0.080, reward_bound=0.204, batch=227\n",
      "1376: loss=0.844, reward_mean=0.060, reward_bound=0.282, batch=222\n",
      "1377: loss=0.845, reward_mean=0.070, reward_bound=0.263, batch=225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378: loss=0.845, reward_mean=0.090, reward_bound=0.314, batch=215\n",
      "1379: loss=0.842, reward_mean=0.070, reward_bound=0.103, batch=220\n",
      "1380: loss=0.842, reward_mean=0.040, reward_bound=0.069, batch=224\n",
      "1381: loss=0.844, reward_mean=0.090, reward_bound=0.185, batch=226\n",
      "1382: loss=0.844, reward_mean=0.070, reward_bound=0.316, batch=228\n",
      "1383: loss=0.848, reward_mean=0.110, reward_bound=0.349, batch=220\n",
      "1384: loss=0.846, reward_mean=0.090, reward_bound=0.376, batch=224\n",
      "1385: loss=0.847, reward_mean=0.060, reward_bound=0.367, batch=227\n",
      "1386: loss=0.845, reward_mean=0.050, reward_bound=0.347, batch=229\n",
      "1387: loss=0.850, reward_mean=0.100, reward_bound=0.387, batch=159\n",
      "1388: loss=0.849, reward_mean=0.050, reward_bound=0.000, batch=164\n",
      "1389: loss=0.852, reward_mean=0.110, reward_bound=0.000, batch=175\n",
      "1390: loss=0.841, reward_mean=0.090, reward_bound=0.000, batch=184\n",
      "1391: loss=0.839, reward_mean=0.100, reward_bound=0.000, batch=194\n",
      "1392: loss=0.839, reward_mean=0.070, reward_bound=0.000, batch=201\n",
      "1393: loss=0.839, reward_mean=0.060, reward_bound=0.000, batch=207\n",
      "1394: loss=0.839, reward_mean=0.030, reward_bound=0.000, batch=210\n",
      "1395: loss=0.842, reward_mean=0.100, reward_bound=0.079, batch=217\n",
      "1396: loss=0.841, reward_mean=0.070, reward_bound=0.132, batch=222\n",
      "1397: loss=0.841, reward_mean=0.120, reward_bound=0.167, batch=223\n",
      "1398: loss=0.837, reward_mean=0.080, reward_bound=0.185, batch=225\n",
      "1399: loss=0.836, reward_mean=0.090, reward_bound=0.210, batch=227\n",
      "1400: loss=0.843, reward_mean=0.060, reward_bound=0.229, batch=226\n",
      "1401: loss=0.847, reward_mean=0.080, reward_bound=0.254, batch=223\n",
      "1402: loss=0.848, reward_mean=0.110, reward_bound=0.282, batch=220\n",
      "1403: loss=0.855, reward_mean=0.060, reward_bound=0.314, batch=212\n",
      "1404: loss=0.856, reward_mean=0.090, reward_bound=0.220, batch=218\n",
      "1405: loss=0.856, reward_mean=0.040, reward_bound=0.025, batch=222\n",
      "1406: loss=0.856, reward_mean=0.080, reward_bound=0.282, batch=223\n",
      "1407: loss=0.856, reward_mean=0.100, reward_bound=0.314, batch=224\n",
      "1408: loss=0.851, reward_mean=0.120, reward_bound=0.349, batch=218\n",
      "1409: loss=0.855, reward_mean=0.050, reward_bound=0.075, batch=222\n",
      "1410: loss=0.855, reward_mean=0.060, reward_bound=0.109, batch=224\n",
      "1411: loss=0.853, reward_mean=0.140, reward_bound=0.349, batch=225\n",
      "1412: loss=0.852, reward_mean=0.060, reward_bound=0.349, batch=226\n",
      "1413: loss=0.852, reward_mean=0.070, reward_bound=0.314, batch=227\n",
      "1414: loss=0.853, reward_mean=0.100, reward_bound=0.387, batch=209\n",
      "1415: loss=0.850, reward_mean=0.120, reward_bound=0.215, batch=216\n",
      "1416: loss=0.854, reward_mean=0.110, reward_bound=0.282, batch=219\n",
      "1417: loss=0.854, reward_mean=0.120, reward_bound=0.314, batch=222\n",
      "1418: loss=0.853, reward_mean=0.040, reward_bound=0.179, batch=225\n",
      "1419: loss=0.851, reward_mean=0.080, reward_bound=0.229, batch=226\n",
      "1420: loss=0.846, reward_mean=0.060, reward_bound=0.368, batch=228\n",
      "1421: loss=0.847, reward_mean=0.050, reward_bound=0.353, batch=229\n",
      "1422: loss=0.848, reward_mean=0.050, reward_bound=0.387, batch=227\n",
      "1423: loss=0.846, reward_mean=0.040, reward_bound=0.302, batch=229\n",
      "1424: loss=0.877, reward_mean=0.100, reward_bound=0.430, batch=122\n",
      "1425: loss=0.872, reward_mean=0.060, reward_bound=0.000, batch=128\n",
      "1426: loss=0.871, reward_mean=0.040, reward_bound=0.000, batch=132\n",
      "1427: loss=0.860, reward_mean=0.100, reward_bound=0.000, batch=142\n",
      "1428: loss=0.859, reward_mean=0.080, reward_bound=0.000, batch=150\n",
      "1429: loss=0.856, reward_mean=0.080, reward_bound=0.000, batch=158\n",
      "1430: loss=0.853, reward_mean=0.040, reward_bound=0.000, batch=162\n",
      "1431: loss=0.852, reward_mean=0.060, reward_bound=0.000, batch=168\n",
      "1432: loss=0.846, reward_mean=0.110, reward_bound=0.000, batch=179\n",
      "1433: loss=0.844, reward_mean=0.070, reward_bound=0.000, batch=186\n",
      "1434: loss=0.848, reward_mean=0.080, reward_bound=0.000, batch=194\n",
      "1435: loss=0.843, reward_mean=0.120, reward_bound=0.034, batch=206\n",
      "1436: loss=0.840, reward_mean=0.060, reward_bound=0.000, batch=212\n",
      "1437: loss=0.845, reward_mean=0.130, reward_bound=0.069, batch=218\n",
      "1438: loss=0.846, reward_mean=0.070, reward_bound=0.081, batch=222\n",
      "1439: loss=0.849, reward_mean=0.100, reward_bound=0.109, batch=224\n",
      "1440: loss=0.854, reward_mean=0.080, reward_bound=0.135, batch=224\n",
      "1441: loss=0.852, reward_mean=0.070, reward_bound=0.165, batch=227\n",
      "1442: loss=0.855, reward_mean=0.110, reward_bound=0.206, batch=224\n",
      "1443: loss=0.858, reward_mean=0.110, reward_bound=0.229, batch=222\n",
      "1444: loss=0.858, reward_mean=0.070, reward_bound=0.254, batch=221\n",
      "1445: loss=0.856, reward_mean=0.040, reward_bound=0.206, batch=224\n",
      "1446: loss=0.857, reward_mean=0.050, reward_bound=0.263, batch=227\n",
      "1447: loss=0.856, reward_mean=0.080, reward_bound=0.282, batch=220\n",
      "1448: loss=0.855, reward_mean=0.040, reward_bound=0.220, batch=224\n",
      "1449: loss=0.856, reward_mean=0.060, reward_bound=0.311, batch=227\n",
      "1450: loss=0.855, reward_mean=0.090, reward_bound=0.314, batch=215\n",
      "1451: loss=0.856, reward_mean=0.090, reward_bound=0.167, batch=219\n",
      "1452: loss=0.854, reward_mean=0.080, reward_bound=0.239, batch=223\n",
      "1453: loss=0.858, reward_mean=0.100, reward_bound=0.335, batch=226\n",
      "1454: loss=0.863, reward_mean=0.100, reward_bound=0.349, batch=193\n",
      "1455: loss=0.857, reward_mean=0.060, reward_bound=0.000, batch=199\n",
      "1456: loss=0.852, reward_mean=0.070, reward_bound=0.000, batch=206\n",
      "1457: loss=0.850, reward_mean=0.040, reward_bound=0.000, batch=210\n",
      "1458: loss=0.854, reward_mean=0.130, reward_bound=0.229, batch=216\n",
      "1459: loss=0.851, reward_mean=0.050, reward_bound=0.015, batch=221\n",
      "1460: loss=0.856, reward_mean=0.070, reward_bound=0.206, batch=223\n",
      "1461: loss=0.857, reward_mean=0.080, reward_bound=0.254, batch=225\n",
      "1462: loss=0.856, reward_mean=0.080, reward_bound=0.260, batch=227\n",
      "1463: loss=0.856, reward_mean=0.070, reward_bound=0.267, batch=229\n",
      "1464: loss=0.859, reward_mean=0.090, reward_bound=0.282, batch=229\n",
      "1465: loss=0.861, reward_mean=0.110, reward_bound=0.314, batch=228\n",
      "1466: loss=0.859, reward_mean=0.040, reward_bound=0.349, batch=224\n",
      "1467: loss=0.859, reward_mean=0.060, reward_bound=0.273, batch=227\n",
      "1468: loss=0.858, reward_mean=0.060, reward_bound=0.308, batch=229\n",
      "1469: loss=0.851, reward_mean=0.060, reward_bound=0.387, batch=203\n",
      "1470: loss=0.853, reward_mean=0.130, reward_bound=0.144, batch=212\n",
      "1471: loss=0.851, reward_mean=0.090, reward_bound=0.172, batch=218\n",
      "1472: loss=0.844, reward_mean=0.060, reward_bound=0.185, batch=220\n",
      "1473: loss=0.842, reward_mean=0.060, reward_bound=0.222, batch=224\n",
      "1474: loss=0.843, reward_mean=0.060, reward_bound=0.226, batch=227\n",
      "1475: loss=0.846, reward_mean=0.060, reward_bound=0.249, batch=229\n",
      "1476: loss=0.846, reward_mean=0.060, reward_bound=0.254, batch=229\n",
      "1477: loss=0.848, reward_mean=0.040, reward_bound=0.282, batch=228\n",
      "1478: loss=0.850, reward_mean=0.080, reward_bound=0.314, batch=223\n",
      "1479: loss=0.852, reward_mean=0.070, reward_bound=0.314, batch=225\n",
      "1480: loss=0.852, reward_mean=0.060, reward_bound=0.349, batch=222\n",
      "1481: loss=0.852, reward_mean=0.040, reward_bound=0.214, batch=225\n",
      "1482: loss=0.852, reward_mean=0.090, reward_bound=0.314, batch=226\n",
      "1483: loss=0.854, reward_mean=0.090, reward_bound=0.387, batch=223\n",
      "1484: loss=0.862, reward_mean=0.090, reward_bound=0.430, batch=182\n",
      "1485: loss=0.860, reward_mean=0.050, reward_bound=0.000, batch=187\n",
      "1486: loss=0.855, reward_mean=0.070, reward_bound=0.000, batch=194\n",
      "1487: loss=0.851, reward_mean=0.100, reward_bound=0.000, batch=204\n",
      "1488: loss=0.857, reward_mean=0.110, reward_bound=0.134, batch=213\n",
      "1489: loss=0.861, reward_mean=0.070, reward_bound=0.135, batch=217\n",
      "1490: loss=0.860, reward_mean=0.050, reward_bound=0.120, batch=222\n",
      "1491: loss=0.860, reward_mean=0.070, reward_bound=0.172, batch=225\n",
      "1492: loss=0.861, reward_mean=0.020, reward_bound=0.024, batch=227\n",
      "1493: loss=0.863, reward_mean=0.090, reward_bound=0.229, batch=224\n",
      "1494: loss=0.862, reward_mean=0.060, reward_bound=0.252, batch=227\n",
      "1495: loss=0.859, reward_mean=0.050, reward_bound=0.254, batch=216\n",
      "1496: loss=0.860, reward_mean=0.060, reward_bound=0.152, batch=221\n",
      "1497: loss=0.857, reward_mean=0.080, reward_bound=0.229, batch=224\n",
      "1498: loss=0.857, reward_mean=0.090, reward_bound=0.282, batch=224\n",
      "1499: loss=0.856, reward_mean=0.070, reward_bound=0.305, batch=227\n",
      "1500: loss=0.852, reward_mean=0.110, reward_bound=0.314, batch=224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501: loss=0.853, reward_mean=0.060, reward_bound=0.311, batch=227\n",
      "1502: loss=0.854, reward_mean=0.060, reward_bound=0.308, batch=229\n",
      "1503: loss=0.851, reward_mean=0.050, reward_bound=0.349, batch=217\n",
      "1504: loss=0.851, reward_mean=0.080, reward_bound=0.245, batch=222\n",
      "1505: loss=0.851, reward_mean=0.060, reward_bound=0.282, batch=223\n",
      "1506: loss=0.848, reward_mean=0.040, reward_bound=0.219, batch=226\n",
      "1507: loss=0.850, reward_mean=0.100, reward_bound=0.314, batch=226\n",
      "1508: loss=0.850, reward_mean=0.070, reward_bound=0.349, batch=226\n",
      "1509: loss=0.851, reward_mean=0.090, reward_bound=0.387, batch=211\n",
      "1510: loss=0.851, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "1511: loss=0.849, reward_mean=0.070, reward_bound=0.185, batch=221\n",
      "1512: loss=0.849, reward_mean=0.070, reward_bound=0.185, batch=224\n",
      "1513: loss=0.848, reward_mean=0.060, reward_bound=0.252, batch=227\n",
      "1514: loss=0.850, reward_mean=0.130, reward_bound=0.314, batch=228\n",
      "1515: loss=0.849, reward_mean=0.100, reward_bound=0.353, batch=229\n",
      "1516: loss=0.847, reward_mean=0.110, reward_bound=0.387, batch=226\n",
      "1517: loss=0.847, reward_mean=0.070, reward_bound=0.331, batch=228\n",
      "1518: loss=0.846, reward_mean=0.080, reward_bound=0.353, batch=229\n",
      "1519: loss=0.846, reward_mean=0.050, reward_bound=0.405, batch=230\n",
      "1520: loss=0.846, reward_mean=0.070, reward_bound=0.418, batch=231\n",
      "1521: loss=0.852, reward_mean=0.060, reward_bound=0.430, batch=210\n",
      "1522: loss=0.847, reward_mean=0.060, reward_bound=0.000, batch=216\n",
      "1523: loss=0.850, reward_mean=0.060, reward_bound=0.047, batch=221\n",
      "1524: loss=0.846, reward_mean=0.080, reward_bound=0.254, batch=220\n",
      "1525: loss=0.849, reward_mean=0.050, reward_bound=0.135, batch=224\n",
      "1526: loss=0.844, reward_mean=0.070, reward_bound=0.249, batch=227\n",
      "1527: loss=0.842, reward_mean=0.020, reward_bound=0.203, batch=229\n",
      "1528: loss=0.845, reward_mean=0.030, reward_bound=0.265, batch=230\n",
      "1529: loss=0.847, reward_mean=0.050, reward_bound=0.329, batch=231\n",
      "1530: loss=0.850, reward_mean=0.100, reward_bound=0.349, batch=227\n",
      "1531: loss=0.849, reward_mean=0.060, reward_bound=0.249, batch=229\n",
      "1532: loss=0.849, reward_mean=0.070, reward_bound=0.328, batch=230\n",
      "1533: loss=0.849, reward_mean=0.040, reward_bound=0.376, batch=231\n",
      "1534: loss=0.851, reward_mean=0.070, reward_bound=0.387, batch=225\n",
      "1535: loss=0.851, reward_mean=0.090, reward_bound=0.430, batch=220\n",
      "1536: loss=0.851, reward_mean=0.060, reward_bound=0.247, batch=224\n",
      "1537: loss=0.848, reward_mean=0.100, reward_bound=0.226, batch=227\n",
      "1538: loss=0.849, reward_mean=0.040, reward_bound=0.229, batch=228\n",
      "1539: loss=0.850, reward_mean=0.070, reward_bound=0.387, batch=225\n",
      "1540: loss=0.850, reward_mean=0.090, reward_bound=0.387, batch=226\n",
      "1541: loss=0.851, reward_mean=0.080, reward_bound=0.368, batch=228\n",
      "1542: loss=0.851, reward_mean=0.060, reward_bound=0.286, batch=229\n",
      "1543: loss=0.850, reward_mean=0.070, reward_bound=0.343, batch=230\n",
      "1544: loss=0.849, reward_mean=0.060, reward_bound=0.356, batch=231\n",
      "1545: loss=0.848, reward_mean=0.080, reward_bound=0.387, batch=231\n",
      "1546: loss=0.849, reward_mean=0.080, reward_bound=0.430, batch=229\n",
      "1547: loss=0.849, reward_mean=0.070, reward_bound=0.401, batch=230\n",
      "1548: loss=0.849, reward_mean=0.080, reward_bound=0.387, batch=230\n",
      "1549: loss=0.850, reward_mean=0.040, reward_bound=0.464, batch=231\n",
      "1550: loss=0.850, reward_mean=0.070, reward_bound=0.430, batch=231\n",
      "1551: loss=0.850, reward_mean=0.090, reward_bound=0.387, batch=231\n",
      "1552: loss=0.850, reward_mean=0.080, reward_bound=0.387, batch=231\n",
      "1553: loss=0.862, reward_mean=0.090, reward_bound=0.478, batch=101\n",
      "1554: loss=0.870, reward_mean=0.040, reward_bound=0.000, batch=105\n",
      "1555: loss=0.856, reward_mean=0.060, reward_bound=0.000, batch=111\n",
      "1556: loss=0.854, reward_mean=0.070, reward_bound=0.000, batch=118\n",
      "1557: loss=0.843, reward_mean=0.060, reward_bound=0.000, batch=124\n",
      "1558: loss=0.838, reward_mean=0.120, reward_bound=0.000, batch=136\n",
      "1559: loss=0.835, reward_mean=0.140, reward_bound=0.000, batch=150\n",
      "1560: loss=0.836, reward_mean=0.080, reward_bound=0.000, batch=158\n",
      "1561: loss=0.837, reward_mean=0.080, reward_bound=0.000, batch=166\n",
      "1562: loss=0.826, reward_mean=0.070, reward_bound=0.000, batch=173\n",
      "1563: loss=0.822, reward_mean=0.060, reward_bound=0.000, batch=179\n",
      "1564: loss=0.816, reward_mean=0.050, reward_bound=0.000, batch=184\n",
      "1565: loss=0.817, reward_mean=0.100, reward_bound=0.000, batch=194\n",
      "1566: loss=0.820, reward_mean=0.090, reward_bound=0.000, batch=203\n",
      "1567: loss=0.817, reward_mean=0.050, reward_bound=0.000, batch=208\n",
      "1568: loss=0.813, reward_mean=0.050, reward_bound=0.000, batch=213\n",
      "1569: loss=0.809, reward_mean=0.050, reward_bound=0.000, batch=218\n",
      "1570: loss=0.812, reward_mean=0.120, reward_bound=0.089, batch=218\n",
      "1571: loss=0.818, reward_mean=0.100, reward_bound=0.123, batch=222\n",
      "1572: loss=0.816, reward_mean=0.060, reward_bound=0.140, batch=225\n",
      "1573: loss=0.816, reward_mean=0.030, reward_bound=0.150, batch=222\n",
      "1574: loss=0.820, reward_mean=0.090, reward_bound=0.167, batch=216\n",
      "1575: loss=0.824, reward_mean=0.050, reward_bound=0.093, batch=221\n",
      "1576: loss=0.828, reward_mean=0.060, reward_bound=0.185, batch=218\n",
      "1577: loss=0.826, reward_mean=0.060, reward_bound=0.130, batch=222\n",
      "1578: loss=0.832, reward_mean=0.110, reward_bound=0.213, batch=225\n",
      "1579: loss=0.839, reward_mean=0.130, reward_bound=0.229, batch=222\n",
      "1580: loss=0.838, reward_mean=0.040, reward_bound=0.114, batch=225\n",
      "1581: loss=0.836, reward_mean=0.090, reward_bound=0.229, batch=226\n",
      "1582: loss=0.834, reward_mean=0.050, reward_bound=0.254, batch=212\n",
      "1583: loss=0.837, reward_mean=0.110, reward_bound=0.282, batch=196\n",
      "1584: loss=0.834, reward_mean=0.050, reward_bound=0.000, batch=201\n",
      "1585: loss=0.834, reward_mean=0.040, reward_bound=0.000, batch=205\n",
      "1586: loss=0.830, reward_mean=0.100, reward_bound=0.068, batch=213\n",
      "1587: loss=0.830, reward_mean=0.090, reward_bound=0.080, batch=218\n",
      "1588: loss=0.828, reward_mean=0.070, reward_bound=0.138, batch=222\n",
      "1589: loss=0.826, reward_mean=0.080, reward_bound=0.206, batch=226\n",
      "1590: loss=0.828, reward_mean=0.060, reward_bound=0.206, batch=227\n",
      "1591: loss=0.833, reward_mean=0.080, reward_bound=0.254, batch=225\n",
      "1592: loss=0.833, reward_mean=0.150, reward_bound=0.282, batch=224\n",
      "1593: loss=0.830, reward_mean=0.080, reward_bound=0.314, batch=214\n",
      "1594: loss=0.829, reward_mean=0.080, reward_bound=0.165, batch=220\n",
      "1595: loss=0.829, reward_mean=0.090, reward_bound=0.254, batch=223\n",
      "1596: loss=0.827, reward_mean=0.040, reward_bound=0.129, batch=226\n",
      "1597: loss=0.825, reward_mean=0.030, reward_bound=0.115, batch=228\n",
      "1598: loss=0.829, reward_mean=0.120, reward_bound=0.286, batch=229\n",
      "1599: loss=0.827, reward_mean=0.070, reward_bound=0.314, batch=228\n",
      "1600: loss=0.827, reward_mean=0.090, reward_bound=0.289, batch=229\n",
      "1601: loss=0.827, reward_mean=0.030, reward_bound=0.239, batch=230\n",
      "1602: loss=0.834, reward_mean=0.100, reward_bound=0.349, batch=207\n",
      "1603: loss=0.833, reward_mean=0.070, reward_bound=0.000, batch=214\n",
      "1604: loss=0.836, reward_mean=0.060, reward_bound=0.109, batch=220\n",
      "1605: loss=0.835, reward_mean=0.080, reward_bound=0.185, batch=224\n",
      "1606: loss=0.833, reward_mean=0.080, reward_bound=0.249, batch=227\n",
      "1607: loss=0.833, reward_mean=0.060, reward_bound=0.277, batch=229\n",
      "1608: loss=0.833, reward_mean=0.040, reward_bound=0.282, batch=228\n",
      "1609: loss=0.832, reward_mean=0.120, reward_bound=0.314, batch=228\n",
      "1610: loss=0.829, reward_mean=0.080, reward_bound=0.349, batch=226\n",
      "1611: loss=0.830, reward_mean=0.070, reward_bound=0.271, batch=228\n",
      "1612: loss=0.828, reward_mean=0.150, reward_bound=0.353, batch=229\n",
      "1613: loss=0.826, reward_mean=0.080, reward_bound=0.387, batch=195\n",
      "1614: loss=0.827, reward_mean=0.080, reward_bound=0.000, batch=203\n",
      "1615: loss=0.822, reward_mean=0.080, reward_bound=0.000, batch=211\n",
      "1616: loss=0.822, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "1617: loss=0.826, reward_mean=0.060, reward_bound=0.080, batch=222\n",
      "1618: loss=0.831, reward_mean=0.120, reward_bound=0.198, batch=225\n",
      "1619: loss=0.830, reward_mean=0.080, reward_bound=0.254, batch=222\n",
      "1620: loss=0.829, reward_mean=0.090, reward_bound=0.245, batch=225\n",
      "1621: loss=0.830, reward_mean=0.080, reward_bound=0.260, batch=227\n",
      "1622: loss=0.827, reward_mean=0.100, reward_bound=0.282, batch=227\n",
      "1623: loss=0.826, reward_mean=0.090, reward_bound=0.272, batch=229\n",
      "1624: loss=0.825, reward_mean=0.110, reward_bound=0.314, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625: loss=0.828, reward_mean=0.050, reward_bound=0.229, batch=224\n",
      "1626: loss=0.828, reward_mean=0.100, reward_bound=0.349, batch=221\n",
      "1627: loss=0.828, reward_mean=0.060, reward_bound=0.229, batch=223\n",
      "1628: loss=0.829, reward_mean=0.090, reward_bound=0.372, batch=226\n",
      "1629: loss=0.827, reward_mean=0.090, reward_bound=0.387, batch=218\n",
      "1630: loss=0.827, reward_mean=0.110, reward_bound=0.387, batch=220\n",
      "1631: loss=0.827, reward_mean=0.090, reward_bound=0.304, batch=224\n",
      "1632: loss=0.826, reward_mean=0.060, reward_bound=0.252, batch=227\n",
      "1633: loss=0.827, reward_mean=0.090, reward_bound=0.254, batch=228\n",
      "1634: loss=0.827, reward_mean=0.060, reward_bound=0.317, batch=229\n",
      "1635: loss=0.826, reward_mean=0.060, reward_bound=0.349, batch=228\n",
      "1636: loss=0.827, reward_mean=0.100, reward_bound=0.387, batch=226\n",
      "1637: loss=0.825, reward_mean=0.050, reward_bound=0.284, batch=228\n",
      "1638: loss=0.826, reward_mean=0.080, reward_bound=0.349, batch=228\n",
      "1639: loss=0.826, reward_mean=0.100, reward_bound=0.392, batch=229\n",
      "1640: loss=0.827, reward_mean=0.130, reward_bound=0.430, batch=174\n",
      "1641: loss=0.822, reward_mean=0.120, reward_bound=0.000, batch=186\n",
      "1642: loss=0.830, reward_mean=0.050, reward_bound=0.000, batch=191\n",
      "1643: loss=0.825, reward_mean=0.050, reward_bound=0.000, batch=196\n",
      "1644: loss=0.830, reward_mean=0.130, reward_bound=0.055, batch=207\n",
      "1645: loss=0.829, reward_mean=0.080, reward_bound=0.022, batch=215\n",
      "1646: loss=0.814, reward_mean=0.110, reward_bound=0.124, batch=220\n",
      "1647: loss=0.818, reward_mean=0.080, reward_bound=0.150, batch=222\n",
      "1648: loss=0.819, reward_mean=0.080, reward_bound=0.185, batch=224\n",
      "1649: loss=0.821, reward_mean=0.030, reward_bound=0.167, batch=227\n",
      "1650: loss=0.822, reward_mean=0.110, reward_bound=0.229, batch=227\n",
      "1651: loss=0.821, reward_mean=0.120, reward_bound=0.254, batch=227\n",
      "1652: loss=0.821, reward_mean=0.060, reward_bound=0.282, batch=225\n",
      "1653: loss=0.821, reward_mean=0.120, reward_bound=0.314, batch=223\n",
      "1654: loss=0.822, reward_mean=0.120, reward_bound=0.335, batch=226\n",
      "1655: loss=0.826, reward_mean=0.100, reward_bound=0.349, batch=215\n",
      "1656: loss=0.825, reward_mean=0.120, reward_bound=0.349, batch=217\n",
      "1657: loss=0.823, reward_mean=0.070, reward_bound=0.206, batch=221\n",
      "1658: loss=0.824, reward_mean=0.090, reward_bound=0.314, batch=224\n",
      "1659: loss=0.825, reward_mean=0.070, reward_bound=0.226, batch=227\n",
      "1660: loss=0.821, reward_mean=0.070, reward_bound=0.272, batch=229\n",
      "1661: loss=0.825, reward_mean=0.130, reward_bound=0.328, batch=230\n",
      "1662: loss=0.825, reward_mean=0.110, reward_bound=0.349, batch=230\n",
      "1663: loss=0.827, reward_mean=0.080, reward_bound=0.387, batch=221\n",
      "1664: loss=0.825, reward_mean=0.130, reward_bound=0.387, batch=224\n",
      "1665: loss=0.824, reward_mean=0.100, reward_bound=0.430, batch=207\n",
      "1666: loss=0.819, reward_mean=0.080, reward_bound=0.097, batch=215\n",
      "1667: loss=0.820, reward_mean=0.080, reward_bound=0.185, batch=219\n",
      "1668: loss=0.817, reward_mean=0.060, reward_bound=0.206, batch=222\n",
      "1669: loss=0.813, reward_mean=0.060, reward_bound=0.236, batch=225\n",
      "1670: loss=0.823, reward_mean=0.060, reward_bound=0.254, batch=224\n",
      "1671: loss=0.819, reward_mean=0.090, reward_bound=0.314, batch=224\n",
      "1672: loss=0.820, reward_mean=0.130, reward_bound=0.349, batch=223\n",
      "1673: loss=0.819, reward_mean=0.070, reward_bound=0.261, batch=226\n",
      "1674: loss=0.822, reward_mean=0.100, reward_bound=0.349, batch=224\n",
      "1675: loss=0.823, reward_mean=0.170, reward_bound=0.387, batch=220\n",
      "1676: loss=0.822, reward_mean=0.110, reward_bound=0.338, batch=224\n",
      "1677: loss=0.818, reward_mean=0.110, reward_bound=0.430, batch=217\n",
      "1678: loss=0.822, reward_mean=0.100, reward_bound=0.314, batch=221\n",
      "1679: loss=0.822, reward_mean=0.100, reward_bound=0.314, batch=224\n",
      "1680: loss=0.824, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "1681: loss=0.820, reward_mean=0.120, reward_bound=0.314, batch=227\n",
      "1682: loss=0.818, reward_mean=0.090, reward_bound=0.380, batch=229\n",
      "1683: loss=0.818, reward_mean=0.070, reward_bound=0.349, batch=229\n",
      "1684: loss=0.815, reward_mean=0.070, reward_bound=0.387, batch=224\n",
      "1685: loss=0.811, reward_mean=0.100, reward_bound=0.311, batch=227\n",
      "1686: loss=0.814, reward_mean=0.070, reward_bound=0.373, batch=229\n",
      "1687: loss=0.817, reward_mean=0.070, reward_bound=0.430, batch=224\n",
      "1688: loss=0.814, reward_mean=0.070, reward_bound=0.426, batch=227\n",
      "1689: loss=0.814, reward_mean=0.040, reward_bound=0.206, batch=228\n",
      "1690: loss=0.813, reward_mean=0.090, reward_bound=0.392, batch=229\n",
      "1691: loss=0.816, reward_mean=0.080, reward_bound=0.430, batch=226\n",
      "1692: loss=0.846, reward_mean=0.120, reward_bound=0.478, batch=155\n",
      "1693: loss=0.839, reward_mean=0.070, reward_bound=0.000, batch=162\n",
      "1694: loss=0.835, reward_mean=0.050, reward_bound=0.000, batch=167\n",
      "1695: loss=0.834, reward_mean=0.090, reward_bound=0.000, batch=176\n",
      "1696: loss=0.837, reward_mean=0.030, reward_bound=0.000, batch=179\n",
      "1697: loss=0.831, reward_mean=0.080, reward_bound=0.000, batch=187\n",
      "1698: loss=0.820, reward_mean=0.070, reward_bound=0.000, batch=194\n",
      "1699: loss=0.820, reward_mean=0.110, reward_bound=0.000, batch=205\n",
      "1700: loss=0.819, reward_mean=0.060, reward_bound=0.000, batch=211\n",
      "1701: loss=0.823, reward_mean=0.090, reward_bound=0.080, batch=217\n",
      "1702: loss=0.823, reward_mean=0.070, reward_bound=0.119, batch=222\n",
      "1703: loss=0.824, reward_mean=0.070, reward_bound=0.135, batch=223\n",
      "1704: loss=0.824, reward_mean=0.070, reward_bound=0.160, batch=226\n",
      "1705: loss=0.828, reward_mean=0.080, reward_bound=0.185, batch=225\n",
      "1706: loss=0.828, reward_mean=0.050, reward_bound=0.206, batch=223\n",
      "1707: loss=0.827, reward_mean=0.070, reward_bound=0.211, batch=226\n",
      "1708: loss=0.832, reward_mean=0.090, reward_bound=0.229, batch=222\n",
      "1709: loss=0.836, reward_mean=0.110, reward_bound=0.254, batch=216\n",
      "1710: loss=0.827, reward_mean=0.050, reward_bound=0.049, batch=221\n",
      "1711: loss=0.830, reward_mean=0.080, reward_bound=0.206, batch=224\n",
      "1712: loss=0.830, reward_mean=0.090, reward_bound=0.280, batch=227\n",
      "1713: loss=0.829, reward_mean=0.090, reward_bound=0.282, batch=215\n",
      "1714: loss=0.832, reward_mean=0.100, reward_bound=0.314, batch=210\n",
      "1715: loss=0.832, reward_mean=0.080, reward_bound=0.194, batch=217\n",
      "1716: loss=0.828, reward_mean=0.070, reward_bound=0.206, batch=221\n",
      "1717: loss=0.828, reward_mean=0.090, reward_bound=0.206, batch=224\n",
      "1718: loss=0.827, reward_mean=0.090, reward_bound=0.254, batch=226\n",
      "1719: loss=0.828, reward_mean=0.050, reward_bound=0.282, batch=227\n",
      "1720: loss=0.829, reward_mean=0.050, reward_bound=0.314, batch=228\n",
      "1721: loss=0.834, reward_mean=0.080, reward_bound=0.349, batch=204\n",
      "1722: loss=0.834, reward_mean=0.090, reward_bound=0.072, batch=213\n",
      "1723: loss=0.828, reward_mean=0.130, reward_bound=0.314, batch=218\n",
      "1724: loss=0.832, reward_mean=0.060, reward_bound=0.208, batch=222\n",
      "1725: loss=0.830, reward_mean=0.070, reward_bound=0.213, batch=225\n",
      "1726: loss=0.828, reward_mean=0.060, reward_bound=0.273, batch=227\n",
      "1727: loss=0.829, reward_mean=0.060, reward_bound=0.349, batch=226\n",
      "1728: loss=0.828, reward_mean=0.080, reward_bound=0.314, batch=227\n",
      "1729: loss=0.829, reward_mean=0.070, reward_bound=0.335, batch=229\n",
      "1730: loss=0.828, reward_mean=0.020, reward_bound=0.251, batch=230\n",
      "1731: loss=0.829, reward_mean=0.100, reward_bound=0.349, batch=229\n",
      "1732: loss=0.834, reward_mean=0.060, reward_bound=0.387, batch=209\n",
      "1733: loss=0.843, reward_mean=0.070, reward_bound=0.035, batch=216\n",
      "1734: loss=0.842, reward_mean=0.030, reward_bound=0.000, batch=219\n",
      "1735: loss=0.837, reward_mean=0.060, reward_bound=0.093, batch=223\n",
      "1736: loss=0.835, reward_mean=0.040, reward_bound=0.112, batch=226\n",
      "1737: loss=0.830, reward_mean=0.070, reward_bound=0.176, batch=228\n",
      "1738: loss=0.828, reward_mean=0.070, reward_bound=0.208, batch=229\n",
      "1739: loss=0.825, reward_mean=0.040, reward_bound=0.229, batch=229\n",
      "1740: loss=0.824, reward_mean=0.050, reward_bound=0.254, batch=228\n",
      "1741: loss=0.827, reward_mean=0.070, reward_bound=0.286, batch=229\n",
      "1742: loss=0.825, reward_mean=0.080, reward_bound=0.314, batch=228\n",
      "1743: loss=0.825, reward_mean=0.060, reward_bound=0.317, batch=229\n",
      "1744: loss=0.824, reward_mean=0.080, reward_bound=0.309, batch=230\n",
      "1745: loss=0.822, reward_mean=0.090, reward_bound=0.349, batch=229\n",
      "1746: loss=0.822, reward_mean=0.070, reward_bound=0.364, batch=230\n",
      "1747: loss=0.822, reward_mean=0.060, reward_bound=0.376, batch=231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748: loss=0.827, reward_mean=0.080, reward_bound=0.387, batch=227\n",
      "1749: loss=0.824, reward_mean=0.080, reward_bound=0.308, batch=229\n",
      "1750: loss=0.826, reward_mean=0.090, reward_bound=0.349, batch=229\n",
      "1751: loss=0.826, reward_mean=0.080, reward_bound=0.360, batch=230\n",
      "1752: loss=0.826, reward_mean=0.080, reward_bound=0.356, batch=231\n",
      "1753: loss=0.832, reward_mean=0.100, reward_bound=0.430, batch=197\n",
      "1754: loss=0.829, reward_mean=0.080, reward_bound=0.000, batch=205\n",
      "1755: loss=0.822, reward_mean=0.060, reward_bound=0.000, batch=211\n",
      "1756: loss=0.816, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "1757: loss=0.821, reward_mean=0.090, reward_bound=0.098, batch=221\n",
      "1758: loss=0.818, reward_mean=0.080, reward_bound=0.185, batch=224\n",
      "1759: loss=0.820, reward_mean=0.050, reward_bound=0.206, batch=224\n",
      "1760: loss=0.825, reward_mean=0.140, reward_bound=0.282, batch=222\n",
      "1761: loss=0.827, reward_mean=0.060, reward_bound=0.238, batch=225\n",
      "1762: loss=0.832, reward_mean=0.070, reward_bound=0.314, batch=225\n",
      "1763: loss=0.832, reward_mean=0.090, reward_bound=0.282, batch=226\n",
      "1764: loss=0.832, reward_mean=0.090, reward_bound=0.314, batch=225\n",
      "1765: loss=0.828, reward_mean=0.050, reward_bound=0.349, batch=224\n",
      "1766: loss=0.826, reward_mean=0.080, reward_bound=0.384, batch=227\n",
      "1767: loss=0.833, reward_mean=0.120, reward_bound=0.387, batch=218\n",
      "1768: loss=0.830, reward_mean=0.080, reward_bound=0.187, batch=222\n",
      "1769: loss=0.829, reward_mean=0.080, reward_bound=0.292, batch=225\n",
      "1770: loss=0.830, reward_mean=0.110, reward_bound=0.314, batch=226\n",
      "1771: loss=0.833, reward_mean=0.070, reward_bound=0.349, batch=223\n",
      "1772: loss=0.835, reward_mean=0.080, reward_bound=0.301, batch=226\n",
      "1773: loss=0.830, reward_mean=0.090, reward_bound=0.351, batch=228\n",
      "1774: loss=0.830, reward_mean=0.070, reward_bound=0.321, batch=229\n",
      "1775: loss=0.828, reward_mean=0.110, reward_bound=0.405, batch=230\n",
      "1776: loss=0.829, reward_mean=0.100, reward_bound=0.406, batch=231\n",
      "1777: loss=0.830, reward_mean=0.070, reward_bound=0.430, batch=215\n",
      "1778: loss=0.833, reward_mean=0.060, reward_bound=0.120, batch=220\n",
      "1779: loss=0.832, reward_mean=0.100, reward_bound=0.304, batch=224\n",
      "1780: loss=0.833, reward_mean=0.050, reward_bound=0.271, batch=227\n",
      "1781: loss=0.829, reward_mean=0.130, reward_bound=0.349, batch=228\n",
      "1782: loss=0.825, reward_mean=0.070, reward_bound=0.387, batch=225\n",
      "1783: loss=0.825, reward_mean=0.090, reward_bound=0.430, batch=220\n",
      "1784: loss=0.827, reward_mean=0.080, reward_bound=0.222, batch=224\n",
      "1785: loss=0.825, reward_mean=0.140, reward_bound=0.426, batch=227\n",
      "1786: loss=0.826, reward_mean=0.070, reward_bound=0.395, batch=229\n",
      "1787: loss=0.826, reward_mean=0.080, reward_bound=0.430, batch=227\n",
      "1788: loss=0.827, reward_mean=0.100, reward_bound=0.452, batch=229\n",
      "1789: loss=0.828, reward_mean=0.080, reward_bound=0.380, batch=230\n",
      "1790: loss=0.828, reward_mean=0.080, reward_bound=0.418, batch=231\n",
      "1791: loss=0.827, reward_mean=0.080, reward_bound=0.430, batch=231\n",
      "1792: loss=0.834, reward_mean=0.060, reward_bound=0.478, batch=193\n",
      "1793: loss=0.831, reward_mean=0.040, reward_bound=0.000, batch=197\n",
      "1794: loss=0.830, reward_mean=0.100, reward_bound=0.000, batch=207\n",
      "1795: loss=0.828, reward_mean=0.090, reward_bound=0.130, batch=215\n",
      "1796: loss=0.823, reward_mean=0.080, reward_bound=0.115, batch=220\n",
      "1797: loss=0.824, reward_mean=0.110, reward_bound=0.216, batch=224\n",
      "1798: loss=0.825, reward_mean=0.070, reward_bound=0.277, batch=227\n",
      "1799: loss=0.834, reward_mean=0.070, reward_bound=0.282, batch=223\n",
      "1800: loss=0.834, reward_mean=0.070, reward_bound=0.178, batch=226\n",
      "1801: loss=0.832, reward_mean=0.110, reward_bound=0.314, batch=223\n",
      "1802: loss=0.832, reward_mean=0.080, reward_bound=0.349, batch=223\n",
      "1803: loss=0.834, reward_mean=0.070, reward_bound=0.311, batch=226\n",
      "1804: loss=0.830, reward_mean=0.090, reward_bound=0.349, batch=225\n",
      "1805: loss=0.829, reward_mean=0.070, reward_bound=0.199, batch=227\n",
      "1806: loss=0.831, reward_mean=0.110, reward_bound=0.380, batch=229\n",
      "1807: loss=0.828, reward_mean=0.070, reward_bound=0.387, batch=220\n",
      "1808: loss=0.828, reward_mean=0.080, reward_bound=0.349, batch=222\n",
      "1809: loss=0.827, reward_mean=0.060, reward_bound=0.174, batch=225\n",
      "1810: loss=0.829, reward_mean=0.070, reward_bound=0.356, batch=227\n",
      "1811: loss=0.827, reward_mean=0.120, reward_bound=0.387, batch=226\n",
      "1812: loss=0.828, reward_mean=0.080, reward_bound=0.368, batch=228\n",
      "1813: loss=0.826, reward_mean=0.080, reward_bound=0.392, batch=229\n",
      "1814: loss=0.830, reward_mean=0.030, reward_bound=0.430, batch=214\n",
      "1815: loss=0.827, reward_mean=0.100, reward_bound=0.204, batch=220\n",
      "1816: loss=0.824, reward_mean=0.080, reward_bound=0.240, batch=224\n",
      "1817: loss=0.824, reward_mean=0.100, reward_bound=0.282, batch=225\n",
      "1818: loss=0.822, reward_mean=0.110, reward_bound=0.321, batch=227\n",
      "1819: loss=0.827, reward_mean=0.080, reward_bound=0.349, batch=226\n",
      "1820: loss=0.825, reward_mean=0.090, reward_bound=0.256, batch=228\n",
      "1821: loss=0.827, reward_mean=0.050, reward_bound=0.286, batch=229\n",
      "1822: loss=0.828, reward_mean=0.050, reward_bound=0.328, batch=230\n",
      "1823: loss=0.829, reward_mean=0.060, reward_bound=0.338, batch=231\n",
      "1824: loss=0.826, reward_mean=0.060, reward_bound=0.349, batch=230\n",
      "1825: loss=0.829, reward_mean=0.050, reward_bound=0.387, batch=226\n",
      "1826: loss=0.832, reward_mean=0.080, reward_bound=0.430, batch=224\n",
      "1827: loss=0.834, reward_mean=0.100, reward_bound=0.422, batch=227\n",
      "1828: loss=0.835, reward_mean=0.100, reward_bound=0.387, batch=228\n",
      "1829: loss=0.834, reward_mean=0.060, reward_bound=0.357, batch=229\n",
      "1830: loss=0.833, reward_mean=0.080, reward_bound=0.360, batch=230\n",
      "1831: loss=0.831, reward_mean=0.080, reward_bound=0.430, batch=229\n",
      "1832: loss=0.831, reward_mean=0.050, reward_bound=0.239, batch=230\n",
      "1833: loss=0.833, reward_mean=0.070, reward_bound=0.329, batch=231\n",
      "1834: loss=0.832, reward_mean=0.090, reward_bound=0.349, batch=231\n",
      "1835: loss=0.831, reward_mean=0.060, reward_bound=0.430, batch=230\n",
      "1836: loss=0.831, reward_mean=0.060, reward_bound=0.451, batch=231\n",
      "1837: loss=0.831, reward_mean=0.050, reward_bound=0.387, batch=231\n",
      "1838: loss=0.831, reward_mean=0.060, reward_bound=0.430, batch=231\n",
      "1839: loss=0.831, reward_mean=0.110, reward_bound=0.430, batch=231\n",
      "1840: loss=0.834, reward_mean=0.090, reward_bound=0.478, batch=207\n",
      "1841: loss=0.840, reward_mean=0.090, reward_bound=0.202, batch=215\n",
      "1842: loss=0.840, reward_mean=0.110, reward_bound=0.229, batch=219\n",
      "1843: loss=0.841, reward_mean=0.100, reward_bound=0.239, batch=223\n",
      "1844: loss=0.842, reward_mean=0.070, reward_bound=0.254, batch=225\n",
      "1845: loss=0.841, reward_mean=0.060, reward_bound=0.289, batch=227\n",
      "1846: loss=0.837, reward_mean=0.080, reward_bound=0.308, batch=229\n",
      "1847: loss=0.834, reward_mean=0.050, reward_bound=0.314, batch=226\n",
      "1848: loss=0.830, reward_mean=0.090, reward_bound=0.349, batch=222\n",
      "1849: loss=0.828, reward_mean=0.060, reward_bound=0.263, batch=225\n",
      "1850: loss=0.829, reward_mean=0.090, reward_bound=0.282, batch=226\n",
      "1851: loss=0.828, reward_mean=0.060, reward_bound=0.298, batch=228\n",
      "1852: loss=0.832, reward_mean=0.060, reward_bound=0.387, batch=222\n",
      "1853: loss=0.833, reward_mean=0.110, reward_bound=0.336, batch=225\n",
      "1854: loss=0.831, reward_mean=0.070, reward_bound=0.387, batch=226\n",
      "1855: loss=0.831, reward_mean=0.070, reward_bound=0.409, batch=228\n",
      "1856: loss=0.830, reward_mean=0.040, reward_bound=0.192, batch=229\n",
      "1857: loss=0.832, reward_mean=0.090, reward_bound=0.387, batch=229\n",
      "1858: loss=0.831, reward_mean=0.040, reward_bound=0.173, batch=230\n",
      "1859: loss=0.833, reward_mean=0.100, reward_bound=0.376, batch=231\n",
      "1860: loss=0.833, reward_mean=0.030, reward_bound=0.349, batch=231\n",
      "1861: loss=0.830, reward_mean=0.060, reward_bound=0.430, batch=217\n",
      "1862: loss=0.831, reward_mean=0.060, reward_bound=0.267, batch=222\n",
      "1863: loss=0.830, reward_mean=0.070, reward_bound=0.236, batch=225\n",
      "1864: loss=0.829, reward_mean=0.100, reward_bound=0.314, batch=226\n",
      "1865: loss=0.832, reward_mean=0.090, reward_bound=0.349, batch=225\n",
      "1866: loss=0.829, reward_mean=0.100, reward_bound=0.387, batch=225\n",
      "1867: loss=0.828, reward_mean=0.120, reward_bound=0.430, batch=221\n",
      "1868: loss=0.825, reward_mean=0.140, reward_bound=0.387, batch=224\n",
      "1869: loss=0.826, reward_mean=0.060, reward_bound=0.277, batch=227\n",
      "1870: loss=0.824, reward_mean=0.060, reward_bound=0.342, batch=229\n",
      "1871: loss=0.825, reward_mean=0.070, reward_bound=0.328, batch=230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872: loss=0.827, reward_mean=0.130, reward_bound=0.376, batch=231\n",
      "1873: loss=0.828, reward_mean=0.060, reward_bound=0.430, batch=228\n",
      "1874: loss=0.827, reward_mean=0.060, reward_bound=0.435, batch=229\n",
      "1875: loss=0.828, reward_mean=0.060, reward_bound=0.424, batch=230\n",
      "1876: loss=0.828, reward_mean=0.080, reward_bound=0.338, batch=231\n",
      "1877: loss=0.828, reward_mean=0.030, reward_bound=0.349, batch=231\n",
      "1878: loss=0.831, reward_mean=0.080, reward_bound=0.478, batch=218\n",
      "1879: loss=0.830, reward_mean=0.090, reward_bound=0.282, batch=221\n",
      "1880: loss=0.831, reward_mean=0.100, reward_bound=0.314, batch=223\n",
      "1881: loss=0.828, reward_mean=0.050, reward_bound=0.301, batch=226\n",
      "1882: loss=0.828, reward_mean=0.110, reward_bound=0.331, batch=228\n",
      "1883: loss=0.829, reward_mean=0.100, reward_bound=0.349, batch=228\n",
      "1884: loss=0.830, reward_mean=0.070, reward_bound=0.353, batch=229\n",
      "1885: loss=0.829, reward_mean=0.020, reward_bound=0.387, batch=227\n",
      "1886: loss=0.826, reward_mean=0.050, reward_bound=0.422, batch=229\n",
      "1887: loss=0.825, reward_mean=0.050, reward_bound=0.342, batch=230\n",
      "1888: loss=0.828, reward_mean=0.080, reward_bound=0.406, batch=231\n",
      "1889: loss=0.828, reward_mean=0.050, reward_bound=0.430, batch=224\n",
      "1890: loss=0.828, reward_mean=0.110, reward_bound=0.430, batch=225\n",
      "1891: loss=0.826, reward_mean=0.070, reward_bound=0.406, batch=227\n",
      "1892: loss=0.831, reward_mean=0.090, reward_bound=0.478, batch=222\n",
      "1893: loss=0.831, reward_mean=0.050, reward_bound=0.314, batch=224\n",
      "1894: loss=0.828, reward_mean=0.060, reward_bound=0.311, batch=227\n",
      "1895: loss=0.830, reward_mean=0.070, reward_bound=0.314, batch=228\n",
      "1896: loss=0.827, reward_mean=0.060, reward_bound=0.387, batch=227\n",
      "1897: loss=0.830, reward_mean=0.080, reward_bound=0.469, batch=229\n",
      "1898: loss=0.831, reward_mean=0.100, reward_bound=0.401, batch=230\n",
      "1899: loss=0.829, reward_mean=0.080, reward_bound=0.478, batch=225\n",
      "1900: loss=0.829, reward_mean=0.040, reward_bound=0.321, batch=227\n",
      "1901: loss=0.827, reward_mean=0.030, reward_bound=0.196, batch=229\n",
      "1902: loss=0.830, reward_mean=0.090, reward_bound=0.349, batch=229\n",
      "1903: loss=0.828, reward_mean=0.090, reward_bound=0.430, batch=229\n",
      "1904: loss=0.828, reward_mean=0.120, reward_bound=0.478, batch=232\n",
      "1905: loss=0.829, reward_mean=0.060, reward_bound=0.478, batch=227\n",
      "1906: loss=0.828, reward_mean=0.160, reward_bound=0.430, batch=228\n",
      "1907: loss=0.828, reward_mean=0.050, reward_bound=0.441, batch=229\n",
      "1908: loss=0.828, reward_mean=0.080, reward_bound=0.430, batch=229\n",
      "1909: loss=0.827, reward_mean=0.140, reward_bound=0.445, batch=230\n",
      "1910: loss=0.829, reward_mean=0.090, reward_bound=0.464, batch=231\n",
      "1911: loss=0.827, reward_mean=0.060, reward_bound=0.478, batch=230\n",
      "1912: loss=0.827, reward_mean=0.080, reward_bound=0.349, batch=230\n",
      "1913: loss=0.827, reward_mean=0.040, reward_bound=0.515, batch=231\n",
      "1914: loss=0.827, reward_mean=0.080, reward_bound=0.314, batch=231\n",
      "1915: loss=0.827, reward_mean=0.010, reward_bound=0.254, batch=231\n",
      "1916: loss=0.827, reward_mean=0.100, reward_bound=0.387, batch=231\n",
      "1918: loss=0.831, reward_mean=0.090, reward_bound=0.000, batch=9\n",
      "1919: loss=0.826, reward_mean=0.080, reward_bound=0.000, batch=17\n",
      "1920: loss=0.799, reward_mean=0.040, reward_bound=0.000, batch=21\n",
      "1921: loss=0.766, reward_mean=0.090, reward_bound=0.000, batch=30\n",
      "1922: loss=0.782, reward_mean=0.090, reward_bound=0.000, batch=39\n",
      "1923: loss=0.784, reward_mean=0.110, reward_bound=0.000, batch=50\n",
      "1924: loss=0.785, reward_mean=0.100, reward_bound=0.000, batch=60\n",
      "1925: loss=0.793, reward_mean=0.110, reward_bound=0.000, batch=71\n",
      "1926: loss=0.789, reward_mean=0.070, reward_bound=0.000, batch=78\n",
      "1927: loss=0.791, reward_mean=0.100, reward_bound=0.000, batch=88\n",
      "1928: loss=0.790, reward_mean=0.140, reward_bound=0.000, batch=102\n",
      "1929: loss=0.788, reward_mean=0.110, reward_bound=0.000, batch=113\n",
      "1930: loss=0.794, reward_mean=0.090, reward_bound=0.000, batch=122\n",
      "1931: loss=0.791, reward_mean=0.130, reward_bound=0.000, batch=135\n",
      "1932: loss=0.791, reward_mean=0.090, reward_bound=0.000, batch=144\n",
      "1933: loss=0.785, reward_mean=0.080, reward_bound=0.000, batch=152\n",
      "1934: loss=0.787, reward_mean=0.090, reward_bound=0.000, batch=161\n",
      "1935: loss=0.786, reward_mean=0.060, reward_bound=0.000, batch=167\n",
      "1936: loss=0.786, reward_mean=0.100, reward_bound=0.000, batch=177\n",
      "1937: loss=0.784, reward_mean=0.170, reward_bound=0.006, batch=194\n",
      "1938: loss=0.784, reward_mean=0.120, reward_bound=0.007, batch=206\n",
      "1939: loss=0.785, reward_mean=0.130, reward_bound=0.031, batch=212\n",
      "1940: loss=0.785, reward_mean=0.080, reward_bound=0.035, batch=218\n",
      "1941: loss=0.783, reward_mean=0.080, reward_bound=0.043, batch=222\n",
      "1942: loss=0.783, reward_mean=0.110, reward_bound=0.072, batch=220\n",
      "1943: loss=0.789, reward_mean=0.080, reward_bound=0.089, batch=214\n",
      "1944: loss=0.790, reward_mean=0.100, reward_bound=0.098, batch=219\n",
      "1945: loss=0.783, reward_mean=0.120, reward_bound=0.122, batch=216\n",
      "1946: loss=0.788, reward_mean=0.090, reward_bound=0.135, batch=213\n",
      "1947: loss=0.785, reward_mean=0.070, reward_bound=0.144, batch=219\n",
      "1948: loss=0.781, reward_mean=0.070, reward_bound=0.141, batch=223\n",
      "1949: loss=0.780, reward_mean=0.120, reward_bound=0.150, batch=222\n",
      "1950: loss=0.775, reward_mean=0.110, reward_bound=0.167, batch=213\n",
      "1951: loss=0.775, reward_mean=0.130, reward_bound=0.185, batch=215\n",
      "1952: loss=0.771, reward_mean=0.160, reward_bound=0.206, batch=210\n",
      "1953: loss=0.762, reward_mean=0.070, reward_bound=0.013, batch=217\n",
      "1954: loss=0.766, reward_mean=0.100, reward_bound=0.119, batch=222\n",
      "1955: loss=0.766, reward_mean=0.100, reward_bound=0.172, batch=225\n",
      "1956: loss=0.766, reward_mean=0.090, reward_bound=0.189, batch=227\n",
      "1957: loss=0.765, reward_mean=0.080, reward_bound=0.206, batch=228\n",
      "1958: loss=0.761, reward_mean=0.160, reward_bound=0.229, batch=211\n",
      "1959: loss=0.758, reward_mean=0.120, reward_bound=0.254, batch=190\n",
      "1960: loss=0.762, reward_mean=0.110, reward_bound=0.000, batch=201\n",
      "1961: loss=0.757, reward_mean=0.090, reward_bound=0.000, batch=210\n",
      "1962: loss=0.755, reward_mean=0.160, reward_bound=0.150, batch=215\n",
      "1963: loss=0.752, reward_mean=0.140, reward_bound=0.189, batch=220\n",
      "1964: loss=0.754, reward_mean=0.050, reward_bound=0.131, batch=224\n",
      "1965: loss=0.755, reward_mean=0.100, reward_bound=0.226, batch=227\n",
      "1966: loss=0.758, reward_mean=0.060, reward_bound=0.229, batch=227\n",
      "1967: loss=0.761, reward_mean=0.120, reward_bound=0.277, batch=229\n",
      "1968: loss=0.758, reward_mean=0.120, reward_bound=0.282, batch=202\n",
      "1969: loss=0.758, reward_mean=0.130, reward_bound=0.092, batch=211\n",
      "1970: loss=0.756, reward_mean=0.100, reward_bound=0.185, batch=217\n",
      "1971: loss=0.753, reward_mean=0.170, reward_bound=0.229, batch=221\n",
      "1972: loss=0.753, reward_mean=0.120, reward_bound=0.254, batch=222\n",
      "1973: loss=0.749, reward_mean=0.090, reward_bound=0.179, batch=225\n",
      "1974: loss=0.752, reward_mean=0.090, reward_bound=0.282, batch=226\n",
      "1975: loss=0.748, reward_mean=0.110, reward_bound=0.314, batch=187\n",
      "1976: loss=0.748, reward_mean=0.070, reward_bound=0.000, batch=194\n",
      "1977: loss=0.736, reward_mean=0.120, reward_bound=0.008, batch=206\n",
      "1978: loss=0.737, reward_mean=0.130, reward_bound=0.084, batch=214\n",
      "1979: loss=0.735, reward_mean=0.100, reward_bound=0.133, batch=220\n",
      "1980: loss=0.739, reward_mean=0.140, reward_bound=0.167, batch=223\n",
      "1981: loss=0.740, reward_mean=0.110, reward_bound=0.206, batch=224\n",
      "1982: loss=0.741, reward_mean=0.110, reward_bound=0.226, batch=227\n",
      "1983: loss=0.741, reward_mean=0.090, reward_bound=0.229, batch=225\n",
      "1984: loss=0.744, reward_mean=0.090, reward_bound=0.254, batch=224\n",
      "1985: loss=0.745, reward_mean=0.050, reward_bound=0.247, batch=227\n",
      "1986: loss=0.740, reward_mean=0.120, reward_bound=0.282, batch=221\n",
      "1987: loss=0.740, reward_mean=0.130, reward_bound=0.282, batch=224\n",
      "1988: loss=0.747, reward_mean=0.160, reward_bound=0.314, batch=218\n",
      "1989: loss=0.749, reward_mean=0.100, reward_bound=0.314, batch=221\n",
      "1990: loss=0.748, reward_mean=0.110, reward_bound=0.314, batch=224\n",
      "1991: loss=0.755, reward_mean=0.170, reward_bound=0.349, batch=183\n",
      "1992: loss=0.755, reward_mean=0.080, reward_bound=0.000, batch=191\n",
      "1993: loss=0.739, reward_mean=0.100, reward_bound=0.000, batch=201\n",
      "1994: loss=0.738, reward_mean=0.050, reward_bound=0.000, batch=206\n",
      "1995: loss=0.746, reward_mean=0.160, reward_bound=0.089, batch=213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996: loss=0.749, reward_mean=0.080, reward_bound=0.095, batch=219\n",
      "1997: loss=0.760, reward_mean=0.190, reward_bound=0.174, batch=223\n",
      "1998: loss=0.754, reward_mean=0.080, reward_bound=0.198, batch=226\n",
      "1999: loss=0.752, reward_mean=0.100, reward_bound=0.196, batch=228\n",
      "2000: loss=0.755, reward_mean=0.150, reward_bound=0.229, batch=221\n",
      "2001: loss=0.755, reward_mean=0.100, reward_bound=0.185, batch=224\n",
      "2002: loss=0.756, reward_mean=0.140, reward_bound=0.254, batch=223\n",
      "2003: loss=0.759, reward_mean=0.160, reward_bound=0.282, batch=218\n",
      "2004: loss=0.758, reward_mean=0.130, reward_bound=0.314, batch=216\n",
      "2005: loss=0.761, reward_mean=0.070, reward_bound=0.277, batch=221\n",
      "2006: loss=0.762, reward_mean=0.120, reward_bound=0.254, batch=224\n",
      "2007: loss=0.759, reward_mean=0.060, reward_bound=0.135, batch=226\n",
      "2008: loss=0.759, reward_mean=0.130, reward_bound=0.314, batch=226\n",
      "2009: loss=0.762, reward_mean=0.070, reward_bound=0.349, batch=216\n",
      "2010: loss=0.764, reward_mean=0.120, reward_bound=0.316, batch=221\n",
      "2011: loss=0.761, reward_mean=0.070, reward_bound=0.282, batch=224\n",
      "2012: loss=0.761, reward_mean=0.100, reward_bound=0.308, batch=227\n",
      "2013: loss=0.760, reward_mean=0.060, reward_bound=0.220, batch=229\n",
      "2014: loss=0.764, reward_mean=0.070, reward_bound=0.328, batch=230\n",
      "2015: loss=0.766, reward_mean=0.140, reward_bound=0.349, batch=227\n",
      "2016: loss=0.766, reward_mean=0.120, reward_bound=0.314, batch=228\n",
      "2017: loss=0.784, reward_mean=0.100, reward_bound=0.387, batch=157\n",
      "2018: loss=0.782, reward_mean=0.090, reward_bound=0.000, batch=166\n",
      "2019: loss=0.771, reward_mean=0.090, reward_bound=0.000, batch=175\n",
      "2020: loss=0.770, reward_mean=0.140, reward_bound=0.000, batch=189\n",
      "2021: loss=0.759, reward_mean=0.090, reward_bound=0.000, batch=198\n",
      "2022: loss=0.757, reward_mean=0.100, reward_bound=0.002, batch=208\n",
      "2023: loss=0.754, reward_mean=0.070, reward_bound=0.002, batch=215\n",
      "2024: loss=0.752, reward_mean=0.120, reward_bound=0.073, batch=220\n",
      "2025: loss=0.751, reward_mean=0.170, reward_bound=0.122, batch=222\n",
      "2026: loss=0.748, reward_mean=0.120, reward_bound=0.155, batch=225\n",
      "2027: loss=0.749, reward_mean=0.120, reward_bound=0.206, batch=224\n",
      "2028: loss=0.752, reward_mean=0.120, reward_bound=0.252, batch=227\n",
      "2029: loss=0.758, reward_mean=0.080, reward_bound=0.254, batch=220\n",
      "2030: loss=0.756, reward_mean=0.080, reward_bound=0.259, batch=224\n",
      "2031: loss=0.751, reward_mean=0.130, reward_bound=0.282, batch=223\n",
      "2032: loss=0.757, reward_mean=0.110, reward_bound=0.314, batch=217\n",
      "2033: loss=0.756, reward_mean=0.090, reward_bound=0.277, batch=222\n",
      "2034: loss=0.756, reward_mean=0.080, reward_bound=0.282, batch=222\n",
      "2035: loss=0.754, reward_mean=0.170, reward_bound=0.314, batch=224\n",
      "2036: loss=0.752, reward_mean=0.130, reward_bound=0.280, batch=227\n",
      "2037: loss=0.751, reward_mean=0.110, reward_bound=0.308, batch=229\n",
      "2038: loss=0.755, reward_mean=0.100, reward_bound=0.314, batch=229\n",
      "2039: loss=0.763, reward_mean=0.040, reward_bound=0.349, batch=211\n",
      "2040: loss=0.761, reward_mean=0.070, reward_bound=0.072, batch=217\n",
      "2041: loss=0.756, reward_mean=0.080, reward_bound=0.117, batch=222\n",
      "2042: loss=0.760, reward_mean=0.140, reward_bound=0.191, batch=225\n",
      "2043: loss=0.760, reward_mean=0.120, reward_bound=0.260, batch=227\n",
      "2044: loss=0.763, reward_mean=0.070, reward_bound=0.282, batch=228\n",
      "2045: loss=0.761, reward_mean=0.210, reward_bound=0.317, batch=229\n",
      "2046: loss=0.763, reward_mean=0.080, reward_bound=0.349, batch=226\n",
      "2047: loss=0.762, reward_mean=0.090, reward_bound=0.349, batch=227\n",
      "2048: loss=0.760, reward_mean=0.140, reward_bound=0.373, batch=229\n",
      "2049: loss=0.764, reward_mean=0.090, reward_bound=0.387, batch=209\n",
      "2050: loss=0.764, reward_mean=0.180, reward_bound=0.295, batch=216\n",
      "2051: loss=0.766, reward_mean=0.110, reward_bound=0.284, batch=221\n",
      "2052: loss=0.764, reward_mean=0.090, reward_bound=0.282, batch=224\n",
      "2053: loss=0.761, reward_mean=0.160, reward_bound=0.314, batch=222\n",
      "2054: loss=0.761, reward_mean=0.140, reward_bound=0.324, batch=225\n",
      "2055: loss=0.766, reward_mean=0.120, reward_bound=0.349, batch=223\n",
      "2056: loss=0.760, reward_mean=0.140, reward_bound=0.387, batch=221\n",
      "2057: loss=0.760, reward_mean=0.110, reward_bound=0.349, batch=224\n",
      "2058: loss=0.760, reward_mean=0.100, reward_bound=0.387, batch=225\n",
      "2059: loss=0.770, reward_mean=0.130, reward_bound=0.430, batch=133\n",
      "2060: loss=0.759, reward_mean=0.130, reward_bound=0.000, batch=146\n",
      "2061: loss=0.756, reward_mean=0.060, reward_bound=0.000, batch=152\n",
      "2062: loss=0.747, reward_mean=0.150, reward_bound=0.000, batch=167\n",
      "2063: loss=0.741, reward_mean=0.100, reward_bound=0.000, batch=177\n",
      "2064: loss=0.740, reward_mean=0.080, reward_bound=0.000, batch=185\n",
      "2065: loss=0.732, reward_mean=0.130, reward_bound=0.000, batch=198\n",
      "2066: loss=0.735, reward_mean=0.080, reward_bound=0.000, batch=206\n",
      "2067: loss=0.746, reward_mean=0.120, reward_bound=0.040, batch=214\n",
      "2068: loss=0.751, reward_mean=0.080, reward_bound=0.079, batch=220\n",
      "2069: loss=0.751, reward_mean=0.110, reward_bound=0.096, batch=224\n",
      "2070: loss=0.747, reward_mean=0.060, reward_bound=0.108, batch=227\n",
      "2071: loss=0.740, reward_mean=0.140, reward_bound=0.147, batch=229\n",
      "2072: loss=0.743, reward_mean=0.190, reward_bound=0.167, batch=223\n",
      "2073: loss=0.745, reward_mean=0.100, reward_bound=0.185, batch=220\n",
      "2074: loss=0.742, reward_mean=0.190, reward_bound=0.206, batch=231\n",
      "2075: loss=0.747, reward_mean=0.140, reward_bound=0.206, batch=231\n",
      "2076: loss=0.756, reward_mean=0.120, reward_bound=0.229, batch=220\n",
      "2077: loss=0.759, reward_mean=0.070, reward_bound=0.190, batch=224\n",
      "2078: loss=0.759, reward_mean=0.100, reward_bound=0.226, batch=227\n",
      "2079: loss=0.762, reward_mean=0.150, reward_bound=0.254, batch=218\n",
      "2080: loss=0.772, reward_mean=0.170, reward_bound=0.282, batch=215\n",
      "2081: loss=0.776, reward_mean=0.170, reward_bound=0.314, batch=211\n",
      "2082: loss=0.769, reward_mean=0.120, reward_bound=0.206, batch=217\n",
      "2083: loss=0.769, reward_mean=0.030, reward_bound=0.000, batch=220\n",
      "2084: loss=0.768, reward_mean=0.080, reward_bound=0.190, batch=224\n",
      "2085: loss=0.771, reward_mean=0.160, reward_bound=0.314, batch=221\n",
      "2086: loss=0.769, reward_mean=0.100, reward_bound=0.349, batch=204\n",
      "2087: loss=0.766, reward_mean=0.150, reward_bound=0.206, batch=212\n",
      "2088: loss=0.766, reward_mean=0.100, reward_bound=0.140, batch=218\n",
      "2089: loss=0.765, reward_mean=0.100, reward_bound=0.167, batch=221\n",
      "2090: loss=0.761, reward_mean=0.120, reward_bound=0.229, batch=224\n",
      "2091: loss=0.763, reward_mean=0.100, reward_bound=0.254, batch=225\n",
      "2092: loss=0.766, reward_mean=0.090, reward_bound=0.314, batch=222\n",
      "2093: loss=0.766, reward_mean=0.100, reward_bound=0.254, batch=225\n",
      "2094: loss=0.760, reward_mean=0.140, reward_bound=0.349, batch=224\n",
      "2095: loss=0.760, reward_mean=0.130, reward_bound=0.252, batch=227\n",
      "2096: loss=0.759, reward_mean=0.190, reward_bound=0.373, batch=229\n",
      "2097: loss=0.755, reward_mean=0.090, reward_bound=0.387, batch=194\n",
      "2098: loss=0.758, reward_mean=0.050, reward_bound=0.000, batch=199\n",
      "2099: loss=0.764, reward_mean=0.150, reward_bound=0.113, batch=209\n",
      "2100: loss=0.757, reward_mean=0.110, reward_bound=0.182, batch=216\n",
      "2101: loss=0.748, reward_mean=0.070, reward_bound=0.045, batch=221\n",
      "2102: loss=0.750, reward_mean=0.110, reward_bound=0.206, batch=223\n",
      "2103: loss=0.751, reward_mean=0.150, reward_bound=0.229, batch=224\n",
      "2104: loss=0.758, reward_mean=0.130, reward_bound=0.254, batch=223\n",
      "2105: loss=0.761, reward_mean=0.100, reward_bound=0.282, batch=223\n",
      "2106: loss=0.756, reward_mean=0.080, reward_bound=0.314, batch=220\n",
      "2107: loss=0.754, reward_mean=0.120, reward_bound=0.338, batch=224\n",
      "2108: loss=0.752, reward_mean=0.100, reward_bound=0.349, batch=217\n",
      "2109: loss=0.746, reward_mean=0.110, reward_bound=0.249, batch=222\n",
      "2110: loss=0.751, reward_mean=0.100, reward_bound=0.314, batch=223\n",
      "2111: loss=0.749, reward_mean=0.080, reward_bound=0.349, batch=222\n",
      "2112: loss=0.748, reward_mean=0.070, reward_bound=0.220, batch=225\n",
      "2113: loss=0.749, reward_mean=0.070, reward_bound=0.216, batch=227\n",
      "2114: loss=0.751, reward_mean=0.090, reward_bound=0.314, batch=228\n",
      "2115: loss=0.749, reward_mean=0.060, reward_bound=0.349, batch=227\n",
      "2116: loss=0.745, reward_mean=0.090, reward_bound=0.249, batch=229\n",
      "2117: loss=0.746, reward_mean=0.110, reward_bound=0.328, batch=230\n",
      "2118: loss=0.749, reward_mean=0.070, reward_bound=0.376, batch=231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2119: loss=0.752, reward_mean=0.070, reward_bound=0.387, batch=222\n",
      "2120: loss=0.750, reward_mean=0.140, reward_bound=0.349, batch=224\n",
      "2121: loss=0.751, reward_mean=0.130, reward_bound=0.314, batch=226\n",
      "2122: loss=0.748, reward_mean=0.090, reward_bound=0.368, batch=228\n",
      "2123: loss=0.747, reward_mean=0.160, reward_bound=0.392, batch=229\n",
      "2124: loss=0.747, reward_mean=0.100, reward_bound=0.430, batch=187\n",
      "2125: loss=0.748, reward_mean=0.060, reward_bound=0.000, batch=193\n",
      "2126: loss=0.747, reward_mean=0.080, reward_bound=0.000, batch=201\n",
      "2127: loss=0.740, reward_mean=0.120, reward_bound=0.089, batch=210\n",
      "2128: loss=0.736, reward_mean=0.060, reward_bound=0.000, batch=216\n",
      "2129: loss=0.733, reward_mean=0.120, reward_bound=0.122, batch=219\n",
      "2130: loss=0.728, reward_mean=0.070, reward_bound=0.150, batch=221\n",
      "2131: loss=0.734, reward_mean=0.170, reward_bound=0.254, batch=219\n",
      "2132: loss=0.730, reward_mean=0.130, reward_bound=0.265, batch=223\n",
      "2133: loss=0.745, reward_mean=0.130, reward_bound=0.282, batch=221\n",
      "2134: loss=0.741, reward_mean=0.120, reward_bound=0.282, batch=223\n",
      "2135: loss=0.744, reward_mean=0.170, reward_bound=0.301, batch=226\n",
      "2136: loss=0.741, reward_mean=0.100, reward_bound=0.260, batch=228\n",
      "2137: loss=0.737, reward_mean=0.100, reward_bound=0.314, batch=222\n",
      "2138: loss=0.735, reward_mean=0.100, reward_bound=0.282, batch=224\n",
      "2139: loss=0.733, reward_mean=0.130, reward_bound=0.311, batch=227\n",
      "2140: loss=0.731, reward_mean=0.060, reward_bound=0.297, batch=229\n",
      "2141: loss=0.734, reward_mean=0.110, reward_bound=0.314, batch=228\n",
      "2142: loss=0.738, reward_mean=0.080, reward_bound=0.349, batch=215\n",
      "2143: loss=0.739, reward_mean=0.120, reward_bound=0.281, batch=220\n",
      "2144: loss=0.740, reward_mean=0.170, reward_bound=0.304, batch=224\n",
      "2145: loss=0.738, reward_mean=0.090, reward_bound=0.314, batch=226\n",
      "2146: loss=0.742, reward_mean=0.100, reward_bound=0.349, batch=226\n",
      "2147: loss=0.743, reward_mean=0.110, reward_bound=0.368, batch=228\n",
      "2148: loss=0.744, reward_mean=0.100, reward_bound=0.387, batch=217\n",
      "2149: loss=0.741, reward_mean=0.100, reward_bound=0.277, batch=222\n",
      "2150: loss=0.738, reward_mean=0.150, reward_bound=0.282, batch=223\n",
      "2151: loss=0.740, reward_mean=0.110, reward_bound=0.349, batch=224\n",
      "2152: loss=0.742, reward_mean=0.090, reward_bound=0.314, batch=226\n",
      "2153: loss=0.740, reward_mean=0.090, reward_bound=0.331, batch=228\n",
      "2154: loss=0.742, reward_mean=0.100, reward_bound=0.353, batch=229\n",
      "2155: loss=0.741, reward_mean=0.100, reward_bound=0.343, batch=230\n",
      "2156: loss=0.740, reward_mean=0.090, reward_bound=0.365, batch=231\n",
      "2157: loss=0.741, reward_mean=0.140, reward_bound=0.387, batch=224\n",
      "2158: loss=0.738, reward_mean=0.110, reward_bound=0.311, batch=227\n",
      "2159: loss=0.739, reward_mean=0.100, reward_bound=0.314, batch=228\n",
      "2160: loss=0.737, reward_mean=0.120, reward_bound=0.353, batch=229\n",
      "2161: loss=0.737, reward_mean=0.020, reward_bound=0.265, batch=230\n",
      "2162: loss=0.736, reward_mean=0.170, reward_bound=0.387, batch=230\n",
      "2163: loss=0.742, reward_mean=0.050, reward_bound=0.430, batch=207\n",
      "2164: loss=0.739, reward_mean=0.130, reward_bound=0.245, batch=215\n",
      "2165: loss=0.737, reward_mean=0.110, reward_bound=0.254, batch=218\n",
      "2166: loss=0.743, reward_mean=0.140, reward_bound=0.282, batch=221\n",
      "2167: loss=0.746, reward_mean=0.190, reward_bound=0.349, batch=223\n",
      "2168: loss=0.749, reward_mean=0.120, reward_bound=0.335, batch=226\n",
      "2169: loss=0.746, reward_mean=0.110, reward_bound=0.349, batch=227\n",
      "2170: loss=0.746, reward_mean=0.130, reward_bound=0.387, batch=222\n",
      "2171: loss=0.745, reward_mean=0.160, reward_bound=0.373, batch=225\n",
      "2172: loss=0.745, reward_mean=0.130, reward_bound=0.430, batch=221\n",
      "2173: loss=0.742, reward_mean=0.110, reward_bound=0.282, batch=223\n",
      "2174: loss=0.744, reward_mean=0.100, reward_bound=0.349, batch=224\n",
      "2175: loss=0.747, reward_mean=0.110, reward_bound=0.387, batch=226\n",
      "2176: loss=0.747, reward_mean=0.130, reward_bound=0.409, batch=228\n",
      "2177: loss=0.746, reward_mean=0.120, reward_bound=0.430, batch=227\n",
      "2178: loss=0.705, reward_mean=0.120, reward_bound=0.478, batch=91\n",
      "2179: loss=0.674, reward_mean=0.140, reward_bound=0.000, batch=105\n",
      "2180: loss=0.675, reward_mean=0.150, reward_bound=0.000, batch=120\n",
      "2181: loss=0.664, reward_mean=0.180, reward_bound=0.000, batch=138\n",
      "2182: loss=0.671, reward_mean=0.100, reward_bound=0.000, batch=148\n",
      "2183: loss=0.684, reward_mean=0.140, reward_bound=0.000, batch=162\n",
      "2184: loss=0.684, reward_mean=0.110, reward_bound=0.000, batch=173\n",
      "2185: loss=0.678, reward_mean=0.110, reward_bound=0.000, batch=184\n",
      "2186: loss=0.671, reward_mean=0.130, reward_bound=0.000, batch=197\n",
      "2187: loss=0.673, reward_mean=0.150, reward_bound=0.024, batch=208\n",
      "2188: loss=0.669, reward_mean=0.130, reward_bound=0.048, batch=215\n",
      "2189: loss=0.663, reward_mean=0.180, reward_bound=0.080, batch=217\n",
      "2190: loss=0.661, reward_mean=0.080, reward_bound=0.089, batch=224\n",
      "2191: loss=0.665, reward_mean=0.210, reward_bound=0.149, batch=227\n",
      "2192: loss=0.666, reward_mean=0.160, reward_bound=0.163, batch=229\n",
      "2193: loss=0.663, reward_mean=0.180, reward_bound=0.174, batch=230\n",
      "2194: loss=0.665, reward_mean=0.230, reward_bound=0.200, batch=231\n",
      "2195: loss=0.660, reward_mean=0.110, reward_bound=0.206, batch=224\n",
      "2196: loss=0.661, reward_mean=0.220, reward_bound=0.229, batch=223\n",
      "2197: loss=0.659, reward_mean=0.140, reward_bound=0.254, batch=212\n",
      "2198: loss=0.660, reward_mean=0.100, reward_bound=0.229, batch=218\n",
      "2199: loss=0.670, reward_mean=0.180, reward_bound=0.282, batch=206\n",
      "2200: loss=0.668, reward_mean=0.180, reward_bound=0.185, batch=213\n",
      "2201: loss=0.668, reward_mean=0.110, reward_bound=0.117, batch=219\n",
      "2202: loss=0.670, reward_mean=0.180, reward_bound=0.215, batch=223\n",
      "2203: loss=0.670, reward_mean=0.150, reward_bound=0.229, batch=223\n",
      "2204: loss=0.671, reward_mean=0.120, reward_bound=0.254, batch=224\n",
      "2205: loss=0.671, reward_mean=0.150, reward_bound=0.280, batch=227\n",
      "2206: loss=0.667, reward_mean=0.140, reward_bound=0.282, batch=228\n",
      "2207: loss=0.666, reward_mean=0.120, reward_bound=0.286, batch=229\n",
      "2208: loss=0.666, reward_mean=0.140, reward_bound=0.314, batch=209\n",
      "2209: loss=0.667, reward_mean=0.140, reward_bound=0.229, batch=215\n",
      "2210: loss=0.668, reward_mean=0.140, reward_bound=0.234, batch=220\n",
      "2211: loss=0.663, reward_mean=0.120, reward_bound=0.254, batch=223\n",
      "2212: loss=0.663, reward_mean=0.240, reward_bound=0.282, batch=225\n",
      "2213: loss=0.664, reward_mean=0.170, reward_bound=0.314, batch=226\n",
      "2214: loss=0.675, reward_mean=0.150, reward_bound=0.349, batch=200\n",
      "2215: loss=0.675, reward_mean=0.190, reward_bound=0.247, batch=210\n",
      "2216: loss=0.670, reward_mean=0.160, reward_bound=0.247, batch=217\n",
      "2217: loss=0.668, reward_mean=0.170, reward_bound=0.254, batch=221\n",
      "2218: loss=0.672, reward_mean=0.160, reward_bound=0.282, batch=222\n",
      "2219: loss=0.667, reward_mean=0.110, reward_bound=0.314, batch=222\n",
      "2220: loss=0.665, reward_mean=0.080, reward_bound=0.201, batch=225\n",
      "2221: loss=0.665, reward_mean=0.190, reward_bound=0.321, batch=227\n",
      "2222: loss=0.669, reward_mean=0.100, reward_bound=0.349, batch=220\n",
      "2223: loss=0.670, reward_mean=0.120, reward_bound=0.338, batch=224\n",
      "2224: loss=0.669, reward_mean=0.170, reward_bound=0.345, batch=227\n",
      "2225: loss=0.670, reward_mean=0.100, reward_bound=0.314, batch=228\n",
      "2226: loss=0.670, reward_mean=0.100, reward_bound=0.349, batch=226\n",
      "2227: loss=0.670, reward_mean=0.100, reward_bound=0.298, batch=228\n",
      "2228: loss=0.670, reward_mean=0.100, reward_bound=0.317, batch=229\n",
      "2229: loss=0.668, reward_mean=0.130, reward_bound=0.349, batch=229\n",
      "2230: loss=0.667, reward_mean=0.150, reward_bound=0.364, batch=230\n",
      "2231: loss=0.681, reward_mean=0.100, reward_bound=0.387, batch=181\n",
      "2232: loss=0.681, reward_mean=0.200, reward_bound=0.150, batch=195\n",
      "2233: loss=0.678, reward_mean=0.120, reward_bound=0.097, batch=206\n",
      "2234: loss=0.671, reward_mean=0.100, reward_bound=0.122, batch=214\n",
      "2235: loss=0.666, reward_mean=0.080, reward_bound=0.084, batch=220\n",
      "2236: loss=0.666, reward_mean=0.100, reward_bound=0.146, batch=224\n",
      "2237: loss=0.663, reward_mean=0.140, reward_bound=0.167, batch=224\n",
      "2238: loss=0.656, reward_mean=0.170, reward_bound=0.226, batch=227\n",
      "2239: loss=0.661, reward_mean=0.080, reward_bound=0.229, batch=225\n",
      "2240: loss=0.662, reward_mean=0.120, reward_bound=0.254, batch=224\n",
      "2241: loss=0.661, reward_mean=0.190, reward_bound=0.282, batch=220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242: loss=0.658, reward_mean=0.150, reward_bound=0.304, batch=224\n",
      "2243: loss=0.656, reward_mean=0.130, reward_bound=0.311, batch=227\n",
      "2244: loss=0.654, reward_mean=0.100, reward_bound=0.282, batch=228\n",
      "2245: loss=0.669, reward_mean=0.140, reward_bound=0.314, batch=219\n",
      "2246: loss=0.667, reward_mean=0.150, reward_bound=0.328, batch=223\n",
      "2247: loss=0.665, reward_mean=0.080, reward_bound=0.280, batch=226\n",
      "2248: loss=0.668, reward_mean=0.070, reward_bound=0.268, batch=228\n",
      "2249: loss=0.665, reward_mean=0.080, reward_bound=0.314, batch=228\n",
      "2250: loss=0.670, reward_mean=0.090, reward_bound=0.349, batch=217\n",
      "2251: loss=0.664, reward_mean=0.140, reward_bound=0.277, batch=222\n",
      "2252: loss=0.666, reward_mean=0.110, reward_bound=0.245, batch=225\n",
      "2253: loss=0.668, reward_mean=0.080, reward_bound=0.260, batch=227\n",
      "2254: loss=0.668, reward_mean=0.120, reward_bound=0.282, batch=225\n",
      "2255: loss=0.674, reward_mean=0.100, reward_bound=0.321, batch=227\n",
      "2256: loss=0.675, reward_mean=0.120, reward_bound=0.349, batch=227\n",
      "2257: loss=0.680, reward_mean=0.150, reward_bound=0.387, batch=221\n",
      "2258: loss=0.679, reward_mean=0.150, reward_bound=0.314, batch=223\n",
      "2259: loss=0.676, reward_mean=0.070, reward_bound=0.154, batch=226\n",
      "2260: loss=0.676, reward_mean=0.180, reward_bound=0.314, batch=226\n",
      "2261: loss=0.675, reward_mean=0.130, reward_bound=0.331, batch=228\n",
      "2262: loss=0.675, reward_mean=0.100, reward_bound=0.387, batch=226\n",
      "2263: loss=0.672, reward_mean=0.110, reward_bound=0.430, batch=162\n",
      "2264: loss=0.670, reward_mean=0.110, reward_bound=0.000, batch=173\n",
      "2265: loss=0.657, reward_mean=0.100, reward_bound=0.000, batch=183\n",
      "2266: loss=0.644, reward_mean=0.140, reward_bound=0.000, batch=197\n",
      "2267: loss=0.644, reward_mean=0.090, reward_bound=0.000, batch=206\n",
      "2268: loss=0.659, reward_mean=0.140, reward_bound=0.047, batch=213\n",
      "2269: loss=0.653, reward_mean=0.170, reward_bound=0.139, batch=219\n",
      "2270: loss=0.650, reward_mean=0.080, reward_bound=0.150, batch=221\n",
      "2271: loss=0.651, reward_mean=0.120, reward_bound=0.185, batch=222\n",
      "2272: loss=0.647, reward_mean=0.170, reward_bound=0.206, batch=230\n",
      "2273: loss=0.643, reward_mean=0.120, reward_bound=0.206, batch=236\n",
      "2274: loss=0.646, reward_mean=0.110, reward_bound=0.206, batch=232\n",
      "2275: loss=0.650, reward_mean=0.070, reward_bound=0.229, batch=228\n",
      "2276: loss=0.652, reward_mean=0.120, reward_bound=0.254, batch=227\n",
      "2277: loss=0.661, reward_mean=0.140, reward_bound=0.282, batch=226\n",
      "2278: loss=0.663, reward_mean=0.120, reward_bound=0.244, batch=228\n",
      "2279: loss=0.658, reward_mean=0.120, reward_bound=0.314, batch=219\n",
      "2280: loss=0.652, reward_mean=0.140, reward_bound=0.250, batch=223\n",
      "2281: loss=0.655, reward_mean=0.200, reward_bound=0.335, batch=226\n",
      "2282: loss=0.654, reward_mean=0.110, reward_bound=0.316, batch=228\n",
      "2283: loss=0.651, reward_mean=0.110, reward_bound=0.349, batch=214\n",
      "2284: loss=0.651, reward_mean=0.110, reward_bound=0.183, batch=220\n",
      "2285: loss=0.651, reward_mean=0.090, reward_bound=0.222, batch=224\n",
      "2286: loss=0.649, reward_mean=0.130, reward_bound=0.311, batch=227\n",
      "2287: loss=0.649, reward_mean=0.100, reward_bound=0.314, batch=227\n",
      "2288: loss=0.651, reward_mean=0.160, reward_bound=0.349, batch=223\n",
      "2289: loss=0.655, reward_mean=0.130, reward_bound=0.387, batch=213\n",
      "2290: loss=0.658, reward_mean=0.110, reward_bound=0.244, batch=219\n",
      "2291: loss=0.658, reward_mean=0.110, reward_bound=0.254, batch=222\n",
      "2292: loss=0.660, reward_mean=0.100, reward_bound=0.282, batch=223\n",
      "2293: loss=0.660, reward_mean=0.120, reward_bound=0.301, batch=226\n",
      "2294: loss=0.661, reward_mean=0.110, reward_bound=0.298, batch=228\n",
      "2295: loss=0.661, reward_mean=0.090, reward_bound=0.317, batch=229\n",
      "2296: loss=0.659, reward_mean=0.190, reward_bound=0.349, batch=229\n",
      "2297: loss=0.659, reward_mean=0.190, reward_bound=0.387, batch=229\n",
      "2298: loss=0.659, reward_mean=0.180, reward_bound=0.405, batch=230\n",
      "2299: loss=0.673, reward_mean=0.190, reward_bound=0.430, batch=202\n",
      "2300: loss=0.670, reward_mean=0.110, reward_bound=0.150, batch=210\n",
      "2301: loss=0.670, reward_mean=0.150, reward_bound=0.210, batch=217\n",
      "2302: loss=0.667, reward_mean=0.140, reward_bound=0.224, batch=222\n",
      "2303: loss=0.665, reward_mean=0.130, reward_bound=0.254, batch=224\n",
      "2304: loss=0.664, reward_mean=0.130, reward_bound=0.282, batch=226\n",
      "2305: loss=0.662, reward_mean=0.170, reward_bound=0.298, batch=228\n",
      "2306: loss=0.661, reward_mean=0.140, reward_bound=0.314, batch=225\n",
      "2307: loss=0.668, reward_mean=0.110, reward_bound=0.349, batch=221\n",
      "2308: loss=0.664, reward_mean=0.090, reward_bound=0.254, batch=224\n",
      "2309: loss=0.666, reward_mean=0.150, reward_bound=0.280, batch=227\n",
      "2310: loss=0.665, reward_mean=0.130, reward_bound=0.314, batch=227\n",
      "2311: loss=0.666, reward_mean=0.160, reward_bound=0.380, batch=229\n",
      "2312: loss=0.665, reward_mean=0.080, reward_bound=0.295, batch=230\n",
      "2313: loss=0.669, reward_mean=0.200, reward_bound=0.387, batch=221\n",
      "2314: loss=0.672, reward_mean=0.200, reward_bound=0.314, batch=224\n",
      "2315: loss=0.671, reward_mean=0.090, reward_bound=0.252, batch=227\n",
      "2316: loss=0.670, reward_mean=0.090, reward_bound=0.308, batch=229\n",
      "2317: loss=0.669, reward_mean=0.070, reward_bound=0.249, batch=230\n",
      "2318: loss=0.670, reward_mean=0.070, reward_bound=0.314, batch=229\n",
      "2319: loss=0.669, reward_mean=0.190, reward_bound=0.328, batch=230\n",
      "2320: loss=0.671, reward_mean=0.150, reward_bound=0.376, batch=231\n",
      "2321: loss=0.673, reward_mean=0.180, reward_bound=0.430, batch=220\n",
      "2322: loss=0.673, reward_mean=0.100, reward_bound=0.240, batch=224\n",
      "2323: loss=0.674, reward_mean=0.100, reward_bound=0.280, batch=227\n",
      "2324: loss=0.675, reward_mean=0.120, reward_bound=0.277, batch=229\n",
      "2325: loss=0.671, reward_mean=0.200, reward_bound=0.349, batch=228\n",
      "2326: loss=0.667, reward_mean=0.130, reward_bound=0.387, batch=226\n",
      "2327: loss=0.672, reward_mean=0.120, reward_bound=0.430, batch=224\n",
      "2328: loss=0.673, reward_mean=0.110, reward_bound=0.254, batch=226\n",
      "2329: loss=0.671, reward_mean=0.060, reward_bound=0.316, batch=228\n",
      "2330: loss=0.676, reward_mean=0.120, reward_bound=0.349, batch=228\n",
      "2331: loss=0.674, reward_mean=0.150, reward_bound=0.387, batch=228\n",
      "2332: loss=0.673, reward_mean=0.080, reward_bound=0.430, batch=227\n",
      "2333: loss=0.676, reward_mean=0.100, reward_bound=0.422, batch=229\n",
      "2334: loss=0.676, reward_mean=0.100, reward_bound=0.387, batch=229\n",
      "2335: loss=0.674, reward_mean=0.100, reward_bound=0.478, batch=231\n",
      "2336: loss=0.694, reward_mean=0.140, reward_bound=0.478, batch=149\n",
      "2337: loss=0.679, reward_mean=0.110, reward_bound=0.000, batch=160\n",
      "2338: loss=0.669, reward_mean=0.120, reward_bound=0.000, batch=172\n",
      "2339: loss=0.645, reward_mean=0.170, reward_bound=0.000, batch=189\n",
      "2340: loss=0.641, reward_mean=0.110, reward_bound=0.000, batch=200\n",
      "2341: loss=0.643, reward_mean=0.110, reward_bound=0.026, batch=210\n",
      "2342: loss=0.646, reward_mean=0.150, reward_bound=0.070, batch=217\n",
      "2343: loss=0.655, reward_mean=0.120, reward_bound=0.098, batch=219\n",
      "2344: loss=0.657, reward_mean=0.160, reward_bound=0.150, batch=219\n",
      "2345: loss=0.659, reward_mean=0.090, reward_bound=0.167, batch=218\n",
      "2346: loss=0.668, reward_mean=0.110, reward_bound=0.185, batch=217\n",
      "2347: loss=0.664, reward_mean=0.110, reward_bound=0.135, batch=221\n",
      "2348: loss=0.669, reward_mean=0.130, reward_bound=0.206, batch=224\n",
      "2349: loss=0.669, reward_mean=0.120, reward_bound=0.229, batch=226\n",
      "2350: loss=0.673, reward_mean=0.080, reward_bound=0.254, batch=222\n",
      "2351: loss=0.680, reward_mean=0.120, reward_bound=0.282, batch=213\n",
      "2352: loss=0.687, reward_mean=0.120, reward_bound=0.314, batch=205\n",
      "2353: loss=0.684, reward_mean=0.090, reward_bound=0.091, batch=213\n",
      "2354: loss=0.676, reward_mean=0.180, reward_bound=0.301, batch=219\n",
      "2355: loss=0.676, reward_mean=0.110, reward_bound=0.174, batch=223\n",
      "2356: loss=0.674, reward_mean=0.090, reward_bound=0.261, batch=226\n",
      "2357: loss=0.685, reward_mean=0.170, reward_bound=0.314, batch=223\n",
      "2358: loss=0.683, reward_mean=0.100, reward_bound=0.282, batch=225\n",
      "2359: loss=0.674, reward_mean=0.070, reward_bound=0.349, batch=210\n",
      "2360: loss=0.668, reward_mean=0.100, reward_bound=0.211, batch=217\n",
      "2361: loss=0.662, reward_mean=0.070, reward_bound=0.216, batch=222\n",
      "2362: loss=0.660, reward_mean=0.080, reward_bound=0.254, batch=224\n",
      "2363: loss=0.666, reward_mean=0.120, reward_bound=0.282, batch=225\n",
      "2364: loss=0.667, reward_mean=0.180, reward_bound=0.349, batch=224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2365: loss=0.670, reward_mean=0.090, reward_bound=0.384, batch=227\n",
      "2366: loss=0.672, reward_mean=0.180, reward_bound=0.349, batch=228\n",
      "2367: loss=0.682, reward_mean=0.180, reward_bound=0.387, batch=208\n",
      "2368: loss=0.685, reward_mean=0.130, reward_bound=0.158, batch=215\n",
      "2369: loss=0.690, reward_mean=0.170, reward_bound=0.254, batch=219\n",
      "2370: loss=0.691, reward_mean=0.180, reward_bound=0.314, batch=222\n",
      "2371: loss=0.688, reward_mean=0.080, reward_bound=0.302, batch=225\n",
      "2372: loss=0.691, reward_mean=0.100, reward_bound=0.349, batch=224\n",
      "2373: loss=0.692, reward_mean=0.140, reward_bound=0.384, batch=227\n",
      "2374: loss=0.693, reward_mean=0.130, reward_bound=0.349, batch=228\n",
      "2375: loss=0.688, reward_mean=0.110, reward_bound=0.387, batch=223\n",
      "2376: loss=0.687, reward_mean=0.140, reward_bound=0.349, batch=225\n",
      "2377: loss=0.686, reward_mean=0.100, reward_bound=0.337, batch=227\n",
      "2378: loss=0.685, reward_mean=0.090, reward_bound=0.314, batch=228\n",
      "2379: loss=0.682, reward_mean=0.150, reward_bound=0.430, batch=202\n",
      "2380: loss=0.684, reward_mean=0.130, reward_bound=0.236, batch=211\n",
      "2381: loss=0.677, reward_mean=0.150, reward_bound=0.254, batch=217\n",
      "2382: loss=0.675, reward_mean=0.090, reward_bound=0.267, batch=222\n",
      "2383: loss=0.673, reward_mean=0.070, reward_bound=0.213, batch=225\n",
      "2384: loss=0.674, reward_mean=0.090, reward_bound=0.282, batch=224\n",
      "2385: loss=0.676, reward_mean=0.120, reward_bound=0.349, batch=224\n",
      "2386: loss=0.676, reward_mean=0.110, reward_bound=0.282, batch=226\n",
      "2387: loss=0.683, reward_mean=0.120, reward_bound=0.387, batch=221\n",
      "2388: loss=0.680, reward_mean=0.140, reward_bound=0.349, batch=223\n",
      "2389: loss=0.682, reward_mean=0.080, reward_bound=0.254, batch=225\n",
      "2390: loss=0.683, reward_mean=0.140, reward_bound=0.296, batch=227\n",
      "2391: loss=0.675, reward_mean=0.160, reward_bound=0.430, batch=217\n",
      "2392: loss=0.671, reward_mean=0.130, reward_bound=0.198, batch=222\n",
      "2393: loss=0.667, reward_mean=0.120, reward_bound=0.314, batch=224\n",
      "2394: loss=0.666, reward_mean=0.100, reward_bound=0.349, batch=226\n",
      "2395: loss=0.674, reward_mean=0.110, reward_bound=0.387, batch=223\n",
      "2396: loss=0.673, reward_mean=0.110, reward_bound=0.430, batch=223\n",
      "2397: loss=0.672, reward_mean=0.050, reward_bound=0.301, batch=226\n",
      "2398: loss=0.671, reward_mean=0.150, reward_bound=0.454, batch=228\n",
      "2399: loss=0.671, reward_mean=0.160, reward_bound=0.478, batch=231\n",
      "2400: loss=0.680, reward_mean=0.110, reward_bound=0.478, batch=189\n",
      "2401: loss=0.679, reward_mean=0.100, reward_bound=0.000, batch=199\n",
      "2402: loss=0.677, reward_mean=0.090, reward_bound=0.000, batch=208\n",
      "2403: loss=0.660, reward_mean=0.100, reward_bound=0.090, batch=215\n",
      "2404: loss=0.659, reward_mean=0.130, reward_bound=0.134, batch=220\n",
      "2405: loss=0.659, reward_mean=0.060, reward_bound=0.170, batch=224\n",
      "2406: loss=0.663, reward_mean=0.150, reward_bound=0.185, batch=226\n",
      "2407: loss=0.662, reward_mean=0.120, reward_bound=0.217, batch=228\n",
      "2408: loss=0.669, reward_mean=0.090, reward_bound=0.254, batch=227\n",
      "2409: loss=0.678, reward_mean=0.150, reward_bound=0.282, batch=223\n",
      "2410: loss=0.681, reward_mean=0.070, reward_bound=0.301, batch=226\n",
      "2411: loss=0.681, reward_mean=0.130, reward_bound=0.314, batch=225\n",
      "2412: loss=0.683, reward_mean=0.090, reward_bound=0.349, batch=215\n",
      "2413: loss=0.683, reward_mean=0.090, reward_bound=0.122, batch=219\n",
      "2414: loss=0.680, reward_mean=0.130, reward_bound=0.328, batch=223\n",
      "2415: loss=0.678, reward_mean=0.180, reward_bound=0.387, batch=218\n",
      "2416: loss=0.674, reward_mean=0.140, reward_bound=0.349, batch=220\n",
      "2417: loss=0.671, reward_mean=0.180, reward_bound=0.347, batch=224\n",
      "2418: loss=0.680, reward_mean=0.100, reward_bound=0.387, batch=224\n",
      "2419: loss=0.683, reward_mean=0.120, reward_bound=0.342, batch=227\n",
      "2420: loss=0.678, reward_mean=0.070, reward_bound=0.213, batch=229\n",
      "2421: loss=0.680, reward_mean=0.110, reward_bound=0.364, batch=230\n",
      "2422: loss=0.679, reward_mean=0.120, reward_bound=0.376, batch=231\n",
      "2423: loss=0.680, reward_mean=0.090, reward_bound=0.387, batch=230\n",
      "2424: loss=0.681, reward_mean=0.100, reward_bound=0.376, batch=231\n",
      "2425: loss=0.681, reward_mean=0.110, reward_bound=0.349, batch=231\n",
      "2426: loss=0.674, reward_mean=0.100, reward_bound=0.430, batch=210\n",
      "2427: loss=0.679, reward_mean=0.130, reward_bound=0.266, batch=217\n",
      "2428: loss=0.678, reward_mean=0.070, reward_bound=0.172, batch=222\n",
      "2429: loss=0.668, reward_mean=0.170, reward_bound=0.324, batch=225\n",
      "2430: loss=0.668, reward_mean=0.170, reward_bound=0.349, batch=224\n",
      "2431: loss=0.673, reward_mean=0.120, reward_bound=0.387, batch=223\n",
      "2432: loss=0.674, reward_mean=0.090, reward_bound=0.254, batch=225\n",
      "2433: loss=0.678, reward_mean=0.180, reward_bound=0.430, batch=218\n",
      "2434: loss=0.678, reward_mean=0.070, reward_bound=0.206, batch=221\n",
      "2435: loss=0.671, reward_mean=0.070, reward_bound=0.229, batch=223\n",
      "2436: loss=0.671, reward_mean=0.100, reward_bound=0.349, batch=225\n",
      "2437: loss=0.677, reward_mean=0.110, reward_bound=0.387, batch=225\n",
      "2438: loss=0.678, reward_mean=0.120, reward_bound=0.387, batch=226\n",
      "2439: loss=0.680, reward_mean=0.090, reward_bound=0.349, batch=227\n",
      "2440: loss=0.678, reward_mean=0.110, reward_bound=0.430, batch=227\n",
      "2441: loss=0.672, reward_mean=0.120, reward_bound=0.335, batch=229\n",
      "2442: loss=0.675, reward_mean=0.120, reward_bound=0.364, batch=230\n",
      "2443: loss=0.673, reward_mean=0.010, reward_bound=0.117, batch=231\n",
      "2444: loss=0.675, reward_mean=0.080, reward_bound=0.282, batch=231\n",
      "2445: loss=0.675, reward_mean=0.120, reward_bound=0.387, batch=230\n",
      "2446: loss=0.674, reward_mean=0.090, reward_bound=0.376, batch=231\n",
      "2447: loss=0.679, reward_mean=0.140, reward_bound=0.430, batch=230\n",
      "2448: loss=0.680, reward_mean=0.070, reward_bound=0.376, batch=231\n",
      "2449: loss=0.679, reward_mean=0.090, reward_bound=0.387, batch=231\n",
      "2450: loss=0.677, reward_mean=0.130, reward_bound=0.430, batch=231\n",
      "2451: loss=0.677, reward_mean=0.070, reward_bound=0.387, batch=231\n",
      "2452: loss=0.677, reward_mean=0.130, reward_bound=0.478, batch=203\n",
      "2453: loss=0.675, reward_mean=0.130, reward_bound=0.150, batch=210\n",
      "2454: loss=0.678, reward_mean=0.120, reward_bound=0.296, batch=217\n",
      "2455: loss=0.679, reward_mean=0.100, reward_bound=0.308, batch=222\n",
      "2456: loss=0.676, reward_mean=0.130, reward_bound=0.292, batch=225\n",
      "2457: loss=0.676, reward_mean=0.120, reward_bound=0.314, batch=224\n",
      "2458: loss=0.676, reward_mean=0.080, reward_bound=0.229, batch=226\n",
      "2459: loss=0.674, reward_mean=0.130, reward_bound=0.349, batch=226\n",
      "2460: loss=0.669, reward_mean=0.110, reward_bound=0.387, batch=223\n",
      "2461: loss=0.665, reward_mean=0.090, reward_bound=0.290, batch=226\n",
      "2462: loss=0.670, reward_mean=0.090, reward_bound=0.351, batch=228\n",
      "2463: loss=0.667, reward_mean=0.080, reward_bound=0.392, batch=229\n",
      "2464: loss=0.667, reward_mean=0.110, reward_bound=0.349, batch=229\n",
      "2465: loss=0.668, reward_mean=0.100, reward_bound=0.405, batch=230\n",
      "2466: loss=0.668, reward_mean=0.100, reward_bound=0.314, batch=230\n",
      "2467: loss=0.671, reward_mean=0.100, reward_bound=0.430, batch=220\n",
      "2468: loss=0.672, reward_mean=0.100, reward_bound=0.274, batch=224\n",
      "2469: loss=0.666, reward_mean=0.100, reward_bound=0.314, batch=226\n",
      "2470: loss=0.665, reward_mean=0.130, reward_bound=0.331, batch=228\n",
      "2471: loss=0.663, reward_mean=0.120, reward_bound=0.353, batch=229\n",
      "2472: loss=0.666, reward_mean=0.130, reward_bound=0.387, batch=229\n",
      "2473: loss=0.663, reward_mean=0.090, reward_bound=0.405, batch=230\n",
      "2474: loss=0.679, reward_mean=0.150, reward_bound=0.478, batch=215\n",
      "2475: loss=0.676, reward_mean=0.180, reward_bound=0.266, batch=220\n",
      "2476: loss=0.681, reward_mean=0.120, reward_bound=0.304, batch=224\n",
      "2477: loss=0.678, reward_mean=0.120, reward_bound=0.314, batch=225\n",
      "2478: loss=0.672, reward_mean=0.100, reward_bound=0.349, batch=226\n",
      "2479: loss=0.671, reward_mean=0.110, reward_bound=0.335, batch=228\n",
      "2480: loss=0.673, reward_mean=0.170, reward_bound=0.387, batch=228\n",
      "2481: loss=0.674, reward_mean=0.120, reward_bound=0.357, batch=229\n",
      "2482: loss=0.673, reward_mean=0.130, reward_bound=0.387, batch=229\n",
      "2483: loss=0.674, reward_mean=0.140, reward_bound=0.430, batch=229\n",
      "2484: loss=0.680, reward_mean=0.120, reward_bound=0.478, batch=232\n",
      "2485: loss=0.684, reward_mean=0.110, reward_bound=0.478, batch=221\n",
      "2486: loss=0.685, reward_mean=0.120, reward_bound=0.430, batch=224\n",
      "2487: loss=0.688, reward_mean=0.100, reward_bound=0.339, batch=227\n",
      "2488: loss=0.686, reward_mean=0.080, reward_bound=0.342, batch=229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489: loss=0.686, reward_mean=0.110, reward_bound=0.349, batch=229\n",
      "2490: loss=0.683, reward_mean=0.080, reward_bound=0.387, batch=227\n",
      "2491: loss=0.681, reward_mean=0.090, reward_bound=0.380, batch=229\n",
      "2492: loss=0.684, reward_mean=0.140, reward_bound=0.430, batch=228\n",
      "2493: loss=0.684, reward_mean=0.100, reward_bound=0.430, batch=228\n",
      "2494: loss=0.684, reward_mean=0.130, reward_bound=0.387, batch=228\n",
      "2495: loss=0.685, reward_mean=0.080, reward_bound=0.321, batch=229\n",
      "2496: loss=0.686, reward_mean=0.140, reward_bound=0.364, batch=230\n",
      "2497: loss=0.685, reward_mean=0.090, reward_bound=0.418, batch=231\n",
      "2498: loss=0.685, reward_mean=0.130, reward_bound=0.430, batch=229\n",
      "2499: loss=0.686, reward_mean=0.100, reward_bound=0.381, batch=230\n",
      "2500: loss=0.687, reward_mean=0.180, reward_bound=0.406, batch=231\n",
      "2501: loss=0.683, reward_mean=0.110, reward_bound=0.478, batch=225\n",
      "2502: loss=0.686, reward_mean=0.100, reward_bound=0.321, batch=227\n",
      "2503: loss=0.685, reward_mean=0.130, reward_bound=0.349, batch=228\n",
      "2504: loss=0.683, reward_mean=0.120, reward_bound=0.357, batch=229\n",
      "2505: loss=0.683, reward_mean=0.110, reward_bound=0.450, batch=230\n",
      "2506: loss=0.683, reward_mean=0.080, reward_bound=0.478, batch=227\n",
      "2507: loss=0.682, reward_mean=0.120, reward_bound=0.302, batch=229\n",
      "2508: loss=0.685, reward_mean=0.130, reward_bound=0.381, batch=230\n",
      "2509: loss=0.686, reward_mean=0.100, reward_bound=0.338, batch=231\n",
      "2510: loss=0.684, reward_mean=0.130, reward_bound=0.387, batch=231\n",
      "2511: loss=0.684, reward_mean=0.130, reward_bound=0.430, batch=230\n",
      "2512: loss=0.685, reward_mean=0.090, reward_bound=0.439, batch=231\n",
      "2513: loss=0.685, reward_mean=0.130, reward_bound=0.430, batch=231\n",
      "2514: loss=0.686, reward_mean=0.120, reward_bound=0.478, batch=230\n",
      "2515: loss=0.686, reward_mean=0.080, reward_bound=0.430, batch=230\n",
      "2516: loss=0.685, reward_mean=0.100, reward_bound=0.329, batch=231\n",
      "2517: loss=0.685, reward_mean=0.100, reward_bound=0.387, batch=231\n",
      "2519: loss=0.754, reward_mean=0.130, reward_bound=0.000, batch=13\n",
      "2520: loss=0.738, reward_mean=0.100, reward_bound=0.000, batch=23\n",
      "2521: loss=0.716, reward_mean=0.070, reward_bound=0.000, batch=30\n",
      "2522: loss=0.699, reward_mean=0.090, reward_bound=0.000, batch=39\n",
      "2523: loss=0.679, reward_mean=0.110, reward_bound=0.000, batch=50\n",
      "2524: loss=0.671, reward_mean=0.180, reward_bound=0.000, batch=68\n",
      "2525: loss=0.677, reward_mean=0.280, reward_bound=0.000, batch=96\n",
      "2526: loss=0.667, reward_mean=0.170, reward_bound=0.000, batch=113\n",
      "2527: loss=0.656, reward_mean=0.130, reward_bound=0.000, batch=126\n",
      "2528: loss=0.657, reward_mean=0.120, reward_bound=0.000, batch=138\n",
      "2529: loss=0.648, reward_mean=0.250, reward_bound=0.000, batch=163\n",
      "2530: loss=0.653, reward_mean=0.170, reward_bound=0.000, batch=180\n",
      "2531: loss=0.651, reward_mean=0.150, reward_bound=0.000, batch=195\n",
      "2532: loss=0.647, reward_mean=0.140, reward_bound=0.010, batch=206\n",
      "2533: loss=0.661, reward_mean=0.160, reward_bound=0.021, batch=214\n",
      "2534: loss=0.668, reward_mean=0.200, reward_bound=0.052, batch=220\n",
      "2535: loss=0.682, reward_mean=0.180, reward_bound=0.080, batch=219\n",
      "2536: loss=0.682, reward_mean=0.170, reward_bound=0.098, batch=216\n",
      "2537: loss=0.686, reward_mean=0.130, reward_bound=0.109, batch=220\n",
      "2538: loss=0.691, reward_mean=0.210, reward_bound=0.135, batch=214\n",
      "2539: loss=0.693, reward_mean=0.240, reward_bound=0.150, batch=215\n",
      "2540: loss=0.693, reward_mean=0.130, reward_bound=0.167, batch=213\n",
      "2541: loss=0.690, reward_mean=0.140, reward_bound=0.167, batch=218\n",
      "2542: loss=0.681, reward_mean=0.100, reward_bound=0.185, batch=205\n",
      "2543: loss=0.674, reward_mean=0.100, reward_bound=0.057, batch=213\n",
      "2544: loss=0.677, reward_mean=0.130, reward_bound=0.144, batch=219\n",
      "2545: loss=0.677, reward_mean=0.130, reward_bound=0.194, batch=223\n",
      "2546: loss=0.665, reward_mean=0.130, reward_bound=0.206, batch=207\n",
      "2547: loss=0.665, reward_mean=0.140, reward_bound=0.167, batch=214\n",
      "2548: loss=0.660, reward_mean=0.120, reward_bound=0.229, batch=201\n",
      "2549: loss=0.654, reward_mean=0.160, reward_bound=0.229, batch=210\n",
      "2550: loss=0.651, reward_mean=0.170, reward_bound=0.157, batch=217\n",
      "2551: loss=0.647, reward_mean=0.220, reward_bound=0.254, batch=201\n",
      "2552: loss=0.644, reward_mean=0.090, reward_bound=0.000, batch=210\n",
      "2553: loss=0.641, reward_mean=0.240, reward_bound=0.185, batch=216\n",
      "2554: loss=0.644, reward_mean=0.150, reward_bound=0.229, batch=219\n",
      "2555: loss=0.641, reward_mean=0.130, reward_bound=0.206, batch=222\n",
      "2556: loss=0.640, reward_mean=0.150, reward_bound=0.213, batch=225\n",
      "2557: loss=0.644, reward_mean=0.120, reward_bound=0.254, batch=220\n",
      "2558: loss=0.647, reward_mean=0.180, reward_bound=0.282, batch=190\n",
      "2559: loss=0.647, reward_mean=0.160, reward_bound=0.086, batch=203\n",
      "2560: loss=0.646, reward_mean=0.160, reward_bound=0.098, batch=211\n",
      "2561: loss=0.643, reward_mean=0.190, reward_bound=0.167, batch=217\n",
      "2562: loss=0.640, reward_mean=0.120, reward_bound=0.122, batch=221\n",
      "2563: loss=0.635, reward_mean=0.160, reward_bound=0.229, batch=221\n",
      "2564: loss=0.641, reward_mean=0.190, reward_bound=0.282, batch=216\n",
      "2565: loss=0.641, reward_mean=0.150, reward_bound=0.314, batch=184\n",
      "2566: loss=0.628, reward_mean=0.140, reward_bound=0.000, batch=198\n",
      "2567: loss=0.628, reward_mean=0.150, reward_bound=0.043, batch=208\n",
      "2568: loss=0.623, reward_mean=0.170, reward_bound=0.111, batch=215\n",
      "2569: loss=0.626, reward_mean=0.110, reward_bound=0.150, batch=217\n",
      "2570: loss=0.631, reward_mean=0.170, reward_bound=0.185, batch=217\n",
      "2571: loss=0.633, reward_mean=0.160, reward_bound=0.198, batch=222\n",
      "2572: loss=0.633, reward_mean=0.140, reward_bound=0.229, batch=220\n",
      "2573: loss=0.631, reward_mean=0.250, reward_bound=0.282, batch=216\n",
      "2574: loss=0.631, reward_mean=0.160, reward_bound=0.268, batch=221\n",
      "2575: loss=0.626, reward_mean=0.140, reward_bound=0.314, batch=220\n",
      "2576: loss=0.629, reward_mean=0.110, reward_bound=0.266, batch=224\n",
      "2577: loss=0.630, reward_mean=0.190, reward_bound=0.349, batch=173\n",
      "2578: loss=0.619, reward_mean=0.190, reward_bound=0.005, batch=191\n",
      "2579: loss=0.624, reward_mean=0.170, reward_bound=0.038, batch=203\n",
      "2580: loss=0.628, reward_mean=0.210, reward_bound=0.117, batch=212\n",
      "2581: loss=0.625, reward_mean=0.140, reward_bound=0.122, batch=216\n",
      "2582: loss=0.624, reward_mean=0.210, reward_bound=0.150, batch=220\n",
      "2583: loss=0.626, reward_mean=0.200, reward_bound=0.200, batch=224\n",
      "2584: loss=0.625, reward_mean=0.160, reward_bound=0.206, batch=226\n",
      "2585: loss=0.624, reward_mean=0.140, reward_bound=0.229, batch=226\n",
      "2586: loss=0.625, reward_mean=0.230, reward_bound=0.254, batch=222\n",
      "2587: loss=0.625, reward_mean=0.140, reward_bound=0.282, batch=218\n",
      "2588: loss=0.625, reward_mean=0.180, reward_bound=0.254, batch=221\n",
      "2589: loss=0.632, reward_mean=0.180, reward_bound=0.314, batch=213\n",
      "2590: loss=0.629, reward_mean=0.210, reward_bound=0.185, batch=218\n",
      "2591: loss=0.633, reward_mean=0.150, reward_bound=0.282, batch=221\n",
      "2592: loss=0.632, reward_mean=0.150, reward_bound=0.314, batch=223\n",
      "2593: loss=0.630, reward_mean=0.200, reward_bound=0.349, batch=219\n",
      "2594: loss=0.629, reward_mean=0.130, reward_bound=0.295, batch=223\n",
      "2595: loss=0.630, reward_mean=0.130, reward_bound=0.261, batch=226\n",
      "2596: loss=0.629, reward_mean=0.180, reward_bound=0.368, batch=228\n",
      "2597: loss=0.633, reward_mean=0.200, reward_bound=0.387, batch=162\n",
      "2598: loss=0.616, reward_mean=0.120, reward_bound=0.000, batch=174\n",
      "2599: loss=0.624, reward_mean=0.170, reward_bound=0.000, batch=191\n",
      "2600: loss=0.622, reward_mean=0.170, reward_bound=0.038, batch=202\n",
      "2601: loss=0.625, reward_mean=0.190, reward_bound=0.109, batch=209\n",
      "2602: loss=0.624, reward_mean=0.150, reward_bound=0.150, batch=215\n",
      "2603: loss=0.620, reward_mean=0.160, reward_bound=0.170, batch=220\n",
      "2604: loss=0.620, reward_mean=0.170, reward_bound=0.200, batch=224\n",
      "2605: loss=0.623, reward_mean=0.160, reward_bound=0.229, batch=220\n",
      "2606: loss=0.627, reward_mean=0.200, reward_bound=0.254, batch=216\n",
      "2607: loss=0.621, reward_mean=0.180, reward_bound=0.282, batch=217\n",
      "2608: loss=0.622, reward_mean=0.230, reward_bound=0.249, batch=222\n",
      "2609: loss=0.620, reward_mean=0.160, reward_bound=0.263, batch=225\n",
      "2610: loss=0.620, reward_mean=0.150, reward_bound=0.260, batch=227\n",
      "2611: loss=0.621, reward_mean=0.180, reward_bound=0.282, batch=228\n",
      "2612: loss=0.625, reward_mean=0.210, reward_bound=0.314, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2613: loss=0.628, reward_mean=0.200, reward_bound=0.254, batch=224\n",
      "2614: loss=0.627, reward_mean=0.190, reward_bound=0.311, batch=227\n",
      "2615: loss=0.629, reward_mean=0.240, reward_bound=0.349, batch=220\n",
      "2616: loss=0.633, reward_mean=0.220, reward_bound=0.349, batch=223\n",
      "2617: loss=0.636, reward_mean=0.180, reward_bound=0.387, batch=206\n",
      "2618: loss=0.626, reward_mean=0.190, reward_bound=0.241, batch=214\n",
      "2619: loss=0.621, reward_mean=0.140, reward_bound=0.150, batch=219\n",
      "2620: loss=0.617, reward_mean=0.140, reward_bound=0.229, batch=222\n",
      "2621: loss=0.620, reward_mean=0.140, reward_bound=0.292, batch=225\n",
      "2622: loss=0.621, reward_mean=0.120, reward_bound=0.314, batch=225\n",
      "2623: loss=0.622, reward_mean=0.170, reward_bound=0.356, batch=227\n",
      "2624: loss=0.621, reward_mean=0.140, reward_bound=0.380, batch=229\n",
      "2625: loss=0.631, reward_mean=0.100, reward_bound=0.387, batch=224\n",
      "2626: loss=0.630, reward_mean=0.130, reward_bound=0.384, batch=227\n",
      "2627: loss=0.649, reward_mean=0.200, reward_bound=0.430, batch=119\n",
      "2628: loss=0.634, reward_mean=0.130, reward_bound=0.000, batch=132\n",
      "2629: loss=0.629, reward_mean=0.160, reward_bound=0.000, batch=148\n",
      "2630: loss=0.631, reward_mean=0.170, reward_bound=0.000, batch=165\n",
      "2631: loss=0.611, reward_mean=0.210, reward_bound=0.004, batch=185\n",
      "2632: loss=0.609, reward_mean=0.150, reward_bound=0.007, batch=199\n",
      "2633: loss=0.607, reward_mean=0.150, reward_bound=0.038, batch=209\n",
      "2634: loss=0.621, reward_mean=0.180, reward_bound=0.055, batch=216\n",
      "2635: loss=0.621, reward_mean=0.170, reward_bound=0.080, batch=220\n",
      "2636: loss=0.616, reward_mean=0.180, reward_bound=0.122, batch=221\n",
      "2637: loss=0.618, reward_mean=0.210, reward_bound=0.167, batch=216\n",
      "2638: loss=0.621, reward_mean=0.200, reward_bound=0.185, batch=218\n",
      "2639: loss=0.623, reward_mean=0.220, reward_bound=0.206, batch=216\n",
      "2640: loss=0.620, reward_mean=0.180, reward_bound=0.229, batch=206\n",
      "2641: loss=0.615, reward_mean=0.170, reward_bound=0.217, batch=214\n",
      "2642: loss=0.616, reward_mean=0.160, reward_bound=0.254, batch=207\n",
      "2643: loss=0.608, reward_mean=0.200, reward_bound=0.167, batch=213\n",
      "2644: loss=0.613, reward_mean=0.240, reward_bound=0.271, batch=219\n",
      "2645: loss=0.618, reward_mean=0.240, reward_bound=0.282, batch=212\n",
      "2646: loss=0.623, reward_mean=0.190, reward_bound=0.314, batch=201\n",
      "2647: loss=0.623, reward_mean=0.240, reward_bound=0.167, batch=210\n",
      "2648: loss=0.618, reward_mean=0.180, reward_bound=0.206, batch=220\n",
      "2649: loss=0.625, reward_mean=0.210, reward_bound=0.254, batch=223\n",
      "2650: loss=0.630, reward_mean=0.190, reward_bound=0.271, batch=226\n",
      "2651: loss=0.624, reward_mean=0.220, reward_bound=0.314, batch=225\n",
      "2652: loss=0.625, reward_mean=0.160, reward_bound=0.349, batch=196\n",
      "2653: loss=0.620, reward_mean=0.220, reward_bound=0.176, batch=207\n",
      "2654: loss=0.615, reward_mean=0.170, reward_bound=0.182, batch=215\n",
      "2655: loss=0.612, reward_mean=0.190, reward_bound=0.189, batch=220\n",
      "2656: loss=0.619, reward_mean=0.170, reward_bound=0.229, batch=221\n",
      "2657: loss=0.616, reward_mean=0.140, reward_bound=0.254, batch=222\n",
      "2658: loss=0.620, reward_mean=0.230, reward_bound=0.282, batch=223\n",
      "2659: loss=0.620, reward_mean=0.080, reward_bound=0.178, batch=226\n",
      "2660: loss=0.628, reward_mean=0.200, reward_bound=0.314, batch=225\n",
      "2661: loss=0.623, reward_mean=0.230, reward_bound=0.349, batch=222\n",
      "2662: loss=0.623, reward_mean=0.190, reward_bound=0.314, batch=224\n",
      "2663: loss=0.624, reward_mean=0.150, reward_bound=0.384, batch=227\n",
      "2664: loss=0.645, reward_mean=0.190, reward_bound=0.387, batch=188\n",
      "2665: loss=0.627, reward_mean=0.160, reward_bound=0.035, batch=201\n",
      "2666: loss=0.623, reward_mean=0.140, reward_bound=0.098, batch=208\n",
      "2667: loss=0.632, reward_mean=0.170, reward_bound=0.169, batch=215\n",
      "2668: loss=0.630, reward_mean=0.120, reward_bound=0.170, batch=220\n",
      "2669: loss=0.627, reward_mean=0.180, reward_bound=0.206, batch=228\n",
      "2670: loss=0.634, reward_mean=0.180, reward_bound=0.206, batch=228\n",
      "2671: loss=0.633, reward_mean=0.190, reward_bound=0.254, batch=226\n",
      "2672: loss=0.635, reward_mean=0.160, reward_bound=0.282, batch=223\n",
      "2673: loss=0.643, reward_mean=0.180, reward_bound=0.314, batch=222\n",
      "2674: loss=0.642, reward_mean=0.190, reward_bound=0.283, batch=225\n",
      "2675: loss=0.630, reward_mean=0.170, reward_bound=0.349, batch=221\n",
      "2676: loss=0.634, reward_mean=0.140, reward_bound=0.387, batch=212\n",
      "2677: loss=0.628, reward_mean=0.180, reward_bound=0.254, batch=217\n",
      "2678: loss=0.630, reward_mean=0.120, reward_bound=0.155, batch=222\n",
      "2679: loss=0.635, reward_mean=0.230, reward_bound=0.314, batch=222\n",
      "2680: loss=0.638, reward_mean=0.200, reward_bound=0.314, batch=224\n",
      "2681: loss=0.635, reward_mean=0.080, reward_bound=0.280, batch=227\n",
      "2682: loss=0.634, reward_mean=0.130, reward_bound=0.342, batch=229\n",
      "2683: loss=0.627, reward_mean=0.130, reward_bound=0.349, batch=228\n",
      "2684: loss=0.626, reward_mean=0.150, reward_bound=0.353, batch=229\n",
      "2685: loss=0.631, reward_mean=0.170, reward_bound=0.387, batch=222\n",
      "2686: loss=0.630, reward_mean=0.130, reward_bound=0.236, batch=225\n",
      "2687: loss=0.630, reward_mean=0.150, reward_bound=0.282, batch=226\n",
      "2688: loss=0.630, reward_mean=0.120, reward_bound=0.349, batch=226\n",
      "2689: loss=0.631, reward_mean=0.240, reward_bound=0.331, batch=228\n",
      "2690: loss=0.629, reward_mean=0.190, reward_bound=0.353, batch=229\n",
      "2691: loss=0.629, reward_mean=0.230, reward_bound=0.387, batch=226\n",
      "2692: loss=0.639, reward_mean=0.240, reward_bound=0.430, batch=179\n",
      "2693: loss=0.635, reward_mean=0.170, reward_bound=0.041, batch=195\n",
      "2694: loss=0.628, reward_mean=0.120, reward_bound=0.038, batch=206\n",
      "2695: loss=0.632, reward_mean=0.220, reward_bound=0.098, batch=212\n",
      "2696: loss=0.633, reward_mean=0.110, reward_bound=0.140, batch=218\n",
      "2697: loss=0.633, reward_mean=0.110, reward_bound=0.137, batch=222\n",
      "2698: loss=0.627, reward_mean=0.190, reward_bound=0.172, batch=225\n",
      "2699: loss=0.628, reward_mean=0.140, reward_bound=0.189, batch=227\n",
      "2700: loss=0.630, reward_mean=0.180, reward_bound=0.229, batch=225\n",
      "2701: loss=0.635, reward_mean=0.260, reward_bound=0.282, batch=216\n",
      "2702: loss=0.629, reward_mean=0.160, reward_bound=0.185, batch=220\n",
      "2703: loss=0.628, reward_mean=0.200, reward_bound=0.274, batch=224\n",
      "2704: loss=0.627, reward_mean=0.110, reward_bound=0.282, batch=225\n",
      "2705: loss=0.626, reward_mean=0.130, reward_bound=0.314, batch=212\n",
      "2706: loss=0.627, reward_mean=0.200, reward_bound=0.324, batch=218\n",
      "2707: loss=0.631, reward_mean=0.180, reward_bound=0.349, batch=210\n",
      "2708: loss=0.633, reward_mean=0.150, reward_bound=0.229, batch=216\n",
      "2709: loss=0.639, reward_mean=0.180, reward_bound=0.254, batch=219\n",
      "2710: loss=0.642, reward_mean=0.240, reward_bound=0.282, batch=220\n",
      "2711: loss=0.641, reward_mean=0.220, reward_bound=0.338, batch=224\n",
      "2712: loss=0.637, reward_mean=0.220, reward_bound=0.349, batch=224\n",
      "2713: loss=0.634, reward_mean=0.220, reward_bound=0.387, batch=205\n",
      "2714: loss=0.634, reward_mean=0.130, reward_bound=0.117, batch=213\n",
      "2715: loss=0.637, reward_mean=0.170, reward_bound=0.167, batch=218\n",
      "2716: loss=0.631, reward_mean=0.170, reward_bound=0.229, batch=221\n",
      "2717: loss=0.633, reward_mean=0.230, reward_bound=0.282, batch=220\n",
      "2718: loss=0.636, reward_mean=0.180, reward_bound=0.314, batch=223\n",
      "2719: loss=0.638, reward_mean=0.230, reward_bound=0.349, batch=222\n",
      "2720: loss=0.636, reward_mean=0.080, reward_bound=0.272, batch=225\n",
      "2721: loss=0.637, reward_mean=0.190, reward_bound=0.314, batch=226\n",
      "2722: loss=0.637, reward_mean=0.140, reward_bound=0.331, batch=228\n",
      "2723: loss=0.637, reward_mean=0.170, reward_bound=0.387, batch=222\n",
      "2724: loss=0.634, reward_mean=0.100, reward_bound=0.174, batch=225\n",
      "2725: loss=0.634, reward_mean=0.230, reward_bound=0.240, batch=227\n",
      "2726: loss=0.639, reward_mean=0.180, reward_bound=0.282, batch=228\n",
      "2727: loss=0.641, reward_mean=0.110, reward_bound=0.257, batch=229\n",
      "2728: loss=0.638, reward_mean=0.200, reward_bound=0.328, batch=230\n",
      "2729: loss=0.639, reward_mean=0.160, reward_bound=0.349, batch=230\n",
      "2730: loss=0.637, reward_mean=0.170, reward_bound=0.418, batch=231\n",
      "2731: loss=0.639, reward_mean=0.070, reward_bound=0.430, batch=205\n",
      "2732: loss=0.634, reward_mean=0.120, reward_bound=0.080, batch=212\n",
      "2733: loss=0.630, reward_mean=0.110, reward_bound=0.089, batch=217\n",
      "2734: loss=0.628, reward_mean=0.170, reward_bound=0.173, batch=222\n",
      "2735: loss=0.629, reward_mean=0.160, reward_bound=0.185, batch=224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2736: loss=0.628, reward_mean=0.150, reward_bound=0.206, batch=226\n",
      "2737: loss=0.632, reward_mean=0.130, reward_bound=0.254, batch=227\n",
      "2738: loss=0.631, reward_mean=0.190, reward_bound=0.282, batch=227\n",
      "2739: loss=0.631, reward_mean=0.140, reward_bound=0.314, batch=228\n",
      "2740: loss=0.635, reward_mean=0.190, reward_bound=0.387, batch=221\n",
      "2741: loss=0.633, reward_mean=0.130, reward_bound=0.349, batch=224\n",
      "2742: loss=0.630, reward_mean=0.140, reward_bound=0.384, batch=227\n",
      "2743: loss=0.628, reward_mean=0.220, reward_bound=0.380, batch=229\n",
      "2744: loss=0.632, reward_mean=0.200, reward_bound=0.405, batch=230\n",
      "2745: loss=0.631, reward_mean=0.140, reward_bound=0.418, batch=231\n",
      "2746: loss=0.638, reward_mean=0.160, reward_bound=0.430, batch=219\n",
      "2747: loss=0.642, reward_mean=0.190, reward_bound=0.282, batch=221\n",
      "2748: loss=0.645, reward_mean=0.140, reward_bound=0.229, batch=224\n",
      "2749: loss=0.638, reward_mean=0.210, reward_bound=0.282, batch=226\n",
      "2750: loss=0.633, reward_mean=0.270, reward_bound=0.349, batch=225\n",
      "2751: loss=0.632, reward_mean=0.210, reward_bound=0.387, batch=225\n",
      "2752: loss=0.636, reward_mean=0.140, reward_bound=0.430, batch=222\n",
      "2753: loss=0.638, reward_mean=0.170, reward_bound=0.292, batch=225\n",
      "2754: loss=0.633, reward_mean=0.190, reward_bound=0.356, batch=227\n",
      "2755: loss=0.636, reward_mean=0.190, reward_bound=0.387, batch=228\n",
      "2756: loss=0.635, reward_mean=0.120, reward_bound=0.353, batch=229\n",
      "2757: loss=0.637, reward_mean=0.120, reward_bound=0.387, batch=229\n",
      "2758: loss=0.637, reward_mean=0.180, reward_bound=0.405, batch=230\n",
      "2759: loss=0.638, reward_mean=0.160, reward_bound=0.406, batch=231\n",
      "2760: loss=0.639, reward_mean=0.210, reward_bound=0.430, batch=227\n",
      "2761: loss=0.639, reward_mean=0.220, reward_bound=0.430, batch=228\n",
      "2762: loss=0.637, reward_mean=0.180, reward_bound=0.357, batch=229\n",
      "2763: loss=0.640, reward_mean=0.160, reward_bound=0.478, batch=231\n",
      "2764: loss=0.640, reward_mean=0.160, reward_bound=0.430, batch=231\n",
      "2765: loss=0.626, reward_mean=0.180, reward_bound=0.478, batch=106\n",
      "2766: loss=0.604, reward_mean=0.190, reward_bound=0.000, batch=125\n",
      "2767: loss=0.596, reward_mean=0.200, reward_bound=0.000, batch=145\n",
      "2768: loss=0.594, reward_mean=0.120, reward_bound=0.000, batch=157\n",
      "2769: loss=0.584, reward_mean=0.190, reward_bound=0.000, batch=176\n",
      "2770: loss=0.588, reward_mean=0.170, reward_bound=0.004, batch=193\n",
      "2771: loss=0.594, reward_mean=0.220, reward_bound=0.031, batch=203\n",
      "2772: loss=0.595, reward_mean=0.210, reward_bound=0.052, batch=211\n",
      "2773: loss=0.599, reward_mean=0.200, reward_bound=0.072, batch=217\n",
      "2774: loss=0.595, reward_mean=0.160, reward_bound=0.098, batch=220\n",
      "2775: loss=0.599, reward_mean=0.260, reward_bound=0.150, batch=216\n",
      "2776: loss=0.593, reward_mean=0.210, reward_bound=0.167, batch=220\n",
      "2777: loss=0.597, reward_mean=0.170, reward_bound=0.185, batch=214\n",
      "2778: loss=0.600, reward_mean=0.200, reward_bound=0.206, batch=213\n",
      "2779: loss=0.598, reward_mean=0.220, reward_bound=0.229, batch=211\n",
      "2780: loss=0.588, reward_mean=0.220, reward_bound=0.254, batch=201\n",
      "2781: loss=0.582, reward_mean=0.190, reward_bound=0.150, batch=210\n",
      "2782: loss=0.580, reward_mean=0.170, reward_bound=0.206, batch=219\n",
      "2783: loss=0.583, reward_mean=0.200, reward_bound=0.254, batch=220\n",
      "2784: loss=0.592, reward_mean=0.220, reward_bound=0.282, batch=207\n",
      "2785: loss=0.589, reward_mean=0.170, reward_bound=0.147, batch=215\n",
      "2786: loss=0.590, reward_mean=0.170, reward_bound=0.229, batch=219\n",
      "2787: loss=0.588, reward_mean=0.170, reward_bound=0.254, batch=222\n",
      "2788: loss=0.594, reward_mean=0.150, reward_bound=0.282, batch=222\n",
      "2789: loss=0.591, reward_mean=0.180, reward_bound=0.292, batch=225\n",
      "2790: loss=0.595, reward_mean=0.170, reward_bound=0.314, batch=206\n",
      "2791: loss=0.595, reward_mean=0.180, reward_bound=0.230, batch=214\n",
      "2792: loss=0.590, reward_mean=0.250, reward_bound=0.314, batch=217\n",
      "2793: loss=0.589, reward_mean=0.190, reward_bound=0.182, batch=222\n",
      "2794: loss=0.589, reward_mean=0.240, reward_bound=0.324, batch=225\n",
      "2795: loss=0.606, reward_mean=0.220, reward_bound=0.349, batch=196\n",
      "2796: loss=0.607, reward_mean=0.230, reward_bound=0.206, batch=205\n",
      "2797: loss=0.609, reward_mean=0.220, reward_bound=0.210, batch=213\n",
      "2798: loss=0.608, reward_mean=0.150, reward_bound=0.229, batch=215\n",
      "2799: loss=0.600, reward_mean=0.190, reward_bound=0.254, batch=218\n",
      "2800: loss=0.602, reward_mean=0.110, reward_bound=0.257, batch=222\n",
      "2801: loss=0.608, reward_mean=0.240, reward_bound=0.314, batch=224\n",
      "2802: loss=0.614, reward_mean=0.220, reward_bound=0.349, batch=219\n",
      "2803: loss=0.609, reward_mean=0.200, reward_bound=0.278, batch=223\n",
      "2804: loss=0.610, reward_mean=0.150, reward_bound=0.349, batch=225\n",
      "2805: loss=0.609, reward_mean=0.180, reward_bound=0.266, batch=227\n",
      "2806: loss=0.610, reward_mean=0.150, reward_bound=0.349, batch=228\n",
      "2807: loss=0.604, reward_mean=0.190, reward_bound=0.387, batch=199\n",
      "2808: loss=0.605, reward_mean=0.220, reward_bound=0.295, batch=209\n",
      "2809: loss=0.609, reward_mean=0.190, reward_bound=0.254, batch=215\n",
      "2810: loss=0.606, reward_mean=0.160, reward_bound=0.234, batch=220\n",
      "2811: loss=0.609, reward_mean=0.180, reward_bound=0.282, batch=223\n",
      "2812: loss=0.612, reward_mean=0.150, reward_bound=0.314, batch=220\n",
      "2813: loss=0.609, reward_mean=0.230, reward_bound=0.329, batch=224\n",
      "2814: loss=0.610, reward_mean=0.160, reward_bound=0.280, batch=227\n",
      "2815: loss=0.605, reward_mean=0.220, reward_bound=0.349, batch=220\n",
      "2816: loss=0.601, reward_mean=0.150, reward_bound=0.229, batch=223\n",
      "2817: loss=0.604, reward_mean=0.210, reward_bound=0.349, batch=225\n",
      "2818: loss=0.606, reward_mean=0.180, reward_bound=0.387, batch=217\n",
      "2819: loss=0.604, reward_mean=0.160, reward_bound=0.349, batch=219\n",
      "2820: loss=0.601, reward_mean=0.180, reward_bound=0.265, batch=223\n",
      "2821: loss=0.599, reward_mean=0.210, reward_bound=0.387, batch=225\n",
      "2822: loss=0.614, reward_mean=0.160, reward_bound=0.430, batch=164\n",
      "2823: loss=0.610, reward_mean=0.270, reward_bound=0.088, batch=185\n",
      "2824: loss=0.596, reward_mean=0.200, reward_bound=0.109, batch=201\n",
      "2825: loss=0.600, reward_mean=0.180, reward_bound=0.135, batch=207\n",
      "2826: loss=0.603, reward_mean=0.180, reward_bound=0.167, batch=213\n",
      "2827: loss=0.606, reward_mean=0.110, reward_bound=0.185, batch=216\n",
      "2828: loss=0.599, reward_mean=0.300, reward_bound=0.217, batch=221\n",
      "2829: loss=0.598, reward_mean=0.200, reward_bound=0.254, batch=219\n",
      "2830: loss=0.597, reward_mean=0.190, reward_bound=0.194, batch=223\n",
      "2831: loss=0.605, reward_mean=0.160, reward_bound=0.282, batch=219\n",
      "2832: loss=0.604, reward_mean=0.120, reward_bound=0.295, batch=223\n",
      "2833: loss=0.607, reward_mean=0.190, reward_bound=0.301, batch=226\n",
      "2834: loss=0.607, reward_mean=0.110, reward_bound=0.314, batch=220\n",
      "2835: loss=0.605, reward_mean=0.160, reward_bound=0.304, batch=224\n",
      "2836: loss=0.606, reward_mean=0.150, reward_bound=0.345, batch=227\n",
      "2837: loss=0.604, reward_mean=0.120, reward_bound=0.330, batch=229\n",
      "2838: loss=0.626, reward_mean=0.230, reward_bound=0.349, batch=208\n",
      "2839: loss=0.627, reward_mean=0.140, reward_bound=0.135, batch=214\n",
      "2840: loss=0.627, reward_mean=0.180, reward_bound=0.249, batch=220\n",
      "2841: loss=0.622, reward_mean=0.210, reward_bound=0.274, batch=224\n",
      "2842: loss=0.617, reward_mean=0.150, reward_bound=0.280, batch=227\n",
      "2843: loss=0.619, reward_mean=0.180, reward_bound=0.314, batch=228\n",
      "2844: loss=0.620, reward_mean=0.120, reward_bound=0.349, batch=226\n",
      "2845: loss=0.620, reward_mean=0.140, reward_bound=0.335, batch=228\n",
      "2846: loss=0.621, reward_mean=0.190, reward_bound=0.387, batch=212\n",
      "2847: loss=0.621, reward_mean=0.150, reward_bound=0.282, batch=217\n",
      "2848: loss=0.620, reward_mean=0.200, reward_bound=0.206, batch=221\n",
      "2849: loss=0.617, reward_mean=0.230, reward_bound=0.314, batch=221\n",
      "2850: loss=0.615, reward_mean=0.150, reward_bound=0.349, batch=221\n",
      "2851: loss=0.612, reward_mean=0.190, reward_bound=0.349, batch=223\n",
      "2852: loss=0.610, reward_mean=0.180, reward_bound=0.290, batch=226\n",
      "2853: loss=0.610, reward_mean=0.180, reward_bound=0.314, batch=227\n",
      "2854: loss=0.608, reward_mean=0.110, reward_bound=0.224, batch=229\n",
      "2855: loss=0.615, reward_mean=0.200, reward_bound=0.387, batch=223\n",
      "2856: loss=0.615, reward_mean=0.210, reward_bound=0.254, batch=225\n",
      "2857: loss=0.615, reward_mean=0.180, reward_bound=0.260, batch=227\n",
      "2858: loss=0.616, reward_mean=0.190, reward_bound=0.314, batch=228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2859: loss=0.613, reward_mean=0.210, reward_bound=0.387, batch=228\n",
      "2860: loss=0.620, reward_mean=0.220, reward_bound=0.430, batch=198\n",
      "2861: loss=0.618, reward_mean=0.150, reward_bound=0.098, batch=207\n",
      "2862: loss=0.615, reward_mean=0.130, reward_bound=0.097, batch=215\n",
      "2863: loss=0.615, reward_mean=0.280, reward_bound=0.206, batch=218\n",
      "2864: loss=0.625, reward_mean=0.110, reward_bound=0.229, batch=219\n",
      "2865: loss=0.629, reward_mean=0.130, reward_bound=0.254, batch=222\n",
      "2866: loss=0.628, reward_mean=0.150, reward_bound=0.314, batch=223\n",
      "2867: loss=0.627, reward_mean=0.230, reward_bound=0.349, batch=220\n",
      "2868: loss=0.622, reward_mean=0.140, reward_bound=0.387, batch=212\n",
      "2869: loss=0.623, reward_mean=0.210, reward_bound=0.292, batch=218\n",
      "2870: loss=0.620, reward_mean=0.190, reward_bound=0.286, batch=222\n",
      "2871: loss=0.618, reward_mean=0.220, reward_bound=0.314, batch=222\n",
      "2872: loss=0.621, reward_mean=0.200, reward_bound=0.349, batch=222\n",
      "2873: loss=0.620, reward_mean=0.210, reward_bound=0.360, batch=225\n",
      "2874: loss=0.620, reward_mean=0.190, reward_bound=0.387, batch=226\n",
      "2875: loss=0.618, reward_mean=0.110, reward_bound=0.390, batch=228\n",
      "2876: loss=0.620, reward_mean=0.210, reward_bound=0.430, batch=215\n",
      "2877: loss=0.620, reward_mean=0.320, reward_bound=0.430, batch=218\n",
      "2878: loss=0.617, reward_mean=0.170, reward_bound=0.392, batch=222\n",
      "2879: loss=0.618, reward_mean=0.190, reward_bound=0.324, batch=225\n",
      "2880: loss=0.618, reward_mean=0.160, reward_bound=0.254, batch=226\n",
      "2881: loss=0.619, reward_mean=0.190, reward_bound=0.301, batch=228\n",
      "2882: loss=0.617, reward_mean=0.150, reward_bound=0.349, batch=227\n",
      "2883: loss=0.619, reward_mean=0.180, reward_bound=0.387, batch=228\n",
      "2884: loss=0.619, reward_mean=0.120, reward_bound=0.392, batch=229\n",
      "2885: loss=0.620, reward_mean=0.150, reward_bound=0.430, batch=225\n",
      "2886: loss=0.620, reward_mean=0.160, reward_bound=0.440, batch=227\n",
      "2887: loss=0.621, reward_mean=0.100, reward_bound=0.387, batch=228\n",
      "2888: loss=0.622, reward_mean=0.160, reward_bound=0.430, batch=228\n",
      "2889: loss=0.623, reward_mean=0.180, reward_bound=0.392, batch=229\n",
      "2890: loss=0.622, reward_mean=0.220, reward_bound=0.405, batch=230\n",
      "2891: loss=0.622, reward_mean=0.140, reward_bound=0.378, batch=231\n",
      "2892: loss=0.619, reward_mean=0.200, reward_bound=0.478, batch=161\n",
      "2893: loss=0.600, reward_mean=0.200, reward_bound=0.000, batch=181\n",
      "2894: loss=0.599, reward_mean=0.200, reward_bound=0.042, batch=196\n",
      "2895: loss=0.590, reward_mean=0.110, reward_bound=0.032, batch=207\n",
      "2896: loss=0.588, reward_mean=0.140, reward_bound=0.089, batch=218\n",
      "2897: loss=0.596, reward_mean=0.220, reward_bound=0.100, batch=222\n",
      "2898: loss=0.594, reward_mean=0.200, reward_bound=0.167, batch=222\n",
      "2899: loss=0.596, reward_mean=0.230, reward_bound=0.185, batch=221\n",
      "2900: loss=0.599, reward_mean=0.240, reward_bound=0.206, batch=223\n",
      "2901: loss=0.603, reward_mean=0.250, reward_bound=0.244, batch=226\n",
      "2902: loss=0.603, reward_mean=0.150, reward_bound=0.254, batch=226\n",
      "2903: loss=0.604, reward_mean=0.180, reward_bound=0.282, batch=219\n",
      "2904: loss=0.605, reward_mean=0.150, reward_bound=0.282, batch=222\n",
      "2905: loss=0.612, reward_mean=0.250, reward_bound=0.314, batch=212\n",
      "2906: loss=0.612, reward_mean=0.180, reward_bound=0.283, batch=218\n",
      "2907: loss=0.620, reward_mean=0.130, reward_bound=0.349, batch=207\n",
      "2908: loss=0.617, reward_mean=0.190, reward_bound=0.267, batch=215\n",
      "2909: loss=0.618, reward_mean=0.160, reward_bound=0.282, batch=218\n",
      "2910: loss=0.621, reward_mean=0.220, reward_bound=0.317, batch=222\n",
      "2911: loss=0.621, reward_mean=0.110, reward_bound=0.314, batch=224\n",
      "2912: loss=0.625, reward_mean=0.220, reward_bound=0.282, batch=226\n",
      "2913: loss=0.624, reward_mean=0.160, reward_bound=0.298, batch=228\n",
      "2914: loss=0.624, reward_mean=0.190, reward_bound=0.349, batch=222\n",
      "2915: loss=0.620, reward_mean=0.180, reward_bound=0.292, batch=225\n",
      "2916: loss=0.619, reward_mean=0.220, reward_bound=0.289, batch=227\n",
      "2917: loss=0.618, reward_mean=0.150, reward_bound=0.302, batch=229\n",
      "2918: loss=0.620, reward_mean=0.190, reward_bound=0.314, batch=229\n",
      "2919: loss=0.621, reward_mean=0.190, reward_bound=0.387, batch=212\n",
      "2920: loss=0.619, reward_mean=0.190, reward_bound=0.282, batch=217\n",
      "2921: loss=0.626, reward_mean=0.160, reward_bound=0.254, batch=220\n",
      "2922: loss=0.624, reward_mean=0.180, reward_bound=0.314, batch=222\n",
      "2923: loss=0.625, reward_mean=0.190, reward_bound=0.236, batch=225\n",
      "2924: loss=0.620, reward_mean=0.210, reward_bound=0.282, batch=226\n",
      "2925: loss=0.625, reward_mean=0.220, reward_bound=0.349, batch=227\n",
      "2926: loss=0.625, reward_mean=0.130, reward_bound=0.366, batch=229\n",
      "2927: loss=0.618, reward_mean=0.150, reward_bound=0.387, batch=224\n",
      "2928: loss=0.616, reward_mean=0.220, reward_bound=0.426, batch=227\n",
      "2929: loss=0.615, reward_mean=0.100, reward_bound=0.407, batch=229\n",
      "2930: loss=0.615, reward_mean=0.180, reward_bound=0.360, batch=230\n",
      "2931: loss=0.615, reward_mean=0.160, reward_bound=0.395, batch=231\n",
      "2932: loss=0.615, reward_mean=0.120, reward_bound=0.387, batch=231\n",
      "2933: loss=0.623, reward_mean=0.150, reward_bound=0.430, batch=203\n",
      "2934: loss=0.626, reward_mean=0.160, reward_bound=0.154, batch=212\n",
      "2935: loss=0.623, reward_mean=0.270, reward_bound=0.213, batch=218\n",
      "2936: loss=0.623, reward_mean=0.140, reward_bound=0.254, batch=220\n",
      "2937: loss=0.622, reward_mean=0.180, reward_bound=0.274, batch=224\n",
      "2938: loss=0.624, reward_mean=0.150, reward_bound=0.245, batch=227\n",
      "2939: loss=0.628, reward_mean=0.130, reward_bound=0.282, batch=225\n",
      "2940: loss=0.633, reward_mean=0.190, reward_bound=0.314, batch=222\n",
      "2941: loss=0.632, reward_mean=0.230, reward_bound=0.349, batch=224\n",
      "2942: loss=0.633, reward_mean=0.190, reward_bound=0.311, batch=227\n",
      "2943: loss=0.632, reward_mean=0.100, reward_bound=0.380, batch=229\n",
      "2944: loss=0.632, reward_mean=0.220, reward_bound=0.349, batch=229\n",
      "2945: loss=0.618, reward_mean=0.260, reward_bound=0.387, batch=219\n",
      "2946: loss=0.619, reward_mean=0.160, reward_bound=0.265, batch=223\n",
      "2947: loss=0.620, reward_mean=0.190, reward_bound=0.244, batch=226\n",
      "2948: loss=0.616, reward_mean=0.190, reward_bound=0.351, batch=228\n",
      "2949: loss=0.621, reward_mean=0.250, reward_bound=0.430, batch=214\n",
      "2950: loss=0.621, reward_mean=0.190, reward_bound=0.342, batch=220\n",
      "2951: loss=0.625, reward_mean=0.240, reward_bound=0.349, batch=222\n",
      "2952: loss=0.623, reward_mean=0.160, reward_bound=0.254, batch=225\n",
      "2953: loss=0.618, reward_mean=0.170, reward_bound=0.387, batch=224\n",
      "2954: loss=0.618, reward_mean=0.130, reward_bound=0.280, batch=227\n",
      "2955: loss=0.614, reward_mean=0.170, reward_bound=0.349, batch=228\n",
      "2956: loss=0.616, reward_mean=0.200, reward_bound=0.392, batch=229\n",
      "2957: loss=0.620, reward_mean=0.190, reward_bound=0.430, batch=224\n",
      "2958: loss=0.620, reward_mean=0.200, reward_bound=0.384, batch=227\n",
      "2959: loss=0.619, reward_mean=0.260, reward_bound=0.387, batch=228\n",
      "2960: loss=0.620, reward_mean=0.260, reward_bound=0.430, batch=228\n",
      "2961: loss=0.621, reward_mean=0.180, reward_bound=0.357, batch=229\n",
      "2962: loss=0.622, reward_mean=0.150, reward_bound=0.349, batch=229\n",
      "2963: loss=0.623, reward_mean=0.250, reward_bound=0.364, batch=230\n",
      "2964: loss=0.621, reward_mean=0.230, reward_bound=0.387, batch=230\n",
      "2965: loss=0.621, reward_mean=0.160, reward_bound=0.314, batch=230\n",
      "2966: loss=0.619, reward_mean=0.230, reward_bound=0.430, batch=229\n",
      "2967: loss=0.620, reward_mean=0.200, reward_bound=0.450, batch=230\n",
      "2968: loss=0.619, reward_mean=0.190, reward_bound=0.464, batch=231\n",
      "2969: loss=0.619, reward_mean=0.100, reward_bound=0.314, batch=231\n",
      "2970: loss=0.623, reward_mean=0.200, reward_bound=0.478, batch=187\n",
      "2971: loss=0.615, reward_mean=0.120, reward_bound=0.000, batch=199\n",
      "2972: loss=0.607, reward_mean=0.140, reward_bound=0.074, batch=209\n",
      "2973: loss=0.607, reward_mean=0.180, reward_bound=0.122, batch=215\n",
      "2974: loss=0.614, reward_mean=0.240, reward_bound=0.206, batch=215\n",
      "2975: loss=0.610, reward_mean=0.180, reward_bound=0.210, batch=220\n",
      "2976: loss=0.608, reward_mean=0.190, reward_bound=0.254, batch=221\n",
      "2977: loss=0.611, reward_mean=0.160, reward_bound=0.282, batch=223\n",
      "2978: loss=0.620, reward_mean=0.170, reward_bound=0.314, batch=218\n",
      "2979: loss=0.619, reward_mean=0.140, reward_bound=0.289, batch=222\n",
      "2980: loss=0.621, reward_mean=0.160, reward_bound=0.314, batch=224\n",
      "2981: loss=0.618, reward_mean=0.120, reward_bound=0.349, batch=219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2982: loss=0.617, reward_mean=0.140, reward_bound=0.328, batch=223\n",
      "2983: loss=0.612, reward_mean=0.150, reward_bound=0.271, batch=226\n",
      "2984: loss=0.614, reward_mean=0.190, reward_bound=0.331, batch=228\n",
      "2985: loss=0.616, reward_mean=0.190, reward_bound=0.349, batch=225\n",
      "2986: loss=0.617, reward_mean=0.170, reward_bound=0.329, batch=227\n",
      "2987: loss=0.617, reward_mean=0.190, reward_bound=0.380, batch=229\n",
      "2988: loss=0.619, reward_mean=0.180, reward_bound=0.387, batch=226\n",
      "2989: loss=0.622, reward_mean=0.160, reward_bound=0.351, batch=228\n",
      "2990: loss=0.622, reward_mean=0.170, reward_bound=0.353, batch=229\n",
      "2991: loss=0.623, reward_mean=0.190, reward_bound=0.364, batch=230\n",
      "2992: loss=0.621, reward_mean=0.180, reward_bound=0.418, batch=231\n",
      "2993: loss=0.623, reward_mean=0.110, reward_bound=0.430, batch=212\n",
      "2994: loss=0.622, reward_mean=0.170, reward_bound=0.302, batch=218\n",
      "2995: loss=0.618, reward_mean=0.170, reward_bound=0.257, batch=222\n",
      "2996: loss=0.615, reward_mean=0.150, reward_bound=0.254, batch=224\n",
      "2997: loss=0.617, reward_mean=0.180, reward_bound=0.311, batch=227\n",
      "2998: loss=0.618, reward_mean=0.240, reward_bound=0.314, batch=228\n",
      "2999: loss=0.620, reward_mean=0.160, reward_bound=0.349, batch=221\n",
      "3000: loss=0.623, reward_mean=0.210, reward_bound=0.387, batch=218\n",
      "3001: loss=0.620, reward_mean=0.160, reward_bound=0.349, batch=221\n",
      "3002: loss=0.622, reward_mean=0.200, reward_bound=0.314, batch=224\n",
      "3003: loss=0.620, reward_mean=0.170, reward_bound=0.314, batch=226\n",
      "3004: loss=0.627, reward_mean=0.170, reward_bound=0.368, batch=228\n",
      "3005: loss=0.627, reward_mean=0.220, reward_bound=0.387, batch=227\n",
      "3006: loss=0.628, reward_mean=0.150, reward_bound=0.414, batch=229\n",
      "3007: loss=0.628, reward_mean=0.160, reward_bound=0.430, batch=226\n",
      "3008: loss=0.627, reward_mean=0.150, reward_bound=0.387, batch=227\n",
      "3009: loss=0.627, reward_mean=0.220, reward_bound=0.430, batch=226\n",
      "3010: loss=0.628, reward_mean=0.140, reward_bound=0.390, batch=228\n",
      "3011: loss=0.630, reward_mean=0.160, reward_bound=0.435, batch=229\n",
      "3012: loss=0.629, reward_mean=0.150, reward_bound=0.478, batch=231\n",
      "3013: loss=0.629, reward_mean=0.120, reward_bound=0.387, batch=231\n",
      "3014: loss=0.619, reward_mean=0.180, reward_bound=0.478, batch=201\n",
      "3015: loss=0.611, reward_mean=0.190, reward_bound=0.229, batch=210\n",
      "3016: loss=0.612, reward_mean=0.160, reward_bound=0.247, batch=217\n",
      "3017: loss=0.615, reward_mean=0.170, reward_bound=0.206, batch=221\n",
      "3018: loss=0.614, reward_mean=0.170, reward_bound=0.254, batch=223\n",
      "3019: loss=0.612, reward_mean=0.100, reward_bound=0.282, batch=222\n",
      "3020: loss=0.610, reward_mean=0.160, reward_bound=0.282, batch=224\n",
      "3021: loss=0.610, reward_mean=0.220, reward_bound=0.311, batch=227\n",
      "3022: loss=0.611, reward_mean=0.220, reward_bound=0.314, batch=228\n",
      "3023: loss=0.615, reward_mean=0.240, reward_bound=0.349, batch=221\n",
      "3024: loss=0.614, reward_mean=0.180, reward_bound=0.314, batch=224\n",
      "3025: loss=0.612, reward_mean=0.150, reward_bound=0.282, batch=226\n",
      "3026: loss=0.610, reward_mean=0.140, reward_bound=0.349, batch=225\n",
      "3027: loss=0.608, reward_mean=0.190, reward_bound=0.356, batch=227\n",
      "3028: loss=0.612, reward_mean=0.200, reward_bound=0.387, batch=220\n",
      "3029: loss=0.616, reward_mean=0.150, reward_bound=0.376, batch=224\n",
      "3030: loss=0.621, reward_mean=0.160, reward_bound=0.311, batch=227\n",
      "3031: loss=0.620, reward_mean=0.170, reward_bound=0.342, batch=229\n",
      "3032: loss=0.618, reward_mean=0.110, reward_bound=0.328, batch=230\n",
      "3033: loss=0.618, reward_mean=0.120, reward_bound=0.314, batch=230\n",
      "3034: loss=0.619, reward_mean=0.200, reward_bound=0.349, batch=230\n",
      "3035: loss=0.613, reward_mean=0.130, reward_bound=0.387, batch=228\n",
      "3036: loss=0.622, reward_mean=0.160, reward_bound=0.430, batch=220\n",
      "3037: loss=0.621, reward_mean=0.230, reward_bound=0.349, batch=223\n",
      "3038: loss=0.617, reward_mean=0.140, reward_bound=0.290, batch=226\n",
      "3039: loss=0.616, reward_mean=0.190, reward_bound=0.314, batch=227\n",
      "3040: loss=0.618, reward_mean=0.100, reward_bound=0.349, batch=226\n",
      "3041: loss=0.617, reward_mean=0.200, reward_bound=0.409, batch=228\n",
      "3042: loss=0.619, reward_mean=0.150, reward_bound=0.430, batch=227\n",
      "3043: loss=0.616, reward_mean=0.190, reward_bound=0.335, batch=229\n",
      "3044: loss=0.620, reward_mean=0.140, reward_bound=0.349, batch=229\n",
      "3045: loss=0.620, reward_mean=0.230, reward_bound=0.364, batch=230\n",
      "3046: loss=0.619, reward_mean=0.210, reward_bound=0.430, batch=230\n",
      "3047: loss=0.619, reward_mean=0.240, reward_bound=0.451, batch=231\n",
      "3048: loss=0.621, reward_mean=0.190, reward_bound=0.478, batch=213\n",
      "3049: loss=0.618, reward_mean=0.160, reward_bound=0.254, batch=218\n",
      "3050: loss=0.623, reward_mean=0.170, reward_bound=0.282, batch=220\n",
      "3051: loss=0.622, reward_mean=0.170, reward_bound=0.338, batch=224\n",
      "3052: loss=0.630, reward_mean=0.220, reward_bound=0.384, batch=227\n",
      "3053: loss=0.630, reward_mean=0.110, reward_bound=0.224, batch=229\n",
      "3054: loss=0.630, reward_mean=0.150, reward_bound=0.387, batch=225\n",
      "3055: loss=0.634, reward_mean=0.150, reward_bound=0.321, batch=227\n",
      "3056: loss=0.632, reward_mean=0.160, reward_bound=0.414, batch=229\n",
      "3057: loss=0.633, reward_mean=0.210, reward_bound=0.360, batch=230\n",
      "3058: loss=0.632, reward_mean=0.080, reward_bound=0.418, batch=231\n",
      "3059: loss=0.629, reward_mean=0.150, reward_bound=0.430, batch=226\n",
      "3060: loss=0.628, reward_mean=0.200, reward_bound=0.409, batch=228\n",
      "3061: loss=0.629, reward_mean=0.150, reward_bound=0.325, batch=229\n",
      "3062: loss=0.628, reward_mean=0.170, reward_bound=0.430, batch=228\n",
      "3063: loss=0.628, reward_mean=0.160, reward_bound=0.478, batch=230\n",
      "3064: loss=0.628, reward_mean=0.250, reward_bound=0.464, batch=231\n",
      "3065: loss=0.628, reward_mean=0.200, reward_bound=0.430, batch=231\n",
      "3066: loss=0.628, reward_mean=0.100, reward_bound=0.387, batch=231\n",
      "3067: loss=0.623, reward_mean=0.260, reward_bound=0.478, batch=222\n",
      "3068: loss=0.622, reward_mean=0.150, reward_bound=0.373, batch=225\n",
      "3069: loss=0.622, reward_mean=0.130, reward_bound=0.387, batch=226\n",
      "3070: loss=0.621, reward_mean=0.170, reward_bound=0.390, batch=228\n",
      "3071: loss=0.620, reward_mean=0.230, reward_bound=0.392, batch=229\n",
      "3072: loss=0.621, reward_mean=0.160, reward_bound=0.430, batch=225\n",
      "3073: loss=0.620, reward_mean=0.220, reward_bound=0.478, batch=224\n",
      "3074: loss=0.620, reward_mean=0.170, reward_bound=0.478, batch=226\n",
      "3075: loss=0.619, reward_mean=0.220, reward_bound=0.409, batch=228\n",
      "3076: loss=0.618, reward_mean=0.210, reward_bound=0.430, batch=228\n",
      "3077: loss=0.617, reward_mean=0.180, reward_bound=0.478, batch=230\n",
      "3078: loss=0.617, reward_mean=0.200, reward_bound=0.464, batch=231\n",
      "3079: loss=0.617, reward_mean=0.210, reward_bound=0.349, batch=231\n",
      "3080: loss=0.617, reward_mean=0.190, reward_bound=0.478, batch=230\n",
      "3081: loss=0.615, reward_mean=0.120, reward_bound=0.451, batch=231\n",
      "3082: loss=0.617, reward_mean=0.200, reward_bound=0.478, batch=230\n",
      "3083: loss=0.618, reward_mean=0.140, reward_bound=0.365, batch=231\n",
      "3084: loss=0.619, reward_mean=0.160, reward_bound=0.478, batch=231\n",
      "3085: loss=0.619, reward_mean=0.220, reward_bound=0.430, batch=231\n",
      "3087: loss=0.549, reward_mean=0.160, reward_bound=0.000, batch=16\n",
      "3088: loss=0.592, reward_mean=0.250, reward_bound=0.000, batch=41\n",
      "3089: loss=0.569, reward_mean=0.160, reward_bound=0.000, batch=57\n",
      "3090: loss=0.567, reward_mean=0.250, reward_bound=0.000, batch=82\n",
      "3091: loss=0.566, reward_mean=0.200, reward_bound=0.000, batch=102\n",
      "3092: loss=0.560, reward_mean=0.170, reward_bound=0.000, batch=119\n",
      "3093: loss=0.558, reward_mean=0.160, reward_bound=0.000, batch=135\n",
      "3094: loss=0.553, reward_mean=0.080, reward_bound=0.000, batch=143\n",
      "3095: loss=0.548, reward_mean=0.220, reward_bound=0.000, batch=165\n",
      "3096: loss=0.547, reward_mean=0.260, reward_bound=0.004, batch=185\n",
      "3097: loss=0.553, reward_mean=0.230, reward_bound=0.019, batch=199\n",
      "3098: loss=0.552, reward_mean=0.150, reward_bound=0.024, batch=209\n",
      "3099: loss=0.554, reward_mean=0.180, reward_bound=0.032, batch=216\n",
      "3100: loss=0.560, reward_mean=0.250, reward_bound=0.052, batch=217\n",
      "3101: loss=0.559, reward_mean=0.140, reward_bound=0.058, batch=228\n",
      "3102: loss=0.560, reward_mean=0.250, reward_bound=0.072, batch=226\n",
      "3103: loss=0.565, reward_mean=0.220, reward_bound=0.094, batch=228\n",
      "3104: loss=0.562, reward_mean=0.260, reward_bound=0.109, batch=220\n",
      "3105: loss=0.548, reward_mean=0.280, reward_bound=0.135, batch=214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106: loss=0.550, reward_mean=0.250, reward_bound=0.150, batch=208\n",
      "3107: loss=0.546, reward_mean=0.280, reward_bound=0.167, batch=208\n",
      "3108: loss=0.547, reward_mean=0.230, reward_bound=0.185, batch=197\n",
      "3109: loss=0.540, reward_mean=0.190, reward_bound=0.109, batch=209\n",
      "3110: loss=0.542, reward_mean=0.240, reward_bound=0.167, batch=214\n",
      "3111: loss=0.556, reward_mean=0.220, reward_bound=0.206, batch=202\n",
      "3112: loss=0.552, reward_mean=0.160, reward_bound=0.130, batch=211\n",
      "3113: loss=0.556, reward_mean=0.200, reward_bound=0.229, batch=191\n",
      "3114: loss=0.552, reward_mean=0.110, reward_bound=0.000, batch=202\n",
      "3115: loss=0.550, reward_mean=0.160, reward_bound=0.092, batch=211\n",
      "3116: loss=0.550, reward_mean=0.160, reward_bound=0.109, batch=217\n",
      "3117: loss=0.544, reward_mean=0.140, reward_bound=0.132, batch=222\n",
      "3118: loss=0.545, reward_mean=0.190, reward_bound=0.167, batch=224\n",
      "3119: loss=0.543, reward_mean=0.250, reward_bound=0.226, batch=227\n",
      "3120: loss=0.543, reward_mean=0.180, reward_bound=0.249, batch=229\n",
      "3121: loss=0.550, reward_mean=0.200, reward_bound=0.254, batch=203\n",
      "3122: loss=0.550, reward_mean=0.190, reward_bound=0.154, batch=212\n",
      "3123: loss=0.544, reward_mean=0.260, reward_bound=0.191, batch=218\n",
      "3124: loss=0.541, reward_mean=0.230, reward_bound=0.231, batch=222\n",
      "3125: loss=0.534, reward_mean=0.260, reward_bound=0.282, batch=193\n",
      "3126: loss=0.523, reward_mean=0.180, reward_bound=0.080, batch=204\n",
      "3127: loss=0.529, reward_mean=0.250, reward_bound=0.167, batch=211\n",
      "3128: loss=0.523, reward_mean=0.260, reward_bound=0.206, batch=217\n",
      "3129: loss=0.524, reward_mean=0.210, reward_bound=0.254, batch=220\n",
      "3130: loss=0.523, reward_mean=0.270, reward_bound=0.274, batch=224\n",
      "3131: loss=0.527, reward_mean=0.220, reward_bound=0.282, batch=217\n",
      "3132: loss=0.531, reward_mean=0.250, reward_bound=0.302, batch=222\n",
      "3133: loss=0.543, reward_mean=0.240, reward_bound=0.314, batch=174\n",
      "3134: loss=0.525, reward_mean=0.240, reward_bound=0.027, batch=192\n",
      "3135: loss=0.534, reward_mean=0.270, reward_bound=0.122, batch=203\n",
      "3136: loss=0.535, reward_mean=0.290, reward_bound=0.167, batch=210\n",
      "3137: loss=0.538, reward_mean=0.220, reward_bound=0.229, batch=211\n",
      "3138: loss=0.535, reward_mean=0.240, reward_bound=0.254, batch=216\n",
      "3139: loss=0.533, reward_mean=0.210, reward_bound=0.282, batch=218\n",
      "3140: loss=0.533, reward_mean=0.310, reward_bound=0.314, batch=214\n",
      "3141: loss=0.534, reward_mean=0.230, reward_bound=0.252, batch=220\n",
      "3142: loss=0.535, reward_mean=0.200, reward_bound=0.274, batch=224\n",
      "3143: loss=0.538, reward_mean=0.210, reward_bound=0.314, batch=226\n",
      "3144: loss=0.539, reward_mean=0.190, reward_bound=0.331, batch=228\n",
      "3145: loss=0.538, reward_mean=0.280, reward_bound=0.241, batch=229\n",
      "3146: loss=0.530, reward_mean=0.220, reward_bound=0.349, batch=169\n",
      "3147: loss=0.526, reward_mean=0.240, reward_bound=0.038, batch=187\n",
      "3148: loss=0.518, reward_mean=0.170, reward_bound=0.007, batch=201\n",
      "3149: loss=0.508, reward_mean=0.250, reward_bound=0.058, batch=209\n",
      "3150: loss=0.510, reward_mean=0.230, reward_bound=0.098, batch=215\n",
      "3151: loss=0.501, reward_mean=0.220, reward_bound=0.122, batch=219\n",
      "3152: loss=0.507, reward_mean=0.280, reward_bound=0.167, batch=221\n",
      "3153: loss=0.515, reward_mean=0.250, reward_bound=0.206, batch=219\n",
      "3154: loss=0.522, reward_mean=0.200, reward_bound=0.229, batch=218\n",
      "3155: loss=0.523, reward_mean=0.200, reward_bound=0.254, batch=218\n",
      "3156: loss=0.523, reward_mean=0.250, reward_bound=0.231, batch=222\n",
      "3157: loss=0.519, reward_mean=0.290, reward_bound=0.282, batch=217\n",
      "3158: loss=0.519, reward_mean=0.220, reward_bound=0.267, batch=222\n",
      "3159: loss=0.517, reward_mean=0.250, reward_bound=0.245, batch=225\n",
      "3160: loss=0.525, reward_mean=0.200, reward_bound=0.314, batch=218\n",
      "3161: loss=0.530, reward_mean=0.240, reward_bound=0.349, batch=211\n",
      "3162: loss=0.528, reward_mean=0.250, reward_bound=0.206, batch=216\n",
      "3163: loss=0.526, reward_mean=0.220, reward_bound=0.229, batch=220\n",
      "3164: loss=0.526, reward_mean=0.270, reward_bound=0.282, batch=220\n",
      "3165: loss=0.527, reward_mean=0.220, reward_bound=0.296, batch=224\n",
      "3166: loss=0.529, reward_mean=0.220, reward_bound=0.314, batch=224\n",
      "3167: loss=0.523, reward_mean=0.250, reward_bound=0.349, batch=224\n",
      "3168: loss=0.521, reward_mean=0.200, reward_bound=0.280, batch=227\n",
      "3169: loss=0.524, reward_mean=0.280, reward_bound=0.387, batch=141\n",
      "3170: loss=0.510, reward_mean=0.210, reward_bound=0.000, batch=162\n",
      "3171: loss=0.487, reward_mean=0.300, reward_bound=0.031, batch=182\n",
      "3172: loss=0.492, reward_mean=0.270, reward_bound=0.047, batch=197\n",
      "3173: loss=0.494, reward_mean=0.200, reward_bound=0.058, batch=209\n",
      "3174: loss=0.499, reward_mean=0.190, reward_bound=0.080, batch=215\n",
      "3175: loss=0.501, reward_mean=0.280, reward_bound=0.112, batch=220\n",
      "3176: loss=0.497, reward_mean=0.210, reward_bound=0.122, batch=220\n",
      "3177: loss=0.499, reward_mean=0.330, reward_bound=0.150, batch=222\n",
      "3178: loss=0.509, reward_mean=0.220, reward_bound=0.185, batch=221\n",
      "3179: loss=0.514, reward_mean=0.190, reward_bound=0.206, batch=218\n",
      "3180: loss=0.510, reward_mean=0.240, reward_bound=0.229, batch=211\n",
      "3181: loss=0.514, reward_mean=0.230, reward_bound=0.254, batch=202\n",
      "3182: loss=0.514, reward_mean=0.240, reward_bound=0.109, batch=210\n",
      "3183: loss=0.515, reward_mean=0.200, reward_bound=0.167, batch=214\n",
      "3184: loss=0.516, reward_mean=0.240, reward_bound=0.204, batch=220\n",
      "3185: loss=0.518, reward_mean=0.250, reward_bound=0.229, batch=223\n",
      "3186: loss=0.520, reward_mean=0.330, reward_bound=0.282, batch=209\n",
      "3187: loss=0.525, reward_mean=0.250, reward_bound=0.225, batch=216\n",
      "3188: loss=0.521, reward_mean=0.200, reward_bound=0.230, batch=221\n",
      "3189: loss=0.516, reward_mean=0.200, reward_bound=0.282, batch=224\n",
      "3190: loss=0.524, reward_mean=0.270, reward_bound=0.314, batch=209\n",
      "3191: loss=0.522, reward_mean=0.250, reward_bound=0.215, batch=216\n",
      "3192: loss=0.522, reward_mean=0.180, reward_bound=0.198, batch=221\n",
      "3193: loss=0.526, reward_mean=0.180, reward_bound=0.254, batch=223\n",
      "3194: loss=0.528, reward_mean=0.300, reward_bound=0.282, batch=225\n",
      "3195: loss=0.524, reward_mean=0.190, reward_bound=0.314, batch=224\n",
      "3196: loss=0.527, reward_mean=0.270, reward_bound=0.349, batch=210\n",
      "3197: loss=0.527, reward_mean=0.260, reward_bound=0.254, batch=214\n",
      "3198: loss=0.524, reward_mean=0.200, reward_bound=0.311, batch=220\n",
      "3199: loss=0.526, reward_mean=0.240, reward_bound=0.314, batch=223\n",
      "3200: loss=0.528, reward_mean=0.210, reward_bound=0.349, batch=223\n",
      "3201: loss=0.530, reward_mean=0.170, reward_bound=0.314, batch=225\n",
      "3202: loss=0.529, reward_mean=0.240, reward_bound=0.314, batch=226\n",
      "3203: loss=0.528, reward_mean=0.320, reward_bound=0.349, batch=226\n",
      "3204: loss=0.528, reward_mean=0.200, reward_bound=0.349, batch=227\n",
      "3205: loss=0.520, reward_mean=0.150, reward_bound=0.387, batch=202\n",
      "3206: loss=0.519, reward_mean=0.210, reward_bound=0.263, batch=211\n",
      "3207: loss=0.516, reward_mean=0.220, reward_bound=0.206, batch=217\n",
      "3208: loss=0.515, reward_mean=0.240, reward_bound=0.245, batch=222\n",
      "3209: loss=0.520, reward_mean=0.230, reward_bound=0.282, batch=221\n",
      "3210: loss=0.524, reward_mean=0.190, reward_bound=0.314, batch=219\n",
      "3211: loss=0.516, reward_mean=0.250, reward_bound=0.349, batch=219\n",
      "3212: loss=0.515, reward_mean=0.190, reward_bound=0.254, batch=222\n",
      "3213: loss=0.518, reward_mean=0.260, reward_bound=0.360, batch=225\n",
      "3214: loss=0.523, reward_mean=0.190, reward_bound=0.387, batch=219\n",
      "3215: loss=0.526, reward_mean=0.290, reward_bound=0.328, batch=223\n",
      "3216: loss=0.525, reward_mean=0.210, reward_bound=0.349, batch=225\n",
      "3217: loss=0.526, reward_mean=0.220, reward_bound=0.356, batch=227\n",
      "3218: loss=0.526, reward_mean=0.210, reward_bound=0.342, batch=229\n",
      "3219: loss=0.527, reward_mean=0.170, reward_bound=0.349, batch=229\n",
      "3220: loss=0.529, reward_mean=0.220, reward_bound=0.387, batch=228\n",
      "3221: loss=0.502, reward_mean=0.240, reward_bound=0.430, batch=122\n",
      "3222: loss=0.483, reward_mean=0.180, reward_bound=0.000, batch=140\n",
      "3223: loss=0.473, reward_mean=0.280, reward_bound=0.001, batch=168\n",
      "3224: loss=0.470, reward_mean=0.160, reward_bound=0.000, batch=184\n",
      "3225: loss=0.464, reward_mean=0.180, reward_bound=0.011, batch=199\n",
      "3226: loss=0.479, reward_mean=0.310, reward_bound=0.044, batch=209\n",
      "3227: loss=0.470, reward_mean=0.290, reward_bound=0.072, batch=215\n",
      "3228: loss=0.470, reward_mean=0.250, reward_bound=0.098, batch=219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3229: loss=0.468, reward_mean=0.290, reward_bound=0.135, batch=218\n",
      "3230: loss=0.473, reward_mean=0.290, reward_bound=0.167, batch=213\n",
      "3231: loss=0.479, reward_mean=0.320, reward_bound=0.185, batch=217\n",
      "3232: loss=0.472, reward_mean=0.280, reward_bound=0.206, batch=215\n",
      "3233: loss=0.483, reward_mean=0.310, reward_bound=0.229, batch=210\n",
      "3234: loss=0.481, reward_mean=0.240, reward_bound=0.247, batch=217\n",
      "3235: loss=0.493, reward_mean=0.270, reward_bound=0.254, batch=204\n",
      "3236: loss=0.490, reward_mean=0.240, reward_bound=0.229, batch=212\n",
      "3237: loss=0.483, reward_mean=0.260, reward_bound=0.236, batch=218\n",
      "3238: loss=0.478, reward_mean=0.320, reward_bound=0.282, batch=208\n",
      "3239: loss=0.479, reward_mean=0.260, reward_bound=0.282, batch=214\n",
      "3240: loss=0.476, reward_mean=0.230, reward_bound=0.252, batch=220\n",
      "3241: loss=0.480, reward_mean=0.210, reward_bound=0.282, batch=222\n",
      "3242: loss=0.488, reward_mean=0.290, reward_bound=0.314, batch=209\n",
      "3243: loss=0.491, reward_mean=0.230, reward_bound=0.295, batch=216\n",
      "3244: loss=0.493, reward_mean=0.230, reward_bound=0.331, batch=221\n",
      "3245: loss=0.492, reward_mean=0.160, reward_bound=0.229, batch=224\n",
      "3246: loss=0.496, reward_mean=0.230, reward_bound=0.282, batch=226\n",
      "3247: loss=0.516, reward_mean=0.240, reward_bound=0.349, batch=202\n",
      "3248: loss=0.508, reward_mean=0.240, reward_bound=0.229, batch=210\n",
      "3249: loss=0.509, reward_mean=0.260, reward_bound=0.282, batch=214\n",
      "3250: loss=0.507, reward_mean=0.220, reward_bound=0.282, batch=219\n",
      "3251: loss=0.507, reward_mean=0.230, reward_bound=0.282, batch=221\n",
      "3252: loss=0.508, reward_mean=0.260, reward_bound=0.314, batch=223\n",
      "3253: loss=0.509, reward_mean=0.240, reward_bound=0.349, batch=221\n",
      "3254: loss=0.507, reward_mean=0.250, reward_bound=0.282, batch=224\n",
      "3255: loss=0.507, reward_mean=0.210, reward_bound=0.314, batch=224\n",
      "3256: loss=0.506, reward_mean=0.240, reward_bound=0.311, batch=227\n",
      "3257: loss=0.507, reward_mean=0.240, reward_bound=0.380, batch=229\n",
      "3258: loss=0.505, reward_mean=0.270, reward_bound=0.387, batch=194\n",
      "3259: loss=0.494, reward_mean=0.220, reward_bound=0.146, batch=206\n",
      "3260: loss=0.499, reward_mean=0.310, reward_bound=0.229, batch=212\n",
      "3261: loss=0.499, reward_mean=0.270, reward_bound=0.254, batch=216\n",
      "3262: loss=0.501, reward_mean=0.250, reward_bound=0.282, batch=219\n",
      "3263: loss=0.499, reward_mean=0.310, reward_bound=0.314, batch=219\n",
      "3264: loss=0.506, reward_mean=0.280, reward_bound=0.349, batch=222\n",
      "3265: loss=0.510, reward_mean=0.290, reward_bound=0.387, batch=218\n",
      "3266: loss=0.509, reward_mean=0.280, reward_bound=0.353, batch=222\n",
      "3267: loss=0.510, reward_mean=0.270, reward_bound=0.349, batch=224\n",
      "3268: loss=0.507, reward_mean=0.200, reward_bound=0.345, batch=227\n",
      "3269: loss=0.510, reward_mean=0.190, reward_bound=0.349, batch=228\n",
      "3270: loss=0.509, reward_mean=0.290, reward_bound=0.268, batch=229\n",
      "3271: loss=0.509, reward_mean=0.220, reward_bound=0.343, batch=230\n",
      "3272: loss=0.505, reward_mean=0.260, reward_bound=0.387, batch=230\n",
      "3273: loss=0.504, reward_mean=0.240, reward_bound=0.406, batch=231\n",
      "3274: loss=0.504, reward_mean=0.290, reward_bound=0.430, batch=180\n",
      "3275: loss=0.486, reward_mean=0.220, reward_bound=0.056, batch=196\n",
      "3276: loss=0.481, reward_mean=0.260, reward_bound=0.135, batch=205\n",
      "3277: loss=0.481, reward_mean=0.270, reward_bound=0.206, batch=211\n",
      "3278: loss=0.474, reward_mean=0.270, reward_bound=0.229, batch=212\n",
      "3279: loss=0.484, reward_mean=0.200, reward_bound=0.254, batch=212\n",
      "3280: loss=0.488, reward_mean=0.250, reward_bound=0.172, batch=218\n",
      "3281: loss=0.487, reward_mean=0.260, reward_bound=0.234, batch=222\n",
      "3282: loss=0.489, reward_mean=0.250, reward_bound=0.282, batch=218\n",
      "3283: loss=0.488, reward_mean=0.310, reward_bound=0.314, batch=215\n",
      "3284: loss=0.484, reward_mean=0.260, reward_bound=0.254, batch=219\n",
      "3285: loss=0.487, reward_mean=0.290, reward_bound=0.349, batch=212\n",
      "3286: loss=0.484, reward_mean=0.240, reward_bound=0.282, batch=217\n",
      "3287: loss=0.482, reward_mean=0.280, reward_bound=0.277, batch=222\n",
      "3288: loss=0.488, reward_mean=0.270, reward_bound=0.387, batch=206\n",
      "3289: loss=0.492, reward_mean=0.270, reward_bound=0.282, batch=213\n",
      "3290: loss=0.493, reward_mean=0.260, reward_bound=0.254, batch=218\n",
      "3291: loss=0.498, reward_mean=0.220, reward_bound=0.208, batch=222\n",
      "3292: loss=0.492, reward_mean=0.160, reward_bound=0.229, batch=224\n",
      "3293: loss=0.493, reward_mean=0.300, reward_bound=0.311, batch=227\n",
      "3294: loss=0.495, reward_mean=0.280, reward_bound=0.314, batch=227\n",
      "3295: loss=0.499, reward_mean=0.320, reward_bound=0.349, batch=228\n",
      "3296: loss=0.499, reward_mean=0.310, reward_bound=0.387, batch=228\n",
      "3297: loss=0.498, reward_mean=0.230, reward_bound=0.321, batch=229\n",
      "3298: loss=0.501, reward_mean=0.200, reward_bound=0.405, batch=230\n",
      "3299: loss=0.506, reward_mean=0.270, reward_bound=0.430, batch=210\n",
      "3300: loss=0.499, reward_mean=0.280, reward_bound=0.234, batch=217\n",
      "3301: loss=0.503, reward_mean=0.260, reward_bound=0.254, batch=221\n",
      "3302: loss=0.503, reward_mean=0.180, reward_bound=0.314, batch=220\n",
      "3303: loss=0.502, reward_mean=0.240, reward_bound=0.304, batch=224\n",
      "3304: loss=0.502, reward_mean=0.250, reward_bound=0.314, batch=226\n",
      "3305: loss=0.507, reward_mean=0.320, reward_bound=0.349, batch=222\n",
      "3306: loss=0.507, reward_mean=0.270, reward_bound=0.336, batch=225\n",
      "3307: loss=0.510, reward_mean=0.250, reward_bound=0.387, batch=225\n",
      "3308: loss=0.509, reward_mean=0.210, reward_bound=0.396, batch=227\n",
      "3309: loss=0.508, reward_mean=0.150, reward_bound=0.302, batch=229\n",
      "3310: loss=0.505, reward_mean=0.280, reward_bound=0.430, batch=223\n",
      "3311: loss=0.507, reward_mean=0.230, reward_bound=0.358, batch=226\n",
      "3312: loss=0.506, reward_mean=0.250, reward_bound=0.351, batch=228\n",
      "3313: loss=0.505, reward_mean=0.190, reward_bound=0.293, batch=229\n",
      "3314: loss=0.507, reward_mean=0.260, reward_bound=0.387, batch=228\n",
      "3315: loss=0.507, reward_mean=0.210, reward_bound=0.392, batch=229\n",
      "3316: loss=0.505, reward_mean=0.290, reward_bound=0.364, batch=230\n",
      "3317: loss=0.506, reward_mean=0.320, reward_bound=0.418, batch=231\n",
      "3318: loss=0.506, reward_mean=0.220, reward_bound=0.430, batch=228\n",
      "3319: loss=0.505, reward_mean=0.280, reward_bound=0.478, batch=230\n",
      "3320: loss=0.505, reward_mean=0.280, reward_bound=0.478, batch=90\n",
      "3321: loss=0.447, reward_mean=0.240, reward_bound=0.000, batch=114\n",
      "3322: loss=0.432, reward_mean=0.290, reward_bound=0.000, batch=143\n",
      "3323: loss=0.426, reward_mean=0.200, reward_bound=0.000, batch=163\n",
      "3324: loss=0.432, reward_mean=0.230, reward_bound=0.001, batch=184\n",
      "3325: loss=0.441, reward_mean=0.260, reward_bound=0.009, batch=199\n",
      "3326: loss=0.442, reward_mean=0.200, reward_bound=0.017, batch=209\n",
      "3327: loss=0.445, reward_mean=0.370, reward_bound=0.061, batch=216\n",
      "3328: loss=0.450, reward_mean=0.310, reward_bound=0.076, batch=221\n",
      "3329: loss=0.457, reward_mean=0.270, reward_bound=0.098, batch=219\n",
      "3330: loss=0.447, reward_mean=0.210, reward_bound=0.109, batch=218\n",
      "3331: loss=0.462, reward_mean=0.360, reward_bound=0.135, batch=220\n",
      "3332: loss=0.460, reward_mean=0.270, reward_bound=0.150, batch=222\n",
      "3333: loss=0.465, reward_mean=0.210, reward_bound=0.167, batch=223\n",
      "3334: loss=0.475, reward_mean=0.200, reward_bound=0.185, batch=214\n",
      "3335: loss=0.469, reward_mean=0.270, reward_bound=0.206, batch=217\n",
      "3336: loss=0.464, reward_mean=0.280, reward_bound=0.229, batch=209\n",
      "3337: loss=0.468, reward_mean=0.360, reward_bound=0.254, batch=197\n",
      "3338: loss=0.465, reward_mean=0.260, reward_bound=0.163, batch=208\n",
      "3339: loss=0.465, reward_mean=0.240, reward_bound=0.229, batch=213\n",
      "3340: loss=0.461, reward_mean=0.310, reward_bound=0.220, batch=219\n",
      "3341: loss=0.459, reward_mean=0.220, reward_bound=0.206, batch=222\n",
      "3342: loss=0.464, reward_mean=0.310, reward_bound=0.254, batch=224\n",
      "3343: loss=0.473, reward_mean=0.320, reward_bound=0.282, batch=207\n",
      "3344: loss=0.476, reward_mean=0.340, reward_bound=0.308, batch=215\n",
      "3345: loss=0.483, reward_mean=0.280, reward_bound=0.314, batch=192\n",
      "3346: loss=0.478, reward_mean=0.280, reward_bound=0.072, batch=204\n",
      "3347: loss=0.474, reward_mean=0.210, reward_bound=0.107, batch=213\n",
      "3348: loss=0.467, reward_mean=0.280, reward_bound=0.167, batch=218\n",
      "3349: loss=0.475, reward_mean=0.260, reward_bound=0.229, batch=218\n",
      "3350: loss=0.474, reward_mean=0.270, reward_bound=0.254, batch=220\n",
      "3351: loss=0.474, reward_mean=0.320, reward_bound=0.282, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3352: loss=0.475, reward_mean=0.220, reward_bound=0.314, batch=217\n",
      "3353: loss=0.471, reward_mean=0.350, reward_bound=0.249, batch=222\n",
      "3354: loss=0.470, reward_mean=0.330, reward_bound=0.292, batch=225\n",
      "3355: loss=0.469, reward_mean=0.300, reward_bound=0.266, batch=227\n",
      "3356: loss=0.476, reward_mean=0.300, reward_bound=0.314, batch=228\n",
      "3357: loss=0.493, reward_mean=0.260, reward_bound=0.349, batch=184\n",
      "3358: loss=0.478, reward_mean=0.200, reward_bound=0.034, batch=199\n",
      "3359: loss=0.478, reward_mean=0.280, reward_bound=0.103, batch=209\n",
      "3360: loss=0.474, reward_mean=0.340, reward_bound=0.141, batch=216\n",
      "3361: loss=0.476, reward_mean=0.170, reward_bound=0.150, batch=219\n",
      "3362: loss=0.479, reward_mean=0.300, reward_bound=0.206, batch=218\n",
      "3363: loss=0.474, reward_mean=0.230, reward_bound=0.229, batch=220\n",
      "3364: loss=0.480, reward_mean=0.210, reward_bound=0.254, batch=217\n",
      "3365: loss=0.482, reward_mean=0.260, reward_bound=0.282, batch=217\n",
      "3366: loss=0.484, reward_mean=0.250, reward_bound=0.314, batch=215\n",
      "3367: loss=0.480, reward_mean=0.290, reward_bound=0.254, batch=219\n",
      "3368: loss=0.483, reward_mean=0.220, reward_bound=0.295, batch=223\n",
      "3369: loss=0.482, reward_mean=0.320, reward_bound=0.335, batch=226\n",
      "3370: loss=0.488, reward_mean=0.270, reward_bound=0.349, batch=217\n",
      "3371: loss=0.488, reward_mean=0.330, reward_bound=0.282, batch=221\n",
      "3372: loss=0.490, reward_mean=0.310, reward_bound=0.387, batch=177\n",
      "3373: loss=0.488, reward_mean=0.250, reward_bound=0.063, batch=194\n",
      "3374: loss=0.483, reward_mean=0.330, reward_bound=0.206, batch=205\n",
      "3375: loss=0.484, reward_mean=0.220, reward_bound=0.157, batch=213\n",
      "3376: loss=0.479, reward_mean=0.310, reward_bound=0.220, batch=219\n",
      "3377: loss=0.477, reward_mean=0.260, reward_bound=0.229, batch=220\n",
      "3378: loss=0.475, reward_mean=0.260, reward_bound=0.254, batch=219\n",
      "3379: loss=0.479, reward_mean=0.280, reward_bound=0.282, batch=222\n",
      "3380: loss=0.478, reward_mean=0.260, reward_bound=0.314, batch=221\n",
      "3381: loss=0.496, reward_mean=0.330, reward_bound=0.349, batch=214\n",
      "3382: loss=0.494, reward_mean=0.300, reward_bound=0.254, batch=219\n",
      "3383: loss=0.491, reward_mean=0.270, reward_bound=0.282, batch=222\n",
      "3384: loss=0.488, reward_mean=0.190, reward_bound=0.314, batch=223\n",
      "3385: loss=0.489, reward_mean=0.220, reward_bound=0.349, batch=222\n",
      "3386: loss=0.498, reward_mean=0.200, reward_bound=0.387, batch=209\n",
      "3387: loss=0.491, reward_mean=0.200, reward_bound=0.185, batch=214\n",
      "3388: loss=0.492, reward_mean=0.230, reward_bound=0.254, batch=219\n",
      "3389: loss=0.497, reward_mean=0.300, reward_bound=0.314, batch=220\n",
      "3390: loss=0.493, reward_mean=0.180, reward_bound=0.206, batch=225\n",
      "3391: loss=0.498, reward_mean=0.210, reward_bound=0.282, batch=226\n",
      "3392: loss=0.499, reward_mean=0.270, reward_bound=0.349, batch=224\n",
      "3393: loss=0.499, reward_mean=0.260, reward_bound=0.282, batch=226\n",
      "3394: loss=0.497, reward_mean=0.310, reward_bound=0.368, batch=228\n",
      "3395: loss=0.496, reward_mean=0.270, reward_bound=0.387, batch=217\n",
      "3396: loss=0.497, reward_mean=0.230, reward_bound=0.314, batch=221\n",
      "3397: loss=0.496, reward_mean=0.190, reward_bound=0.206, batch=224\n",
      "3398: loss=0.495, reward_mean=0.220, reward_bound=0.349, batch=226\n",
      "3399: loss=0.495, reward_mean=0.280, reward_bound=0.387, batch=225\n",
      "3400: loss=0.507, reward_mean=0.200, reward_bound=0.430, batch=158\n",
      "3401: loss=0.491, reward_mean=0.210, reward_bound=0.000, batch=179\n",
      "3402: loss=0.492, reward_mean=0.200, reward_bound=0.024, batch=195\n",
      "3403: loss=0.483, reward_mean=0.300, reward_bound=0.072, batch=203\n",
      "3404: loss=0.482, reward_mean=0.260, reward_bound=0.135, batch=211\n",
      "3405: loss=0.479, reward_mean=0.200, reward_bound=0.167, batch=215\n",
      "3406: loss=0.473, reward_mean=0.300, reward_bound=0.206, batch=218\n",
      "3407: loss=0.467, reward_mean=0.210, reward_bound=0.169, batch=222\n",
      "3408: loss=0.465, reward_mean=0.190, reward_bound=0.229, batch=220\n",
      "3409: loss=0.461, reward_mean=0.180, reward_bound=0.222, batch=224\n",
      "3410: loss=0.480, reward_mean=0.330, reward_bound=0.254, batch=214\n",
      "3411: loss=0.475, reward_mean=0.220, reward_bound=0.280, batch=220\n",
      "3412: loss=0.478, reward_mean=0.240, reward_bound=0.282, batch=214\n",
      "3413: loss=0.476, reward_mean=0.270, reward_bound=0.308, batch=220\n",
      "3414: loss=0.481, reward_mean=0.250, reward_bound=0.314, batch=209\n",
      "3415: loss=0.476, reward_mean=0.230, reward_bound=0.206, batch=213\n",
      "3416: loss=0.473, reward_mean=0.250, reward_bound=0.254, batch=218\n",
      "3417: loss=0.475, reward_mean=0.290, reward_bound=0.282, batch=221\n",
      "3418: loss=0.474, reward_mean=0.270, reward_bound=0.314, batch=222\n",
      "3419: loss=0.484, reward_mean=0.310, reward_bound=0.349, batch=209\n",
      "3420: loss=0.482, reward_mean=0.250, reward_bound=0.229, batch=215\n",
      "3421: loss=0.478, reward_mean=0.250, reward_bound=0.254, batch=217\n",
      "3422: loss=0.475, reward_mean=0.280, reward_bound=0.254, batch=221\n",
      "3423: loss=0.477, reward_mean=0.300, reward_bound=0.314, batch=221\n",
      "3424: loss=0.473, reward_mean=0.320, reward_bound=0.349, batch=222\n",
      "3425: loss=0.477, reward_mean=0.320, reward_bound=0.387, batch=205\n",
      "3426: loss=0.479, reward_mean=0.270, reward_bound=0.266, batch=213\n",
      "3427: loss=0.475, reward_mean=0.320, reward_bound=0.314, batch=217\n",
      "3428: loss=0.475, reward_mean=0.230, reward_bound=0.314, batch=220\n",
      "3429: loss=0.474, reward_mean=0.180, reward_bound=0.304, batch=224\n",
      "3430: loss=0.475, reward_mean=0.270, reward_bound=0.349, batch=225\n",
      "3431: loss=0.474, reward_mean=0.240, reward_bound=0.356, batch=227\n",
      "3432: loss=0.472, reward_mean=0.270, reward_bound=0.373, batch=229\n",
      "3433: loss=0.474, reward_mean=0.260, reward_bound=0.387, batch=223\n",
      "3434: loss=0.472, reward_mean=0.180, reward_bound=0.384, batch=226\n",
      "3435: loss=0.471, reward_mean=0.210, reward_bound=0.282, batch=227\n",
      "3436: loss=0.473, reward_mean=0.220, reward_bound=0.422, batch=229\n",
      "3437: loss=0.490, reward_mean=0.370, reward_bound=0.430, batch=201\n",
      "3438: loss=0.485, reward_mean=0.300, reward_bound=0.167, batch=210\n",
      "3439: loss=0.489, reward_mean=0.320, reward_bound=0.247, batch=217\n",
      "3440: loss=0.479, reward_mean=0.270, reward_bound=0.254, batch=220\n",
      "3441: loss=0.487, reward_mean=0.220, reward_bound=0.282, batch=221\n",
      "3442: loss=0.487, reward_mean=0.250, reward_bound=0.254, batch=224\n",
      "3443: loss=0.483, reward_mean=0.320, reward_bound=0.311, batch=227\n",
      "3444: loss=0.490, reward_mean=0.230, reward_bound=0.314, batch=226\n",
      "3445: loss=0.488, reward_mean=0.160, reward_bound=0.268, batch=228\n",
      "3446: loss=0.487, reward_mean=0.310, reward_bound=0.289, batch=229\n",
      "3447: loss=0.490, reward_mean=0.330, reward_bound=0.349, batch=225\n",
      "3448: loss=0.488, reward_mean=0.320, reward_bound=0.329, batch=227\n",
      "3449: loss=0.486, reward_mean=0.240, reward_bound=0.387, batch=221\n",
      "3450: loss=0.485, reward_mean=0.260, reward_bound=0.282, batch=224\n",
      "3451: loss=0.484, reward_mean=0.260, reward_bound=0.314, batch=226\n",
      "3452: loss=0.485, reward_mean=0.210, reward_bound=0.349, batch=227\n",
      "3453: loss=0.487, reward_mean=0.280, reward_bound=0.387, batch=226\n",
      "3454: loss=0.486, reward_mean=0.300, reward_bound=0.331, batch=228\n",
      "3455: loss=0.486, reward_mean=0.190, reward_bound=0.430, batch=217\n",
      "3456: loss=0.486, reward_mean=0.210, reward_bound=0.216, batch=222\n",
      "3457: loss=0.480, reward_mean=0.210, reward_bound=0.292, batch=225\n",
      "3458: loss=0.481, reward_mean=0.280, reward_bound=0.349, batch=225\n",
      "3459: loss=0.480, reward_mean=0.260, reward_bound=0.396, batch=227\n",
      "3460: loss=0.479, reward_mean=0.240, reward_bound=0.342, batch=229\n",
      "3461: loss=0.478, reward_mean=0.240, reward_bound=0.364, batch=230\n",
      "3462: loss=0.479, reward_mean=0.200, reward_bound=0.376, batch=231\n",
      "3463: loss=0.481, reward_mean=0.330, reward_bound=0.387, batch=229\n",
      "3464: loss=0.480, reward_mean=0.220, reward_bound=0.405, batch=230\n",
      "3465: loss=0.480, reward_mean=0.240, reward_bound=0.418, batch=231\n",
      "3466: loss=0.479, reward_mean=0.210, reward_bound=0.430, batch=226\n",
      "3467: loss=0.479, reward_mean=0.220, reward_bound=0.298, batch=228\n",
      "3468: loss=0.480, reward_mean=0.190, reward_bound=0.260, batch=229\n",
      "3469: loss=0.481, reward_mean=0.190, reward_bound=0.364, batch=230\n",
      "3470: loss=0.481, reward_mean=0.220, reward_bound=0.365, batch=231\n",
      "3471: loss=0.482, reward_mean=0.250, reward_bound=0.387, batch=231\n",
      "3472: loss=0.482, reward_mean=0.250, reward_bound=0.387, batch=231\n",
      "3473: loss=0.479, reward_mean=0.270, reward_bound=0.430, batch=230\n",
      "3474: loss=0.485, reward_mean=0.280, reward_bound=0.478, batch=151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3475: loss=0.476, reward_mean=0.240, reward_bound=0.000, batch=175\n",
      "3476: loss=0.468, reward_mean=0.330, reward_bound=0.068, batch=192\n",
      "3477: loss=0.468, reward_mean=0.230, reward_bound=0.092, batch=204\n",
      "3478: loss=0.470, reward_mean=0.220, reward_bound=0.109, batch=212\n",
      "3479: loss=0.472, reward_mean=0.280, reward_bound=0.155, batch=218\n",
      "3480: loss=0.471, reward_mean=0.310, reward_bound=0.185, batch=216\n",
      "3481: loss=0.474, reward_mean=0.240, reward_bound=0.217, batch=221\n",
      "3482: loss=0.473, reward_mean=0.270, reward_bound=0.229, batch=219\n",
      "3483: loss=0.486, reward_mean=0.280, reward_bound=0.254, batch=210\n",
      "3484: loss=0.484, reward_mean=0.240, reward_bound=0.254, batch=215\n",
      "3485: loss=0.496, reward_mean=0.300, reward_bound=0.282, batch=213\n",
      "3486: loss=0.493, reward_mean=0.220, reward_bound=0.314, batch=205\n",
      "3487: loss=0.494, reward_mean=0.180, reward_bound=0.216, batch=213\n",
      "3488: loss=0.489, reward_mean=0.320, reward_bound=0.282, batch=218\n",
      "3489: loss=0.493, reward_mean=0.260, reward_bound=0.214, batch=222\n",
      "3490: loss=0.493, reward_mean=0.260, reward_bound=0.282, batch=224\n",
      "3491: loss=0.496, reward_mean=0.320, reward_bound=0.314, batch=220\n",
      "3492: loss=0.497, reward_mean=0.190, reward_bound=0.229, batch=223\n",
      "3493: loss=0.496, reward_mean=0.240, reward_bound=0.314, batch=224\n",
      "3494: loss=0.501, reward_mean=0.260, reward_bound=0.349, batch=212\n",
      "3495: loss=0.498, reward_mean=0.230, reward_bound=0.360, batch=218\n",
      "3496: loss=0.494, reward_mean=0.220, reward_bound=0.254, batch=221\n",
      "3497: loss=0.497, reward_mean=0.250, reward_bound=0.387, batch=202\n",
      "3498: loss=0.492, reward_mean=0.250, reward_bound=0.236, batch=211\n",
      "3499: loss=0.490, reward_mean=0.230, reward_bound=0.254, batch=216\n",
      "3500: loss=0.492, reward_mean=0.190, reward_bound=0.282, batch=219\n",
      "3501: loss=0.493, reward_mean=0.170, reward_bound=0.314, batch=220\n",
      "3502: loss=0.495, reward_mean=0.200, reward_bound=0.349, batch=217\n",
      "3503: loss=0.496, reward_mean=0.240, reward_bound=0.292, batch=222\n",
      "3504: loss=0.500, reward_mean=0.290, reward_bound=0.387, batch=220\n",
      "3505: loss=0.502, reward_mean=0.230, reward_bound=0.254, batch=223\n",
      "3506: loss=0.498, reward_mean=0.140, reward_bound=0.198, batch=226\n",
      "3507: loss=0.500, reward_mean=0.310, reward_bound=0.349, batch=226\n",
      "3508: loss=0.499, reward_mean=0.310, reward_bound=0.387, batch=227\n",
      "3509: loss=0.499, reward_mean=0.280, reward_bound=0.430, batch=197\n",
      "3510: loss=0.497, reward_mean=0.240, reward_bound=0.182, batch=208\n",
      "3511: loss=0.497, reward_mean=0.190, reward_bound=0.169, batch=215\n",
      "3512: loss=0.496, reward_mean=0.340, reward_bound=0.229, batch=219\n",
      "3513: loss=0.496, reward_mean=0.190, reward_bound=0.203, batch=223\n",
      "3514: loss=0.495, reward_mean=0.240, reward_bound=0.254, batch=223\n",
      "3515: loss=0.494, reward_mean=0.200, reward_bound=0.271, batch=226\n",
      "3516: loss=0.499, reward_mean=0.280, reward_bound=0.349, batch=217\n",
      "3517: loss=0.498, reward_mean=0.250, reward_bound=0.308, batch=222\n",
      "3518: loss=0.496, reward_mean=0.220, reward_bound=0.314, batch=224\n",
      "3519: loss=0.499, reward_mean=0.300, reward_bound=0.349, batch=224\n",
      "3520: loss=0.499, reward_mean=0.240, reward_bound=0.384, batch=227\n",
      "3521: loss=0.497, reward_mean=0.250, reward_bound=0.387, batch=222\n",
      "3522: loss=0.502, reward_mean=0.260, reward_bound=0.430, batch=214\n",
      "3523: loss=0.500, reward_mean=0.230, reward_bound=0.311, batch=220\n",
      "3524: loss=0.497, reward_mean=0.280, reward_bound=0.338, batch=224\n",
      "3525: loss=0.499, reward_mean=0.200, reward_bound=0.349, batch=225\n",
      "3526: loss=0.498, reward_mean=0.250, reward_bound=0.396, batch=227\n",
      "3527: loss=0.497, reward_mean=0.210, reward_bound=0.387, batch=228\n",
      "3528: loss=0.496, reward_mean=0.210, reward_bound=0.231, batch=229\n",
      "3529: loss=0.499, reward_mean=0.270, reward_bound=0.430, batch=223\n",
      "3530: loss=0.496, reward_mean=0.240, reward_bound=0.335, batch=226\n",
      "3531: loss=0.499, reward_mean=0.210, reward_bound=0.349, batch=226\n",
      "3532: loss=0.504, reward_mean=0.310, reward_bound=0.409, batch=228\n",
      "3533: loss=0.503, reward_mean=0.240, reward_bound=0.430, batch=228\n",
      "3534: loss=0.502, reward_mean=0.240, reward_bound=0.293, batch=229\n",
      "3535: loss=0.502, reward_mean=0.240, reward_bound=0.387, batch=229\n",
      "3536: loss=0.502, reward_mean=0.320, reward_bound=0.430, batch=229\n",
      "3537: loss=0.503, reward_mean=0.210, reward_bound=0.450, batch=230\n",
      "3538: loss=0.502, reward_mean=0.200, reward_bound=0.406, batch=231\n",
      "3539: loss=0.502, reward_mean=0.230, reward_bound=0.430, batch=231\n",
      "3540: loss=0.487, reward_mean=0.180, reward_bound=0.478, batch=181\n",
      "3541: loss=0.482, reward_mean=0.350, reward_bound=0.167, batch=196\n",
      "3542: loss=0.476, reward_mean=0.260, reward_bound=0.206, batch=206\n",
      "3543: loss=0.468, reward_mean=0.240, reward_bound=0.185, batch=213\n",
      "3544: loss=0.469, reward_mean=0.270, reward_bound=0.220, batch=219\n",
      "3545: loss=0.470, reward_mean=0.310, reward_bound=0.254, batch=217\n",
      "3546: loss=0.473, reward_mean=0.240, reward_bound=0.282, batch=217\n",
      "3547: loss=0.471, reward_mean=0.150, reward_bound=0.202, batch=222\n",
      "3548: loss=0.470, reward_mean=0.290, reward_bound=0.254, batch=223\n",
      "3549: loss=0.476, reward_mean=0.250, reward_bound=0.314, batch=225\n",
      "3550: loss=0.483, reward_mean=0.260, reward_bound=0.349, batch=210\n",
      "3551: loss=0.484, reward_mean=0.270, reward_bound=0.338, batch=217\n",
      "3552: loss=0.479, reward_mean=0.330, reward_bound=0.277, batch=222\n",
      "3553: loss=0.481, reward_mean=0.270, reward_bound=0.282, batch=223\n",
      "3554: loss=0.484, reward_mean=0.240, reward_bound=0.271, batch=226\n",
      "3555: loss=0.483, reward_mean=0.250, reward_bound=0.331, batch=228\n",
      "3556: loss=0.479, reward_mean=0.200, reward_bound=0.349, batch=224\n",
      "3557: loss=0.483, reward_mean=0.270, reward_bound=0.387, batch=214\n",
      "3558: loss=0.479, reward_mean=0.270, reward_bound=0.204, batch=220\n",
      "3559: loss=0.481, reward_mean=0.200, reward_bound=0.282, batch=223\n",
      "3560: loss=0.477, reward_mean=0.260, reward_bound=0.314, batch=225\n",
      "3561: loss=0.474, reward_mean=0.260, reward_bound=0.349, batch=225\n",
      "3562: loss=0.476, reward_mean=0.320, reward_bound=0.387, batch=226\n",
      "3563: loss=0.474, reward_mean=0.260, reward_bound=0.430, batch=210\n",
      "3564: loss=0.474, reward_mean=0.200, reward_bound=0.338, batch=217\n",
      "3565: loss=0.478, reward_mean=0.290, reward_bound=0.314, batch=221\n",
      "3566: loss=0.479, reward_mean=0.240, reward_bound=0.282, batch=224\n",
      "3567: loss=0.479, reward_mean=0.270, reward_bound=0.349, batch=226\n",
      "3568: loss=0.471, reward_mean=0.250, reward_bound=0.387, batch=223\n",
      "3569: loss=0.471, reward_mean=0.200, reward_bound=0.430, batch=220\n",
      "3570: loss=0.467, reward_mean=0.260, reward_bound=0.365, batch=224\n",
      "3571: loss=0.466, reward_mean=0.270, reward_bound=0.384, batch=227\n",
      "3572: loss=0.471, reward_mean=0.270, reward_bound=0.387, batch=226\n",
      "3573: loss=0.473, reward_mean=0.150, reward_bound=0.368, batch=228\n",
      "3574: loss=0.469, reward_mean=0.270, reward_bound=0.430, batch=224\n",
      "3575: loss=0.468, reward_mean=0.260, reward_bound=0.345, batch=227\n",
      "3576: loss=0.466, reward_mean=0.260, reward_bound=0.387, batch=227\n",
      "3577: loss=0.467, reward_mean=0.190, reward_bound=0.380, batch=229\n",
      "3578: loss=0.465, reward_mean=0.250, reward_bound=0.387, batch=229\n",
      "3579: loss=0.465, reward_mean=0.260, reward_bound=0.430, batch=229\n",
      "3580: loss=0.464, reward_mean=0.270, reward_bound=0.450, batch=230\n",
      "3581: loss=0.464, reward_mean=0.250, reward_bound=0.387, batch=230\n",
      "3582: loss=0.475, reward_mean=0.220, reward_bound=0.478, batch=203\n",
      "3583: loss=0.463, reward_mean=0.210, reward_bound=0.160, batch=212\n",
      "3584: loss=0.462, reward_mean=0.220, reward_bound=0.206, batch=219\n",
      "3585: loss=0.461, reward_mean=0.200, reward_bound=0.206, batch=222\n",
      "3586: loss=0.467, reward_mean=0.280, reward_bound=0.254, batch=224\n",
      "3587: loss=0.466, reward_mean=0.310, reward_bound=0.314, batch=222\n",
      "3588: loss=0.462, reward_mean=0.200, reward_bound=0.263, batch=225\n",
      "3589: loss=0.462, reward_mean=0.280, reward_bound=0.260, batch=227\n",
      "3590: loss=0.464, reward_mean=0.200, reward_bound=0.308, batch=229\n",
      "3591: loss=0.462, reward_mean=0.230, reward_bound=0.349, batch=223\n",
      "3592: loss=0.458, reward_mean=0.270, reward_bound=0.254, batch=225\n",
      "3593: loss=0.457, reward_mean=0.260, reward_bound=0.314, batch=226\n",
      "3594: loss=0.461, reward_mean=0.280, reward_bound=0.349, batch=227\n",
      "3595: loss=0.464, reward_mean=0.320, reward_bound=0.387, batch=224\n",
      "3596: loss=0.462, reward_mean=0.430, reward_bound=0.387, batch=226\n",
      "3597: loss=0.462, reward_mean=0.230, reward_bound=0.260, batch=228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3598: loss=0.463, reward_mean=0.140, reward_bound=0.325, batch=229\n",
      "3599: loss=0.465, reward_mean=0.280, reward_bound=0.430, batch=215\n",
      "3600: loss=0.464, reward_mean=0.380, reward_bound=0.349, batch=218\n",
      "3601: loss=0.463, reward_mean=0.280, reward_bound=0.387, batch=220\n",
      "3602: loss=0.461, reward_mean=0.260, reward_bound=0.349, batch=223\n",
      "3603: loss=0.457, reward_mean=0.270, reward_bound=0.335, batch=226\n",
      "3604: loss=0.458, reward_mean=0.310, reward_bound=0.409, batch=228\n",
      "3605: loss=0.458, reward_mean=0.290, reward_bound=0.392, batch=229\n",
      "3606: loss=0.466, reward_mean=0.330, reward_bound=0.430, batch=221\n",
      "3607: loss=0.467, reward_mean=0.240, reward_bound=0.314, batch=224\n",
      "3608: loss=0.465, reward_mean=0.270, reward_bound=0.384, batch=227\n",
      "3609: loss=0.464, reward_mean=0.200, reward_bound=0.373, batch=229\n",
      "3610: loss=0.468, reward_mean=0.200, reward_bound=0.387, batch=228\n",
      "3611: loss=0.466, reward_mean=0.210, reward_bound=0.430, batch=227\n",
      "3612: loss=0.464, reward_mean=0.160, reward_bound=0.407, batch=229\n",
      "3613: loss=0.463, reward_mean=0.330, reward_bound=0.405, batch=230\n",
      "3614: loss=0.467, reward_mean=0.260, reward_bound=0.430, batch=230\n",
      "3615: loss=0.467, reward_mean=0.230, reward_bound=0.430, batch=230\n",
      "3616: loss=0.466, reward_mean=0.230, reward_bound=0.418, batch=231\n",
      "3617: loss=0.466, reward_mean=0.240, reward_bound=0.430, batch=231\n",
      "3618: loss=0.471, reward_mean=0.240, reward_bound=0.478, batch=213\n",
      "3619: loss=0.470, reward_mean=0.250, reward_bound=0.314, batch=218\n",
      "3620: loss=0.472, reward_mean=0.170, reward_bound=0.206, batch=221\n",
      "3621: loss=0.473, reward_mean=0.210, reward_bound=0.314, batch=222\n",
      "3622: loss=0.470, reward_mean=0.300, reward_bound=0.324, batch=225\n",
      "3623: loss=0.468, reward_mean=0.250, reward_bound=0.349, batch=224\n",
      "3624: loss=0.469, reward_mean=0.260, reward_bound=0.387, batch=221\n",
      "3625: loss=0.468, reward_mean=0.220, reward_bound=0.314, batch=223\n",
      "3626: loss=0.465, reward_mean=0.260, reward_bound=0.335, batch=226\n",
      "3627: loss=0.469, reward_mean=0.290, reward_bound=0.390, batch=228\n",
      "3628: loss=0.469, reward_mean=0.240, reward_bound=0.392, batch=229\n",
      "3629: loss=0.469, reward_mean=0.200, reward_bound=0.405, batch=230\n",
      "3630: loss=0.471, reward_mean=0.220, reward_bound=0.430, batch=227\n",
      "3631: loss=0.475, reward_mean=0.190, reward_bound=0.414, batch=229\n",
      "3632: loss=0.475, reward_mean=0.250, reward_bound=0.387, batch=229\n",
      "3633: loss=0.474, reward_mean=0.210, reward_bound=0.343, batch=230\n",
      "3634: loss=0.474, reward_mean=0.210, reward_bound=0.418, batch=231\n",
      "3635: loss=0.471, reward_mean=0.270, reward_bound=0.430, batch=229\n",
      "3636: loss=0.472, reward_mean=0.200, reward_bound=0.450, batch=230\n",
      "3637: loss=0.471, reward_mean=0.230, reward_bound=0.418, batch=231\n",
      "3638: loss=0.471, reward_mean=0.140, reward_bound=0.314, batch=231\n",
      "3639: loss=0.471, reward_mean=0.260, reward_bound=0.430, batch=231\n",
      "3640: loss=0.466, reward_mean=0.190, reward_bound=0.478, batch=224\n",
      "3641: loss=0.468, reward_mean=0.200, reward_bound=0.384, batch=227\n",
      "3642: loss=0.468, reward_mean=0.230, reward_bound=0.387, batch=228\n",
      "3643: loss=0.468, reward_mean=0.270, reward_bound=0.435, batch=229\n",
      "3644: loss=0.468, reward_mean=0.210, reward_bound=0.387, batch=229\n",
      "3645: loss=0.470, reward_mean=0.270, reward_bound=0.478, batch=231\n",
      "3646: loss=0.467, reward_mean=0.200, reward_bound=0.478, batch=229\n",
      "3647: loss=0.467, reward_mean=0.200, reward_bound=0.471, batch=230\n",
      "3648: loss=0.467, reward_mean=0.210, reward_bound=0.515, batch=231\n",
      "3649: loss=0.467, reward_mean=0.280, reward_bound=0.430, batch=231\n",
      "3650: loss=0.467, reward_mean=0.340, reward_bound=0.430, batch=231\n",
      "3651: loss=0.466, reward_mean=0.240, reward_bound=0.478, batch=231\n",
      "3652: loss=0.466, reward_mean=0.290, reward_bound=0.478, batch=231\n",
      "3653: loss=0.466, reward_mean=0.280, reward_bound=0.478, batch=231\n",
      "3654: loss=0.466, reward_mean=0.270, reward_bound=0.478, batch=231\n",
      "3655: loss=0.466, reward_mean=0.190, reward_bound=0.430, batch=231\n",
      "3656: loss=0.466, reward_mean=0.230, reward_bound=0.387, batch=231\n",
      "3657: loss=0.466, reward_mean=0.290, reward_bound=0.430, batch=231\n",
      "3659: loss=0.428, reward_mean=0.290, reward_bound=0.000, batch=29\n",
      "3660: loss=0.404, reward_mean=0.250, reward_bound=0.000, batch=54\n",
      "3661: loss=0.406, reward_mean=0.230, reward_bound=0.000, batch=77\n",
      "3662: loss=0.405, reward_mean=0.250, reward_bound=0.000, batch=102\n",
      "3663: loss=0.405, reward_mean=0.290, reward_bound=0.000, batch=131\n",
      "3664: loss=0.405, reward_mean=0.290, reward_bound=0.000, batch=160\n",
      "3665: loss=0.412, reward_mean=0.230, reward_bound=0.000, batch=182\n",
      "3666: loss=0.415, reward_mean=0.320, reward_bound=0.009, batch=196\n",
      "3667: loss=0.415, reward_mean=0.250, reward_bound=0.019, batch=207\n",
      "3668: loss=0.421, reward_mean=0.270, reward_bound=0.034, batch=214\n",
      "3669: loss=0.420, reward_mean=0.330, reward_bound=0.058, batch=211\n",
      "3670: loss=0.416, reward_mean=0.270, reward_bound=0.080, batch=217\n",
      "3671: loss=0.415, reward_mean=0.270, reward_bound=0.098, batch=217\n",
      "3672: loss=0.414, reward_mean=0.280, reward_bound=0.109, batch=227\n",
      "3673: loss=0.423, reward_mean=0.290, reward_bound=0.122, batch=216\n",
      "3674: loss=0.422, reward_mean=0.260, reward_bound=0.135, batch=217\n",
      "3675: loss=0.416, reward_mean=0.370, reward_bound=0.150, batch=221\n",
      "3676: loss=0.408, reward_mean=0.310, reward_bound=0.167, batch=207\n",
      "3677: loss=0.405, reward_mean=0.250, reward_bound=0.185, batch=192\n",
      "3678: loss=0.406, reward_mean=0.250, reward_bound=0.140, batch=204\n",
      "3679: loss=0.408, reward_mean=0.350, reward_bound=0.183, batch=213\n",
      "3680: loss=0.413, reward_mean=0.360, reward_bound=0.206, batch=195\n",
      "3681: loss=0.412, reward_mean=0.280, reward_bound=0.135, batch=205\n",
      "3682: loss=0.409, reward_mean=0.300, reward_bound=0.167, batch=212\n",
      "3683: loss=0.414, reward_mean=0.300, reward_bound=0.229, batch=180\n",
      "3684: loss=0.411, reward_mean=0.330, reward_bound=0.096, batch=196\n",
      "3685: loss=0.417, reward_mean=0.350, reward_bound=0.158, batch=207\n",
      "3686: loss=0.415, reward_mean=0.340, reward_bound=0.224, batch=215\n",
      "3687: loss=0.415, reward_mean=0.240, reward_bound=0.194, batch=220\n",
      "3688: loss=0.416, reward_mean=0.290, reward_bound=0.247, batch=224\n",
      "3689: loss=0.423, reward_mean=0.370, reward_bound=0.254, batch=186\n",
      "3690: loss=0.426, reward_mean=0.300, reward_bound=0.094, batch=200\n",
      "3691: loss=0.424, reward_mean=0.310, reward_bound=0.180, batch=210\n",
      "3692: loss=0.422, reward_mean=0.280, reward_bound=0.206, batch=219\n",
      "3693: loss=0.420, reward_mean=0.280, reward_bound=0.206, batch=221\n",
      "3694: loss=0.416, reward_mean=0.310, reward_bound=0.229, batch=218\n",
      "3695: loss=0.410, reward_mean=0.290, reward_bound=0.254, batch=220\n",
      "3696: loss=0.418, reward_mean=0.360, reward_bound=0.282, batch=186\n",
      "3697: loss=0.415, reward_mean=0.340, reward_bound=0.176, batch=200\n",
      "3698: loss=0.413, reward_mean=0.270, reward_bound=0.185, batch=208\n",
      "3699: loss=0.417, reward_mean=0.230, reward_bound=0.229, batch=211\n",
      "3700: loss=0.416, reward_mean=0.300, reward_bound=0.185, batch=216\n",
      "3701: loss=0.416, reward_mean=0.330, reward_bound=0.241, batch=221\n",
      "3702: loss=0.419, reward_mean=0.300, reward_bound=0.254, batch=217\n",
      "3703: loss=0.420, reward_mean=0.350, reward_bound=0.282, batch=221\n",
      "3704: loss=0.420, reward_mean=0.330, reward_bound=0.206, batch=223\n",
      "3705: loss=0.428, reward_mean=0.290, reward_bound=0.314, batch=174\n",
      "3706: loss=0.422, reward_mean=0.340, reward_bound=0.098, batch=191\n",
      "3707: loss=0.419, reward_mean=0.320, reward_bound=0.135, batch=201\n",
      "3708: loss=0.417, reward_mean=0.330, reward_bound=0.167, batch=210\n",
      "3709: loss=0.413, reward_mean=0.280, reward_bound=0.162, batch=217\n",
      "3710: loss=0.421, reward_mean=0.340, reward_bound=0.229, batch=219\n",
      "3711: loss=0.425, reward_mean=0.300, reward_bound=0.254, batch=220\n",
      "3712: loss=0.429, reward_mean=0.250, reward_bound=0.282, batch=217\n",
      "3713: loss=0.427, reward_mean=0.320, reward_bound=0.229, batch=219\n",
      "3714: loss=0.427, reward_mean=0.260, reward_bound=0.314, batch=214\n",
      "3715: loss=0.427, reward_mean=0.310, reward_bound=0.314, batch=218\n",
      "3716: loss=0.426, reward_mean=0.260, reward_bound=0.286, batch=222\n",
      "3717: loss=0.425, reward_mean=0.320, reward_bound=0.324, batch=225\n",
      "3718: loss=0.433, reward_mean=0.220, reward_bound=0.349, batch=163\n",
      "3719: loss=0.424, reward_mean=0.310, reward_bound=0.069, batch=184\n",
      "3720: loss=0.423, reward_mean=0.240, reward_bound=0.080, batch=197\n",
      "3721: loss=0.423, reward_mean=0.270, reward_bound=0.122, batch=204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3722: loss=0.412, reward_mean=0.310, reward_bound=0.150, batch=212\n",
      "3723: loss=0.415, reward_mean=0.330, reward_bound=0.172, batch=218\n",
      "3724: loss=0.419, reward_mean=0.170, reward_bound=0.185, batch=217\n",
      "3725: loss=0.424, reward_mean=0.280, reward_bound=0.206, batch=220\n",
      "3726: loss=0.423, reward_mean=0.300, reward_bound=0.229, batch=222\n",
      "3727: loss=0.427, reward_mean=0.310, reward_bound=0.254, batch=219\n",
      "3728: loss=0.424, reward_mean=0.270, reward_bound=0.182, batch=223\n",
      "3729: loss=0.428, reward_mean=0.250, reward_bound=0.282, batch=214\n",
      "3730: loss=0.429, reward_mean=0.260, reward_bound=0.280, batch=220\n",
      "3731: loss=0.428, reward_mean=0.290, reward_bound=0.274, batch=224\n",
      "3732: loss=0.429, reward_mean=0.320, reward_bound=0.311, batch=227\n",
      "3733: loss=0.427, reward_mean=0.270, reward_bound=0.314, batch=215\n",
      "3734: loss=0.432, reward_mean=0.310, reward_bound=0.349, batch=210\n",
      "3735: loss=0.428, reward_mean=0.310, reward_bound=0.296, batch=217\n",
      "3736: loss=0.428, reward_mean=0.230, reward_bound=0.202, batch=222\n",
      "3737: loss=0.431, reward_mean=0.320, reward_bound=0.324, batch=225\n",
      "3738: loss=0.431, reward_mean=0.300, reward_bound=0.289, batch=227\n",
      "3739: loss=0.430, reward_mean=0.230, reward_bound=0.342, batch=229\n",
      "3740: loss=0.424, reward_mean=0.240, reward_bound=0.349, batch=227\n",
      "3741: loss=0.433, reward_mean=0.330, reward_bound=0.387, batch=160\n",
      "3742: loss=0.412, reward_mean=0.280, reward_bound=0.046, batch=182\n",
      "3743: loss=0.417, reward_mean=0.370, reward_bound=0.109, batch=196\n",
      "3744: loss=0.414, reward_mean=0.230, reward_bound=0.061, batch=207\n",
      "3745: loss=0.413, reward_mean=0.230, reward_bound=0.119, batch=215\n",
      "3746: loss=0.422, reward_mean=0.260, reward_bound=0.135, batch=216\n",
      "3747: loss=0.426, reward_mean=0.270, reward_bound=0.167, batch=218\n",
      "3748: loss=0.423, reward_mean=0.280, reward_bound=0.185, batch=216\n",
      "3749: loss=0.420, reward_mean=0.190, reward_bound=0.168, batch=221\n",
      "3750: loss=0.417, reward_mean=0.270, reward_bound=0.229, batch=220\n",
      "3751: loss=0.412, reward_mean=0.300, reward_bound=0.254, batch=222\n",
      "3752: loss=0.411, reward_mean=0.240, reward_bound=0.263, batch=225\n",
      "3753: loss=0.417, reward_mean=0.270, reward_bound=0.282, batch=223\n",
      "3754: loss=0.423, reward_mean=0.300, reward_bound=0.314, batch=212\n",
      "3755: loss=0.423, reward_mean=0.310, reward_bound=0.292, batch=218\n",
      "3756: loss=0.421, reward_mean=0.250, reward_bound=0.208, batch=222\n",
      "3757: loss=0.419, reward_mean=0.280, reward_bound=0.263, batch=225\n",
      "3758: loss=0.426, reward_mean=0.460, reward_bound=0.349, batch=212\n",
      "3759: loss=0.423, reward_mean=0.310, reward_bound=0.254, batch=217\n",
      "3760: loss=0.424, reward_mean=0.250, reward_bound=0.308, batch=222\n",
      "3761: loss=0.423, reward_mean=0.230, reward_bound=0.314, batch=222\n",
      "3762: loss=0.420, reward_mean=0.250, reward_bound=0.360, batch=225\n",
      "3763: loss=0.422, reward_mean=0.260, reward_bound=0.329, batch=227\n",
      "3764: loss=0.424, reward_mean=0.280, reward_bound=0.387, batch=216\n",
      "3765: loss=0.422, reward_mean=0.400, reward_bound=0.298, batch=221\n",
      "3766: loss=0.420, reward_mean=0.290, reward_bound=0.314, batch=222\n",
      "3767: loss=0.418, reward_mean=0.280, reward_bound=0.349, batch=224\n",
      "3768: loss=0.421, reward_mean=0.300, reward_bound=0.345, batch=227\n",
      "3769: loss=0.421, reward_mean=0.370, reward_bound=0.308, batch=229\n",
      "3770: loss=0.420, reward_mean=0.310, reward_bound=0.364, batch=230\n",
      "3771: loss=0.424, reward_mean=0.320, reward_bound=0.387, batch=227\n",
      "3772: loss=0.424, reward_mean=0.290, reward_bound=0.380, batch=229\n",
      "3773: loss=0.425, reward_mean=0.380, reward_bound=0.387, batch=229\n",
      "3774: loss=0.429, reward_mean=0.260, reward_bound=0.430, batch=116\n",
      "3775: loss=0.387, reward_mean=0.340, reward_bound=0.000, batch=150\n",
      "3776: loss=0.375, reward_mean=0.220, reward_bound=0.000, batch=172\n",
      "3777: loss=0.373, reward_mean=0.250, reward_bound=0.006, batch=190\n",
      "3778: loss=0.370, reward_mean=0.300, reward_bound=0.029, batch=203\n",
      "3779: loss=0.379, reward_mean=0.320, reward_bound=0.058, batch=207\n",
      "3780: loss=0.387, reward_mean=0.330, reward_bound=0.080, batch=210\n",
      "3781: loss=0.396, reward_mean=0.240, reward_bound=0.098, batch=216\n",
      "3782: loss=0.392, reward_mean=0.260, reward_bound=0.122, batch=215\n",
      "3783: loss=0.404, reward_mean=0.330, reward_bound=0.150, batch=213\n",
      "3784: loss=0.396, reward_mean=0.290, reward_bound=0.167, batch=215\n",
      "3785: loss=0.400, reward_mean=0.330, reward_bound=0.185, batch=214\n",
      "3786: loss=0.406, reward_mean=0.230, reward_bound=0.206, batch=208\n",
      "3787: loss=0.410, reward_mean=0.320, reward_bound=0.229, batch=203\n",
      "3788: loss=0.407, reward_mean=0.270, reward_bound=0.198, batch=212\n",
      "3789: loss=0.407, reward_mean=0.310, reward_bound=0.229, batch=217\n",
      "3790: loss=0.409, reward_mean=0.310, reward_bound=0.254, batch=210\n",
      "3791: loss=0.407, reward_mean=0.300, reward_bound=0.274, batch=217\n",
      "3792: loss=0.406, reward_mean=0.350, reward_bound=0.282, batch=206\n",
      "3793: loss=0.404, reward_mean=0.270, reward_bound=0.167, batch=213\n",
      "3794: loss=0.403, reward_mean=0.210, reward_bound=0.109, batch=217\n",
      "3795: loss=0.404, reward_mean=0.330, reward_bound=0.282, batch=217\n",
      "3796: loss=0.401, reward_mean=0.240, reward_bound=0.135, batch=221\n",
      "3797: loss=0.405, reward_mean=0.320, reward_bound=0.206, batch=224\n",
      "3798: loss=0.405, reward_mean=0.330, reward_bound=0.229, batch=226\n",
      "3799: loss=0.405, reward_mean=0.390, reward_bound=0.282, batch=227\n",
      "3800: loss=0.403, reward_mean=0.280, reward_bound=0.314, batch=208\n",
      "3801: loss=0.401, reward_mean=0.290, reward_bound=0.282, batch=214\n",
      "3802: loss=0.402, reward_mean=0.290, reward_bound=0.206, batch=219\n",
      "3803: loss=0.405, reward_mean=0.300, reward_bound=0.349, batch=195\n",
      "3804: loss=0.398, reward_mean=0.260, reward_bound=0.150, batch=204\n",
      "3805: loss=0.391, reward_mean=0.270, reward_bound=0.165, batch=213\n",
      "3806: loss=0.401, reward_mean=0.360, reward_bound=0.229, batch=217\n",
      "3807: loss=0.400, reward_mean=0.340, reward_bound=0.277, batch=222\n",
      "3808: loss=0.405, reward_mean=0.300, reward_bound=0.282, batch=223\n",
      "3809: loss=0.406, reward_mean=0.290, reward_bound=0.314, batch=217\n",
      "3810: loss=0.405, reward_mean=0.240, reward_bound=0.254, batch=221\n",
      "3811: loss=0.405, reward_mean=0.400, reward_bound=0.314, batch=222\n",
      "3812: loss=0.402, reward_mean=0.270, reward_bound=0.292, batch=225\n",
      "3813: loss=0.404, reward_mean=0.230, reward_bound=0.314, batch=226\n",
      "3814: loss=0.404, reward_mean=0.320, reward_bound=0.331, batch=228\n",
      "3815: loss=0.407, reward_mean=0.350, reward_bound=0.349, batch=222\n",
      "3816: loss=0.408, reward_mean=0.270, reward_bound=0.314, batch=224\n",
      "3817: loss=0.406, reward_mean=0.230, reward_bound=0.384, batch=227\n",
      "3818: loss=0.408, reward_mean=0.190, reward_bound=0.380, batch=229\n",
      "3819: loss=0.408, reward_mean=0.250, reward_bound=0.349, batch=229\n",
      "3820: loss=0.408, reward_mean=0.300, reward_bound=0.324, batch=230\n",
      "3821: loss=0.409, reward_mean=0.250, reward_bound=0.274, batch=231\n",
      "3822: loss=0.420, reward_mean=0.330, reward_bound=0.387, batch=185\n",
      "3823: loss=0.408, reward_mean=0.280, reward_bound=0.131, batch=199\n",
      "3824: loss=0.406, reward_mean=0.340, reward_bound=0.167, batch=208\n",
      "3825: loss=0.411, reward_mean=0.350, reward_bound=0.229, batch=213\n",
      "3826: loss=0.411, reward_mean=0.370, reward_bound=0.282, batch=211\n",
      "3827: loss=0.405, reward_mean=0.270, reward_bound=0.229, batch=217\n",
      "3828: loss=0.404, reward_mean=0.330, reward_bound=0.224, batch=222\n",
      "3829: loss=0.407, reward_mean=0.370, reward_bound=0.292, batch=225\n",
      "3830: loss=0.409, reward_mean=0.350, reward_bound=0.314, batch=219\n",
      "3831: loss=0.412, reward_mean=0.280, reward_bound=0.349, batch=215\n",
      "3832: loss=0.412, reward_mean=0.240, reward_bound=0.282, batch=218\n",
      "3833: loss=0.409, reward_mean=0.200, reward_bound=0.208, batch=222\n",
      "3834: loss=0.412, reward_mean=0.290, reward_bound=0.229, batch=224\n",
      "3835: loss=0.414, reward_mean=0.350, reward_bound=0.282, batch=226\n",
      "3836: loss=0.415, reward_mean=0.280, reward_bound=0.314, batch=227\n",
      "3837: loss=0.416, reward_mean=0.320, reward_bound=0.342, batch=229\n",
      "3838: loss=0.414, reward_mean=0.300, reward_bound=0.387, batch=218\n",
      "3839: loss=0.413, reward_mean=0.320, reward_bound=0.392, batch=222\n",
      "3840: loss=0.411, reward_mean=0.350, reward_bound=0.349, batch=224\n",
      "3841: loss=0.410, reward_mean=0.240, reward_bound=0.349, batch=226\n",
      "3842: loss=0.409, reward_mean=0.300, reward_bound=0.368, batch=228\n",
      "3843: loss=0.423, reward_mean=0.340, reward_bound=0.430, batch=169\n",
      "3844: loss=0.409, reward_mean=0.270, reward_bound=0.087, batch=188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3845: loss=0.405, reward_mean=0.280, reward_bound=0.100, batch=201\n",
      "3846: loss=0.407, reward_mean=0.270, reward_bound=0.122, batch=210\n",
      "3847: loss=0.419, reward_mean=0.320, reward_bound=0.185, batch=211\n",
      "3848: loss=0.417, reward_mean=0.220, reward_bound=0.206, batch=211\n",
      "3849: loss=0.422, reward_mean=0.320, reward_bound=0.229, batch=213\n",
      "3850: loss=0.420, reward_mean=0.290, reward_bound=0.254, batch=211\n",
      "3851: loss=0.417, reward_mean=0.330, reward_bound=0.254, batch=217\n",
      "3852: loss=0.416, reward_mean=0.320, reward_bound=0.229, batch=221\n",
      "3853: loss=0.416, reward_mean=0.270, reward_bound=0.282, batch=214\n",
      "3854: loss=0.414, reward_mean=0.280, reward_bound=0.226, batch=220\n",
      "3855: loss=0.413, reward_mean=0.220, reward_bound=0.282, batch=223\n",
      "3856: loss=0.415, reward_mean=0.310, reward_bound=0.314, batch=207\n",
      "3857: loss=0.415, reward_mean=0.290, reward_bound=0.206, batch=214\n",
      "3858: loss=0.414, reward_mean=0.270, reward_bound=0.252, batch=220\n",
      "3859: loss=0.414, reward_mean=0.270, reward_bound=0.247, batch=224\n",
      "3860: loss=0.413, reward_mean=0.320, reward_bound=0.314, batch=223\n",
      "3861: loss=0.414, reward_mean=0.260, reward_bound=0.349, batch=216\n",
      "3862: loss=0.413, reward_mean=0.270, reward_bound=0.241, batch=221\n",
      "3863: loss=0.415, reward_mean=0.230, reward_bound=0.254, batch=224\n",
      "3864: loss=0.413, reward_mean=0.250, reward_bound=0.345, batch=227\n",
      "3865: loss=0.414, reward_mean=0.240, reward_bound=0.342, batch=229\n",
      "3866: loss=0.411, reward_mean=0.300, reward_bound=0.387, batch=208\n",
      "3867: loss=0.411, reward_mean=0.480, reward_bound=0.349, batch=214\n",
      "3868: loss=0.411, reward_mean=0.260, reward_bound=0.254, batch=219\n",
      "3869: loss=0.414, reward_mean=0.280, reward_bound=0.314, batch=222\n",
      "3870: loss=0.414, reward_mean=0.200, reward_bound=0.272, batch=225\n",
      "3871: loss=0.411, reward_mean=0.270, reward_bound=0.321, batch=227\n",
      "3872: loss=0.409, reward_mean=0.220, reward_bound=0.277, batch=229\n",
      "3873: loss=0.416, reward_mean=0.250, reward_bound=0.364, batch=230\n",
      "3874: loss=0.415, reward_mean=0.220, reward_bound=0.387, batch=223\n",
      "3875: loss=0.418, reward_mean=0.300, reward_bound=0.301, batch=226\n",
      "3876: loss=0.416, reward_mean=0.320, reward_bound=0.409, batch=228\n",
      "3877: loss=0.427, reward_mean=0.360, reward_bound=0.430, batch=203\n",
      "3878: loss=0.426, reward_mean=0.250, reward_bound=0.160, batch=212\n",
      "3879: loss=0.420, reward_mean=0.230, reward_bound=0.229, batch=217\n",
      "3880: loss=0.414, reward_mean=0.340, reward_bound=0.254, batch=221\n",
      "3881: loss=0.420, reward_mean=0.290, reward_bound=0.282, batch=220\n",
      "3882: loss=0.418, reward_mean=0.300, reward_bound=0.304, batch=224\n",
      "3883: loss=0.425, reward_mean=0.230, reward_bound=0.314, batch=226\n",
      "3884: loss=0.425, reward_mean=0.370, reward_bound=0.349, batch=222\n",
      "3885: loss=0.427, reward_mean=0.240, reward_bound=0.229, batch=224\n",
      "3886: loss=0.425, reward_mean=0.270, reward_bound=0.345, batch=227\n",
      "3887: loss=0.428, reward_mean=0.300, reward_bound=0.349, batch=227\n",
      "3888: loss=0.427, reward_mean=0.270, reward_bound=0.387, batch=215\n",
      "3889: loss=0.430, reward_mean=0.230, reward_bound=0.289, batch=220\n",
      "3890: loss=0.431, reward_mean=0.360, reward_bound=0.304, batch=224\n",
      "3891: loss=0.430, reward_mean=0.240, reward_bound=0.308, batch=227\n",
      "3892: loss=0.424, reward_mean=0.270, reward_bound=0.314, batch=227\n",
      "3893: loss=0.427, reward_mean=0.330, reward_bound=0.349, batch=225\n",
      "3894: loss=0.427, reward_mean=0.260, reward_bound=0.387, batch=222\n",
      "3895: loss=0.425, reward_mean=0.230, reward_bound=0.430, batch=214\n",
      "3896: loss=0.420, reward_mean=0.260, reward_bound=0.277, batch=220\n",
      "3897: loss=0.422, reward_mean=0.280, reward_bound=0.338, batch=224\n",
      "3898: loss=0.422, reward_mean=0.220, reward_bound=0.345, batch=227\n",
      "3899: loss=0.423, reward_mean=0.300, reward_bound=0.349, batch=226\n",
      "3900: loss=0.423, reward_mean=0.250, reward_bound=0.314, batch=226\n",
      "3901: loss=0.424, reward_mean=0.290, reward_bound=0.368, batch=228\n",
      "3902: loss=0.426, reward_mean=0.270, reward_bound=0.392, batch=229\n",
      "3903: loss=0.428, reward_mean=0.300, reward_bound=0.430, batch=228\n",
      "3904: loss=0.428, reward_mean=0.240, reward_bound=0.478, batch=230\n",
      "3905: loss=0.428, reward_mean=0.290, reward_bound=0.464, batch=231\n",
      "3906: loss=0.428, reward_mean=0.170, reward_bound=0.387, batch=231\n",
      "3907: loss=0.423, reward_mean=0.250, reward_bound=0.478, batch=94\n",
      "3908: loss=0.423, reward_mean=0.280, reward_bound=0.000, batch=122\n",
      "3909: loss=0.389, reward_mean=0.250, reward_bound=0.000, batch=147\n",
      "3910: loss=0.379, reward_mean=0.210, reward_bound=0.000, batch=168\n",
      "3911: loss=0.380, reward_mean=0.350, reward_bound=0.034, batch=185\n",
      "3912: loss=0.385, reward_mean=0.260, reward_bound=0.042, batch=198\n",
      "3913: loss=0.390, reward_mean=0.240, reward_bound=0.065, batch=201\n",
      "3914: loss=0.391, reward_mean=0.340, reward_bound=0.089, batch=207\n",
      "3915: loss=0.394, reward_mean=0.240, reward_bound=0.109, batch=219\n",
      "3916: loss=0.385, reward_mean=0.300, reward_bound=0.122, batch=220\n",
      "3917: loss=0.388, reward_mean=0.230, reward_bound=0.135, batch=219\n",
      "3918: loss=0.383, reward_mean=0.270, reward_bound=0.150, batch=216\n",
      "3919: loss=0.383, reward_mean=0.290, reward_bound=0.167, batch=214\n",
      "3920: loss=0.387, reward_mean=0.250, reward_bound=0.185, batch=210\n",
      "3921: loss=0.389, reward_mean=0.350, reward_bound=0.206, batch=221\n",
      "3922: loss=0.395, reward_mean=0.290, reward_bound=0.206, batch=212\n",
      "3923: loss=0.395, reward_mean=0.280, reward_bound=0.213, batch=218\n",
      "3924: loss=0.394, reward_mean=0.400, reward_bound=0.229, batch=212\n",
      "3925: loss=0.396, reward_mean=0.330, reward_bound=0.236, batch=218\n",
      "3926: loss=0.394, reward_mean=0.270, reward_bound=0.192, batch=222\n",
      "3927: loss=0.396, reward_mean=0.280, reward_bound=0.254, batch=214\n",
      "3928: loss=0.394, reward_mean=0.280, reward_bound=0.185, batch=219\n",
      "3929: loss=0.396, reward_mean=0.230, reward_bound=0.254, batch=222\n",
      "3930: loss=0.411, reward_mean=0.310, reward_bound=0.282, batch=200\n",
      "3931: loss=0.413, reward_mean=0.370, reward_bound=0.200, batch=210\n",
      "3932: loss=0.416, reward_mean=0.430, reward_bound=0.282, batch=216\n",
      "3933: loss=0.417, reward_mean=0.320, reward_bound=0.254, batch=220\n",
      "3934: loss=0.417, reward_mean=0.290, reward_bound=0.314, batch=188\n",
      "3935: loss=0.413, reward_mean=0.270, reward_bound=0.089, batch=200\n",
      "3936: loss=0.409, reward_mean=0.240, reward_bound=0.106, batch=210\n",
      "3937: loss=0.413, reward_mean=0.280, reward_bound=0.135, batch=216\n",
      "3938: loss=0.417, reward_mean=0.280, reward_bound=0.185, batch=217\n",
      "3939: loss=0.419, reward_mean=0.360, reward_bound=0.254, batch=217\n",
      "3940: loss=0.416, reward_mean=0.330, reward_bound=0.202, batch=222\n",
      "3941: loss=0.419, reward_mean=0.300, reward_bound=0.236, batch=225\n",
      "3942: loss=0.420, reward_mean=0.310, reward_bound=0.282, batch=222\n",
      "3943: loss=0.423, reward_mean=0.300, reward_bound=0.314, batch=218\n",
      "3944: loss=0.425, reward_mean=0.210, reward_bound=0.282, batch=221\n",
      "3945: loss=0.431, reward_mean=0.300, reward_bound=0.349, batch=194\n",
      "3946: loss=0.431, reward_mean=0.300, reward_bound=0.135, batch=204\n",
      "3947: loss=0.433, reward_mean=0.270, reward_bound=0.150, batch=212\n",
      "3948: loss=0.428, reward_mean=0.270, reward_bound=0.206, batch=219\n",
      "3949: loss=0.416, reward_mean=0.350, reward_bound=0.239, batch=223\n",
      "3950: loss=0.423, reward_mean=0.290, reward_bound=0.282, batch=224\n",
      "3951: loss=0.423, reward_mean=0.290, reward_bound=0.311, batch=227\n",
      "3952: loss=0.424, reward_mean=0.320, reward_bound=0.314, batch=226\n",
      "3953: loss=0.426, reward_mean=0.300, reward_bound=0.349, batch=225\n",
      "3954: loss=0.438, reward_mean=0.320, reward_bound=0.387, batch=189\n",
      "3955: loss=0.430, reward_mean=0.220, reward_bound=0.150, batch=201\n",
      "3956: loss=0.442, reward_mean=0.340, reward_bound=0.229, batch=208\n",
      "3957: loss=0.439, reward_mean=0.130, reward_bound=0.080, batch=214\n",
      "3958: loss=0.455, reward_mean=0.250, reward_bound=0.229, batch=217\n",
      "3959: loss=0.443, reward_mean=0.280, reward_bound=0.282, batch=217\n",
      "3960: loss=0.442, reward_mean=0.300, reward_bound=0.314, batch=217\n",
      "3961: loss=0.451, reward_mean=0.250, reward_bound=0.282, batch=221\n",
      "3962: loss=0.450, reward_mean=0.240, reward_bound=0.229, batch=224\n",
      "3963: loss=0.447, reward_mean=0.210, reward_bound=0.314, batch=226\n",
      "3964: loss=0.448, reward_mean=0.240, reward_bound=0.331, batch=228\n",
      "3965: loss=0.448, reward_mean=0.330, reward_bound=0.349, batch=223\n",
      "3966: loss=0.439, reward_mean=0.350, reward_bound=0.387, batch=212\n",
      "3967: loss=0.434, reward_mean=0.270, reward_bound=0.191, batch=218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3968: loss=0.438, reward_mean=0.230, reward_bound=0.206, batch=221\n",
      "3969: loss=0.433, reward_mean=0.260, reward_bound=0.282, batch=223\n",
      "3970: loss=0.431, reward_mean=0.300, reward_bound=0.282, batch=225\n",
      "3971: loss=0.435, reward_mean=0.320, reward_bound=0.321, batch=227\n",
      "3972: loss=0.437, reward_mean=0.230, reward_bound=0.349, batch=227\n",
      "3973: loss=0.436, reward_mean=0.330, reward_bound=0.380, batch=229\n",
      "3974: loss=0.436, reward_mean=0.270, reward_bound=0.387, batch=226\n",
      "3975: loss=0.446, reward_mean=0.280, reward_bound=0.430, batch=156\n",
      "3976: loss=0.443, reward_mean=0.330, reward_bound=0.048, batch=179\n",
      "3977: loss=0.435, reward_mean=0.290, reward_bound=0.055, batch=195\n",
      "3978: loss=0.426, reward_mean=0.290, reward_bound=0.101, batch=206\n",
      "3979: loss=0.420, reward_mean=0.340, reward_bound=0.135, batch=212\n",
      "3980: loss=0.423, reward_mean=0.260, reward_bound=0.167, batch=211\n",
      "3981: loss=0.431, reward_mean=0.340, reward_bound=0.206, batch=210\n",
      "3982: loss=0.426, reward_mean=0.240, reward_bound=0.229, batch=216\n",
      "3983: loss=0.425, reward_mean=0.270, reward_bound=0.254, batch=211\n",
      "3984: loss=0.439, reward_mean=0.250, reward_bound=0.282, batch=202\n",
      "3985: loss=0.430, reward_mean=0.290, reward_bound=0.130, batch=211\n",
      "3986: loss=0.435, reward_mean=0.240, reward_bound=0.185, batch=216\n",
      "3987: loss=0.436, reward_mean=0.300, reward_bound=0.229, batch=220\n",
      "3988: loss=0.440, reward_mean=0.250, reward_bound=0.274, batch=224\n",
      "3989: loss=0.439, reward_mean=0.230, reward_bound=0.282, batch=223\n",
      "3990: loss=0.445, reward_mean=0.220, reward_bound=0.314, batch=211\n",
      "3991: loss=0.446, reward_mean=0.300, reward_bound=0.349, batch=201\n",
      "3992: loss=0.443, reward_mean=0.290, reward_bound=0.229, batch=210\n",
      "3993: loss=0.449, reward_mean=0.280, reward_bound=0.254, batch=213\n",
      "3994: loss=0.447, reward_mean=0.280, reward_bound=0.282, batch=218\n",
      "3995: loss=0.446, reward_mean=0.240, reward_bound=0.286, batch=222\n",
      "3996: loss=0.446, reward_mean=0.220, reward_bound=0.272, batch=225\n",
      "3997: loss=0.447, reward_mean=0.300, reward_bound=0.314, batch=220\n",
      "3998: loss=0.448, reward_mean=0.250, reward_bound=0.288, batch=224\n",
      "3999: loss=0.447, reward_mean=0.170, reward_bound=0.282, batch=226\n",
      "4000: loss=0.446, reward_mean=0.240, reward_bound=0.314, batch=227\n",
      "4001: loss=0.442, reward_mean=0.230, reward_bound=0.349, batch=218\n",
      "4002: loss=0.442, reward_mean=0.190, reward_bound=0.257, batch=222\n",
      "4003: loss=0.443, reward_mean=0.320, reward_bound=0.314, batch=224\n",
      "4004: loss=0.439, reward_mean=0.220, reward_bound=0.311, batch=227\n",
      "4005: loss=0.439, reward_mean=0.250, reward_bound=0.342, batch=229\n",
      "4006: loss=0.441, reward_mean=0.190, reward_bound=0.349, batch=229\n",
      "4007: loss=0.447, reward_mean=0.220, reward_bound=0.387, batch=204\n",
      "4008: loss=0.450, reward_mean=0.230, reward_bound=0.185, batch=212\n",
      "4009: loss=0.454, reward_mean=0.320, reward_bound=0.292, batch=218\n",
      "4010: loss=0.449, reward_mean=0.280, reward_bound=0.314, batch=218\n",
      "4011: loss=0.450, reward_mean=0.290, reward_bound=0.349, batch=215\n",
      "4012: loss=0.445, reward_mean=0.270, reward_bound=0.289, batch=220\n",
      "4013: loss=0.445, reward_mean=0.310, reward_bound=0.349, batch=223\n",
      "4014: loss=0.446, reward_mean=0.220, reward_bound=0.387, batch=220\n",
      "4015: loss=0.450, reward_mean=0.220, reward_bound=0.406, batch=224\n",
      "4016: loss=0.451, reward_mean=0.310, reward_bound=0.384, batch=227\n",
      "4017: loss=0.453, reward_mean=0.260, reward_bound=0.387, batch=227\n",
      "4018: loss=0.453, reward_mean=0.240, reward_bound=0.430, batch=202\n",
      "4019: loss=0.450, reward_mean=0.280, reward_bound=0.185, batch=210\n",
      "4020: loss=0.451, reward_mean=0.200, reward_bound=0.167, batch=216\n",
      "4021: loss=0.449, reward_mean=0.210, reward_bound=0.217, batch=221\n",
      "4022: loss=0.449, reward_mean=0.280, reward_bound=0.254, batch=223\n",
      "4023: loss=0.445, reward_mean=0.230, reward_bound=0.244, batch=226\n",
      "4024: loss=0.461, reward_mean=0.230, reward_bound=0.282, batch=226\n",
      "4025: loss=0.453, reward_mean=0.310, reward_bound=0.349, batch=224\n",
      "4026: loss=0.454, reward_mean=0.210, reward_bound=0.337, batch=227\n",
      "4027: loss=0.455, reward_mean=0.260, reward_bound=0.349, batch=228\n",
      "4028: loss=0.454, reward_mean=0.300, reward_bound=0.387, batch=219\n",
      "4029: loss=0.453, reward_mean=0.240, reward_bound=0.250, batch=223\n",
      "4030: loss=0.454, reward_mean=0.280, reward_bound=0.349, batch=225\n",
      "4031: loss=0.455, reward_mean=0.270, reward_bound=0.387, batch=225\n",
      "4032: loss=0.452, reward_mean=0.210, reward_bound=0.430, batch=211\n",
      "4033: loss=0.448, reward_mean=0.290, reward_bound=0.254, batch=217\n",
      "4034: loss=0.442, reward_mean=0.250, reward_bound=0.224, batch=222\n",
      "4035: loss=0.444, reward_mean=0.240, reward_bound=0.292, batch=225\n",
      "4036: loss=0.449, reward_mean=0.250, reward_bound=0.349, batch=224\n",
      "4037: loss=0.448, reward_mean=0.310, reward_bound=0.380, batch=227\n",
      "4038: loss=0.448, reward_mean=0.260, reward_bound=0.380, batch=229\n",
      "4039: loss=0.449, reward_mean=0.310, reward_bound=0.364, batch=230\n",
      "4040: loss=0.449, reward_mean=0.310, reward_bound=0.430, batch=222\n",
      "4041: loss=0.450, reward_mean=0.320, reward_bound=0.400, batch=225\n",
      "4042: loss=0.448, reward_mean=0.300, reward_bound=0.321, batch=227\n",
      "4043: loss=0.450, reward_mean=0.280, reward_bound=0.349, batch=227\n",
      "4044: loss=0.452, reward_mean=0.250, reward_bound=0.422, batch=229\n",
      "4045: loss=0.455, reward_mean=0.340, reward_bound=0.430, batch=227\n",
      "4046: loss=0.453, reward_mean=0.270, reward_bound=0.380, batch=229\n",
      "4047: loss=0.454, reward_mean=0.300, reward_bound=0.387, batch=229\n",
      "4048: loss=0.454, reward_mean=0.280, reward_bound=0.430, batch=229\n",
      "4049: loss=0.452, reward_mean=0.290, reward_bound=0.424, batch=230\n",
      "4050: loss=0.447, reward_mean=0.250, reward_bound=0.478, batch=147\n",
      "4051: loss=0.432, reward_mean=0.310, reward_bound=0.027, batch=173\n",
      "4052: loss=0.429, reward_mean=0.330, reward_bound=0.085, batch=191\n",
      "4053: loss=0.425, reward_mean=0.280, reward_bound=0.098, batch=203\n",
      "4054: loss=0.421, reward_mean=0.330, reward_bound=0.144, batch=212\n",
      "4055: loss=0.422, reward_mean=0.380, reward_bound=0.185, batch=215\n",
      "4056: loss=0.434, reward_mean=0.320, reward_bound=0.229, batch=209\n",
      "4057: loss=0.434, reward_mean=0.270, reward_bound=0.239, batch=216\n",
      "4058: loss=0.433, reward_mean=0.270, reward_bound=0.254, batch=213\n",
      "4059: loss=0.436, reward_mean=0.270, reward_bound=0.198, batch=219\n",
      "4060: loss=0.435, reward_mean=0.310, reward_bound=0.265, batch=223\n",
      "4061: loss=0.440, reward_mean=0.310, reward_bound=0.282, batch=215\n",
      "4062: loss=0.438, reward_mean=0.240, reward_bound=0.282, batch=219\n",
      "4063: loss=0.431, reward_mean=0.260, reward_bound=0.314, batch=205\n",
      "4064: loss=0.430, reward_mean=0.290, reward_bound=0.314, batch=208\n",
      "4065: loss=0.427, reward_mean=0.330, reward_bound=0.282, batch=214\n",
      "4066: loss=0.429, reward_mean=0.220, reward_bound=0.200, batch=220\n",
      "4067: loss=0.432, reward_mean=0.250, reward_bound=0.314, batch=220\n",
      "4068: loss=0.430, reward_mean=0.270, reward_bound=0.304, batch=224\n",
      "4069: loss=0.429, reward_mean=0.250, reward_bound=0.314, batch=225\n",
      "4070: loss=0.431, reward_mean=0.270, reward_bound=0.349, batch=197\n",
      "4071: loss=0.418, reward_mean=0.280, reward_bound=0.119, batch=208\n",
      "4072: loss=0.419, reward_mean=0.190, reward_bound=0.208, batch=215\n",
      "4073: loss=0.420, reward_mean=0.340, reward_bound=0.254, batch=218\n",
      "4074: loss=0.420, reward_mean=0.290, reward_bound=0.314, batch=214\n",
      "4075: loss=0.422, reward_mean=0.270, reward_bound=0.183, batch=220\n",
      "4076: loss=0.424, reward_mean=0.290, reward_bound=0.304, batch=224\n",
      "4077: loss=0.418, reward_mean=0.340, reward_bound=0.345, batch=227\n",
      "4078: loss=0.417, reward_mean=0.280, reward_bound=0.342, batch=229\n",
      "4079: loss=0.423, reward_mean=0.300, reward_bound=0.349, batch=227\n",
      "4080: loss=0.421, reward_mean=0.380, reward_bound=0.387, batch=209\n",
      "4081: loss=0.421, reward_mean=0.220, reward_bound=0.229, batch=214\n",
      "4082: loss=0.419, reward_mean=0.370, reward_bound=0.280, batch=220\n",
      "4083: loss=0.422, reward_mean=0.270, reward_bound=0.274, batch=224\n",
      "4084: loss=0.424, reward_mean=0.250, reward_bound=0.282, batch=226\n",
      "4085: loss=0.428, reward_mean=0.300, reward_bound=0.331, batch=228\n",
      "4086: loss=0.428, reward_mean=0.260, reward_bound=0.349, batch=225\n",
      "4087: loss=0.427, reward_mean=0.370, reward_bound=0.387, batch=221\n",
      "4088: loss=0.425, reward_mean=0.250, reward_bound=0.349, batch=224\n",
      "4089: loss=0.424, reward_mean=0.310, reward_bound=0.349, batch=226\n",
      "4090: loss=0.426, reward_mean=0.240, reward_bound=0.230, batch=228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4091: loss=0.425, reward_mean=0.240, reward_bound=0.317, batch=229\n",
      "4092: loss=0.423, reward_mean=0.240, reward_bound=0.349, batch=229\n",
      "4093: loss=0.424, reward_mean=0.290, reward_bound=0.343, batch=230\n",
      "4094: loss=0.423, reward_mean=0.280, reward_bound=0.365, batch=231\n",
      "4095: loss=0.425, reward_mean=0.330, reward_bound=0.387, batch=227\n",
      "4096: loss=0.425, reward_mean=0.310, reward_bound=0.387, batch=228\n",
      "4097: loss=0.432, reward_mean=0.350, reward_bound=0.430, batch=187\n",
      "4098: loss=0.413, reward_mean=0.240, reward_bound=0.052, batch=200\n",
      "4099: loss=0.412, reward_mean=0.230, reward_bound=0.086, batch=210\n",
      "4100: loss=0.422, reward_mean=0.270, reward_bound=0.150, batch=216\n",
      "4101: loss=0.419, reward_mean=0.200, reward_bound=0.167, batch=220\n",
      "4102: loss=0.431, reward_mean=0.320, reward_bound=0.229, batch=219\n",
      "4103: loss=0.430, reward_mean=0.340, reward_bound=0.265, batch=223\n",
      "4104: loss=0.431, reward_mean=0.310, reward_bound=0.282, batch=220\n",
      "4105: loss=0.429, reward_mean=0.270, reward_bound=0.314, batch=215\n",
      "4106: loss=0.427, reward_mean=0.280, reward_bound=0.282, batch=219\n",
      "4107: loss=0.426, reward_mean=0.310, reward_bound=0.250, batch=223\n",
      "4108: loss=0.431, reward_mean=0.310, reward_bound=0.301, batch=226\n",
      "4109: loss=0.429, reward_mean=0.220, reward_bound=0.349, batch=225\n",
      "4110: loss=0.428, reward_mean=0.250, reward_bound=0.356, batch=227\n",
      "4111: loss=0.427, reward_mean=0.240, reward_bound=0.387, batch=215\n",
      "4112: loss=0.426, reward_mean=0.290, reward_bound=0.314, batch=219\n",
      "4113: loss=0.430, reward_mean=0.270, reward_bound=0.229, batch=222\n",
      "4114: loss=0.430, reward_mean=0.270, reward_bound=0.292, batch=225\n",
      "4115: loss=0.431, reward_mean=0.210, reward_bound=0.321, batch=227\n",
      "4116: loss=0.431, reward_mean=0.290, reward_bound=0.387, batch=224\n",
      "4117: loss=0.431, reward_mean=0.260, reward_bound=0.314, batch=226\n",
      "4118: loss=0.433, reward_mean=0.270, reward_bound=0.349, batch=227\n",
      "4119: loss=0.433, reward_mean=0.230, reward_bound=0.366, batch=229\n",
      "4120: loss=0.433, reward_mean=0.260, reward_bound=0.364, batch=230\n",
      "4121: loss=0.431, reward_mean=0.250, reward_bound=0.304, batch=231\n",
      "4122: loss=0.434, reward_mean=0.250, reward_bound=0.430, batch=209\n",
      "4123: loss=0.434, reward_mean=0.290, reward_bound=0.295, batch=216\n",
      "4124: loss=0.434, reward_mean=0.280, reward_bound=0.282, batch=219\n",
      "4125: loss=0.431, reward_mean=0.230, reward_bound=0.265, batch=223\n",
      "4126: loss=0.435, reward_mean=0.290, reward_bound=0.314, batch=222\n",
      "4127: loss=0.436, reward_mean=0.310, reward_bound=0.349, batch=220\n",
      "4128: loss=0.437, reward_mean=0.210, reward_bound=0.304, batch=224\n",
      "4129: loss=0.435, reward_mean=0.270, reward_bound=0.314, batch=224\n",
      "4130: loss=0.433, reward_mean=0.290, reward_bound=0.345, batch=227\n",
      "4131: loss=0.434, reward_mean=0.360, reward_bound=0.349, batch=226\n",
      "4132: loss=0.434, reward_mean=0.210, reward_bound=0.298, batch=228\n",
      "4133: loss=0.434, reward_mean=0.310, reward_bound=0.317, batch=229\n",
      "4134: loss=0.433, reward_mean=0.260, reward_bound=0.364, batch=230\n",
      "4135: loss=0.435, reward_mean=0.220, reward_bound=0.387, batch=221\n",
      "4136: loss=0.436, reward_mean=0.360, reward_bound=0.387, batch=223\n",
      "4137: loss=0.433, reward_mean=0.190, reward_bound=0.324, batch=226\n",
      "4138: loss=0.434, reward_mean=0.340, reward_bound=0.387, batch=226\n",
      "4139: loss=0.434, reward_mean=0.240, reward_bound=0.349, batch=227\n",
      "4140: loss=0.433, reward_mean=0.370, reward_bound=0.342, batch=229\n",
      "4141: loss=0.433, reward_mean=0.240, reward_bound=0.405, batch=230\n",
      "4142: loss=0.433, reward_mean=0.250, reward_bound=0.418, batch=231\n",
      "4143: loss=0.438, reward_mean=0.250, reward_bound=0.430, batch=223\n",
      "4144: loss=0.439, reward_mean=0.240, reward_bound=0.398, batch=226\n",
      "4145: loss=0.437, reward_mean=0.270, reward_bound=0.331, batch=228\n",
      "4146: loss=0.439, reward_mean=0.330, reward_bound=0.392, batch=229\n",
      "4147: loss=0.438, reward_mean=0.300, reward_bound=0.381, batch=230\n",
      "4148: loss=0.438, reward_mean=0.270, reward_bound=0.430, batch=227\n",
      "4149: loss=0.438, reward_mean=0.260, reward_bound=0.380, batch=229\n",
      "4150: loss=0.438, reward_mean=0.200, reward_bound=0.430, batch=228\n",
      "4151: loss=0.439, reward_mean=0.310, reward_bound=0.435, batch=229\n",
      "4152: loss=0.440, reward_mean=0.260, reward_bound=0.478, batch=232\n",
      "4153: loss=0.435, reward_mean=0.330, reward_bound=0.478, batch=180\n",
      "4154: loss=0.421, reward_mean=0.160, reward_bound=0.003, batch=196\n",
      "4155: loss=0.434, reward_mean=0.240, reward_bound=0.089, batch=206\n",
      "4156: loss=0.428, reward_mean=0.240, reward_bound=0.158, batch=214\n",
      "4157: loss=0.431, reward_mean=0.230, reward_bound=0.183, batch=220\n",
      "4158: loss=0.438, reward_mean=0.230, reward_bound=0.185, batch=221\n",
      "4159: loss=0.439, reward_mean=0.300, reward_bound=0.229, batch=223\n",
      "4160: loss=0.448, reward_mean=0.220, reward_bound=0.254, batch=222\n",
      "4161: loss=0.450, reward_mean=0.210, reward_bound=0.282, batch=221\n",
      "4162: loss=0.445, reward_mean=0.340, reward_bound=0.314, batch=220\n",
      "4163: loss=0.442, reward_mean=0.240, reward_bound=0.296, batch=224\n",
      "4164: loss=0.444, reward_mean=0.290, reward_bound=0.314, batch=225\n",
      "4165: loss=0.440, reward_mean=0.270, reward_bound=0.349, batch=217\n",
      "4166: loss=0.435, reward_mean=0.190, reward_bound=0.245, batch=222\n",
      "4167: loss=0.434, reward_mean=0.260, reward_bound=0.236, batch=225\n",
      "4168: loss=0.437, reward_mean=0.250, reward_bound=0.296, batch=227\n",
      "4169: loss=0.437, reward_mean=0.290, reward_bound=0.387, batch=215\n",
      "4170: loss=0.434, reward_mean=0.280, reward_bound=0.396, batch=220\n",
      "4171: loss=0.434, reward_mean=0.290, reward_bound=0.406, batch=224\n",
      "4172: loss=0.434, reward_mean=0.240, reward_bound=0.349, batch=226\n",
      "4173: loss=0.433, reward_mean=0.250, reward_bound=0.351, batch=228\n",
      "4174: loss=0.434, reward_mean=0.310, reward_bound=0.387, batch=227\n",
      "4175: loss=0.434, reward_mean=0.270, reward_bound=0.373, batch=229\n",
      "4176: loss=0.433, reward_mean=0.320, reward_bound=0.343, batch=230\n",
      "4177: loss=0.440, reward_mean=0.230, reward_bound=0.430, batch=210\n",
      "4178: loss=0.438, reward_mean=0.240, reward_bound=0.274, batch=217\n",
      "4179: loss=0.438, reward_mean=0.270, reward_bound=0.308, batch=222\n",
      "4180: loss=0.441, reward_mean=0.310, reward_bound=0.314, batch=224\n",
      "4181: loss=0.443, reward_mean=0.300, reward_bound=0.349, batch=223\n",
      "4182: loss=0.442, reward_mean=0.210, reward_bound=0.372, batch=226\n",
      "4183: loss=0.444, reward_mean=0.320, reward_bound=0.387, batch=221\n",
      "4184: loss=0.444, reward_mean=0.270, reward_bound=0.349, batch=221\n",
      "4185: loss=0.441, reward_mean=0.290, reward_bound=0.314, batch=223\n",
      "4186: loss=0.447, reward_mean=0.260, reward_bound=0.335, batch=226\n",
      "4187: loss=0.444, reward_mean=0.250, reward_bound=0.244, batch=228\n",
      "4188: loss=0.447, reward_mean=0.240, reward_bound=0.317, batch=229\n",
      "4189: loss=0.444, reward_mean=0.260, reward_bound=0.349, batch=225\n",
      "4190: loss=0.444, reward_mean=0.310, reward_bound=0.387, batch=225\n",
      "4191: loss=0.444, reward_mean=0.300, reward_bound=0.396, batch=227\n",
      "4192: loss=0.443, reward_mean=0.230, reward_bound=0.422, batch=229\n",
      "4193: loss=0.443, reward_mean=0.250, reward_bound=0.430, batch=220\n",
      "4194: loss=0.443, reward_mean=0.200, reward_bound=0.314, batch=222\n",
      "4195: loss=0.442, reward_mean=0.330, reward_bound=0.349, batch=223\n",
      "4196: loss=0.443, reward_mean=0.180, reward_bound=0.334, batch=226\n",
      "4197: loss=0.441, reward_mean=0.290, reward_bound=0.387, batch=226\n",
      "4198: loss=0.440, reward_mean=0.210, reward_bound=0.409, batch=228\n",
      "4199: loss=0.440, reward_mean=0.250, reward_bound=0.430, batch=224\n",
      "4200: loss=0.438, reward_mean=0.250, reward_bound=0.311, batch=227\n",
      "4201: loss=0.439, reward_mean=0.380, reward_bound=0.387, batch=227\n",
      "4202: loss=0.437, reward_mean=0.300, reward_bound=0.422, batch=229\n",
      "4203: loss=0.439, reward_mean=0.360, reward_bound=0.430, batch=226\n",
      "4204: loss=0.438, reward_mean=0.200, reward_bound=0.368, batch=228\n",
      "4205: loss=0.438, reward_mean=0.370, reward_bound=0.321, batch=229\n",
      "4206: loss=0.438, reward_mean=0.260, reward_bound=0.364, batch=230\n",
      "4207: loss=0.439, reward_mean=0.320, reward_bound=0.430, batch=230\n",
      "4208: loss=0.438, reward_mean=0.230, reward_bound=0.418, batch=231\n",
      "4209: loss=0.438, reward_mean=0.200, reward_bound=0.387, batch=231\n",
      "4210: loss=0.439, reward_mean=0.260, reward_bound=0.430, batch=230\n",
      "4211: loss=0.439, reward_mean=0.270, reward_bound=0.387, batch=230\n",
      "4212: loss=0.439, reward_mean=0.210, reward_bound=0.451, batch=231\n",
      "4213: loss=0.439, reward_mean=0.230, reward_bound=0.430, batch=231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4214: loss=0.434, reward_mean=0.280, reward_bound=0.478, batch=199\n",
      "4215: loss=0.438, reward_mean=0.300, reward_bound=0.229, batch=208\n",
      "4216: loss=0.437, reward_mean=0.270, reward_bound=0.254, batch=214\n",
      "4217: loss=0.441, reward_mean=0.260, reward_bound=0.282, batch=218\n",
      "4218: loss=0.438, reward_mean=0.250, reward_bound=0.286, batch=222\n",
      "4219: loss=0.436, reward_mean=0.190, reward_bound=0.314, batch=224\n",
      "4220: loss=0.437, reward_mean=0.280, reward_bound=0.345, batch=227\n",
      "4221: loss=0.438, reward_mean=0.270, reward_bound=0.282, batch=228\n",
      "4222: loss=0.444, reward_mean=0.290, reward_bound=0.349, batch=220\n",
      "4223: loss=0.444, reward_mean=0.270, reward_bound=0.274, batch=224\n",
      "4224: loss=0.439, reward_mean=0.230, reward_bound=0.345, batch=227\n",
      "4225: loss=0.437, reward_mean=0.240, reward_bound=0.349, batch=227\n",
      "4226: loss=0.437, reward_mean=0.180, reward_bound=0.387, batch=221\n",
      "4227: loss=0.435, reward_mean=0.290, reward_bound=0.314, batch=224\n",
      "4228: loss=0.435, reward_mean=0.270, reward_bound=0.387, batch=223\n",
      "4229: loss=0.435, reward_mean=0.280, reward_bound=0.387, batch=225\n",
      "4230: loss=0.434, reward_mean=0.250, reward_bound=0.430, batch=213\n",
      "4231: loss=0.432, reward_mean=0.280, reward_bound=0.314, batch=218\n",
      "4232: loss=0.434, reward_mean=0.260, reward_bound=0.257, batch=222\n",
      "4233: loss=0.434, reward_mean=0.210, reward_bound=0.263, batch=225\n",
      "4234: loss=0.432, reward_mean=0.300, reward_bound=0.349, batch=223\n",
      "4235: loss=0.432, reward_mean=0.260, reward_bound=0.372, batch=226\n",
      "4236: loss=0.434, reward_mean=0.220, reward_bound=0.335, batch=228\n",
      "4237: loss=0.433, reward_mean=0.260, reward_bound=0.387, batch=225\n",
      "4238: loss=0.438, reward_mean=0.190, reward_bound=0.430, batch=223\n",
      "4239: loss=0.435, reward_mean=0.210, reward_bound=0.398, batch=226\n",
      "4240: loss=0.435, reward_mean=0.230, reward_bound=0.409, batch=228\n",
      "4241: loss=0.435, reward_mean=0.200, reward_bound=0.349, batch=228\n",
      "4242: loss=0.435, reward_mean=0.300, reward_bound=0.387, batch=228\n",
      "4243: loss=0.436, reward_mean=0.200, reward_bound=0.430, batch=228\n",
      "4244: loss=0.435, reward_mean=0.360, reward_bound=0.435, batch=229\n",
      "4245: loss=0.436, reward_mean=0.230, reward_bound=0.405, batch=230\n",
      "4246: loss=0.436, reward_mean=0.310, reward_bound=0.464, batch=231\n",
      "4247: loss=0.430, reward_mean=0.260, reward_bound=0.478, batch=211\n",
      "4248: loss=0.427, reward_mean=0.260, reward_bound=0.229, batch=216\n",
      "4249: loss=0.432, reward_mean=0.280, reward_bound=0.282, batch=218\n",
      "4250: loss=0.428, reward_mean=0.290, reward_bound=0.314, batch=220\n",
      "4251: loss=0.426, reward_mean=0.350, reward_bound=0.349, batch=220\n",
      "4252: loss=0.422, reward_mean=0.270, reward_bound=0.282, batch=223\n",
      "4253: loss=0.426, reward_mean=0.300, reward_bound=0.314, batch=225\n",
      "4254: loss=0.429, reward_mean=0.250, reward_bound=0.387, batch=223\n",
      "4255: loss=0.430, reward_mean=0.330, reward_bound=0.387, batch=224\n",
      "4256: loss=0.430, reward_mean=0.280, reward_bound=0.349, batch=226\n",
      "4257: loss=0.430, reward_mean=0.270, reward_bound=0.331, batch=228\n",
      "4258: loss=0.428, reward_mean=0.300, reward_bound=0.349, batch=228\n",
      "4259: loss=0.427, reward_mean=0.260, reward_bound=0.392, batch=229\n",
      "4260: loss=0.427, reward_mean=0.290, reward_bound=0.387, batch=229\n",
      "4261: loss=0.427, reward_mean=0.360, reward_bound=0.430, batch=221\n",
      "4262: loss=0.425, reward_mean=0.320, reward_bound=0.349, batch=224\n",
      "4263: loss=0.430, reward_mean=0.240, reward_bound=0.387, batch=224\n",
      "4264: loss=0.430, reward_mean=0.320, reward_bound=0.377, batch=227\n",
      "4265: loss=0.431, reward_mean=0.220, reward_bound=0.237, batch=229\n",
      "4266: loss=0.427, reward_mean=0.270, reward_bound=0.349, batch=229\n",
      "4267: loss=0.427, reward_mean=0.250, reward_bound=0.387, batch=229\n",
      "4268: loss=0.426, reward_mean=0.320, reward_bound=0.343, batch=230\n",
      "4269: loss=0.427, reward_mean=0.250, reward_bound=0.365, batch=231\n",
      "4270: loss=0.428, reward_mean=0.280, reward_bound=0.430, batch=227\n",
      "4271: loss=0.427, reward_mean=0.310, reward_bound=0.422, batch=229\n",
      "4272: loss=0.427, reward_mean=0.180, reward_bound=0.325, batch=230\n",
      "4273: loss=0.425, reward_mean=0.240, reward_bound=0.376, batch=231\n",
      "4274: loss=0.427, reward_mean=0.310, reward_bound=0.430, batch=230\n",
      "4275: loss=0.427, reward_mean=0.270, reward_bound=0.349, batch=230\n",
      "4276: loss=0.428, reward_mean=0.310, reward_bound=0.451, batch=231\n",
      "4277: loss=0.426, reward_mean=0.260, reward_bound=0.478, batch=224\n",
      "4278: loss=0.428, reward_mean=0.250, reward_bound=0.426, batch=227\n",
      "4279: loss=0.427, reward_mean=0.290, reward_bound=0.349, batch=228\n",
      "4280: loss=0.427, reward_mean=0.310, reward_bound=0.392, batch=229\n",
      "4281: loss=0.427, reward_mean=0.280, reward_bound=0.405, batch=230\n",
      "4282: loss=0.426, reward_mean=0.330, reward_bound=0.406, batch=231\n",
      "4283: loss=0.427, reward_mean=0.290, reward_bound=0.430, batch=228\n",
      "4284: loss=0.425, reward_mean=0.260, reward_bound=0.478, batch=230\n",
      "4285: loss=0.425, reward_mean=0.430, reward_bound=0.349, batch=230\n",
      "4286: loss=0.424, reward_mean=0.210, reward_bound=0.329, batch=231\n",
      "4287: loss=0.425, reward_mean=0.230, reward_bound=0.478, batch=228\n",
      "4288: loss=0.425, reward_mean=0.260, reward_bound=0.397, batch=229\n",
      "4289: loss=0.424, reward_mean=0.200, reward_bound=0.405, batch=230\n",
      "4290: loss=0.424, reward_mean=0.280, reward_bound=0.430, batch=230\n",
      "4291: loss=0.426, reward_mean=0.180, reward_bound=0.340, batch=231\n",
      "4292: loss=0.423, reward_mean=0.330, reward_bound=0.478, batch=230\n",
      "4293: loss=0.425, reward_mean=0.260, reward_bound=0.418, batch=231\n",
      "4294: loss=0.423, reward_mean=0.300, reward_bound=0.430, batch=231\n",
      "4295: loss=0.422, reward_mean=0.170, reward_bound=0.478, batch=231\n",
      "4297: loss=0.389, reward_mean=0.330, reward_bound=0.000, batch=33\n",
      "4298: loss=0.396, reward_mean=0.290, reward_bound=0.000, batch=62\n",
      "4299: loss=0.380, reward_mean=0.350, reward_bound=0.000, batch=97\n",
      "4300: loss=0.376, reward_mean=0.170, reward_bound=0.000, batch=114\n",
      "4301: loss=0.368, reward_mean=0.340, reward_bound=0.000, batch=148\n",
      "4302: loss=0.365, reward_mean=0.320, reward_bound=0.001, batch=172\n",
      "4303: loss=0.362, reward_mean=0.250, reward_bound=0.002, batch=190\n",
      "4304: loss=0.357, reward_mean=0.340, reward_bound=0.013, batch=201\n",
      "4305: loss=0.359, reward_mean=0.320, reward_bound=0.025, batch=209\n",
      "4306: loss=0.358, reward_mean=0.370, reward_bound=0.047, batch=214\n",
      "4307: loss=0.354, reward_mean=0.380, reward_bound=0.072, batch=218\n",
      "4308: loss=0.359, reward_mean=0.270, reward_bound=0.089, batch=210\n",
      "4309: loss=0.368, reward_mean=0.340, reward_bound=0.109, batch=211\n",
      "4310: loss=0.364, reward_mean=0.300, reward_bound=0.122, batch=212\n",
      "4311: loss=0.358, reward_mean=0.220, reward_bound=0.135, batch=208\n",
      "4312: loss=0.356, reward_mean=0.360, reward_bound=0.150, batch=203\n",
      "4313: loss=0.356, reward_mean=0.290, reward_bound=0.160, batch=212\n",
      "4314: loss=0.359, reward_mean=0.330, reward_bound=0.167, batch=203\n",
      "4315: loss=0.356, reward_mean=0.390, reward_bound=0.178, batch=212\n",
      "4316: loss=0.357, reward_mean=0.350, reward_bound=0.185, batch=197\n",
      "4317: loss=0.355, reward_mean=0.290, reward_bound=0.135, batch=207\n",
      "4318: loss=0.369, reward_mean=0.360, reward_bound=0.206, batch=187\n",
      "4319: loss=0.366, reward_mean=0.380, reward_bound=0.147, batch=201\n",
      "4320: loss=0.370, reward_mean=0.300, reward_bound=0.167, batch=208\n",
      "4321: loss=0.373, reward_mean=0.340, reward_bound=0.185, batch=212\n",
      "4322: loss=0.367, reward_mean=0.350, reward_bound=0.206, batch=221\n",
      "4323: loss=0.378, reward_mean=0.390, reward_bound=0.229, batch=200\n",
      "4324: loss=0.375, reward_mean=0.360, reward_bound=0.222, batch=210\n",
      "4325: loss=0.383, reward_mean=0.330, reward_bound=0.254, batch=178\n",
      "4326: loss=0.376, reward_mean=0.310, reward_bound=0.098, batch=193\n",
      "4327: loss=0.372, reward_mean=0.340, reward_bound=0.144, batch=205\n",
      "4328: loss=0.374, reward_mean=0.360, reward_bound=0.185, batch=212\n",
      "4329: loss=0.375, reward_mean=0.360, reward_bound=0.206, batch=222\n",
      "4330: loss=0.380, reward_mean=0.270, reward_bound=0.206, batch=231\n",
      "4331: loss=0.380, reward_mean=0.250, reward_bound=0.229, batch=225\n",
      "4332: loss=0.376, reward_mean=0.370, reward_bound=0.254, batch=225\n",
      "4333: loss=0.385, reward_mean=0.420, reward_bound=0.282, batch=196\n",
      "4334: loss=0.381, reward_mean=0.340, reward_bound=0.153, batch=207\n",
      "4335: loss=0.384, reward_mean=0.320, reward_bound=0.206, batch=212\n",
      "4336: loss=0.386, reward_mean=0.360, reward_bound=0.263, batch=218\n",
      "4337: loss=0.383, reward_mean=0.350, reward_bound=0.282, batch=217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4338: loss=0.378, reward_mean=0.370, reward_bound=0.240, batch=222\n",
      "4339: loss=0.389, reward_mean=0.350, reward_bound=0.314, batch=175\n",
      "4340: loss=0.374, reward_mean=0.340, reward_bound=0.089, batch=193\n",
      "4341: loss=0.369, reward_mean=0.240, reward_bound=0.101, batch=205\n",
      "4342: loss=0.375, reward_mean=0.340, reward_bound=0.135, batch=210\n",
      "4343: loss=0.378, reward_mean=0.350, reward_bound=0.185, batch=216\n",
      "4344: loss=0.383, reward_mean=0.420, reward_bound=0.229, batch=218\n",
      "4345: loss=0.386, reward_mean=0.310, reward_bound=0.254, batch=210\n",
      "4346: loss=0.379, reward_mean=0.360, reward_bound=0.282, batch=211\n",
      "4347: loss=0.378, reward_mean=0.290, reward_bound=0.314, batch=212\n",
      "4348: loss=0.378, reward_mean=0.300, reward_bound=0.214, batch=218\n",
      "4349: loss=0.384, reward_mean=0.290, reward_bound=0.286, batch=222\n",
      "4350: loss=0.385, reward_mean=0.370, reward_bound=0.349, batch=160\n",
      "4351: loss=0.380, reward_mean=0.310, reward_bound=0.037, batch=182\n",
      "4352: loss=0.383, reward_mean=0.410, reward_bound=0.092, batch=197\n",
      "4353: loss=0.383, reward_mean=0.330, reward_bound=0.122, batch=204\n",
      "4354: loss=0.381, reward_mean=0.310, reward_bound=0.149, batch=213\n",
      "4355: loss=0.384, reward_mean=0.340, reward_bound=0.150, batch=217\n",
      "4356: loss=0.390, reward_mean=0.300, reward_bound=0.185, batch=216\n",
      "4357: loss=0.382, reward_mean=0.380, reward_bound=0.229, batch=217\n",
      "4358: loss=0.378, reward_mean=0.360, reward_bound=0.254, batch=213\n",
      "4359: loss=0.381, reward_mean=0.370, reward_bound=0.282, batch=208\n",
      "4360: loss=0.380, reward_mean=0.410, reward_bound=0.187, batch=215\n",
      "4361: loss=0.380, reward_mean=0.350, reward_bound=0.260, batch=220\n",
      "4362: loss=0.377, reward_mean=0.340, reward_bound=0.282, batch=222\n",
      "4363: loss=0.375, reward_mean=0.300, reward_bound=0.292, batch=225\n",
      "4364: loss=0.383, reward_mean=0.320, reward_bound=0.314, batch=216\n",
      "4365: loss=0.381, reward_mean=0.250, reward_bound=0.256, batch=221\n",
      "4366: loss=0.381, reward_mean=0.240, reward_bound=0.282, batch=222\n",
      "4367: loss=0.384, reward_mean=0.360, reward_bound=0.314, batch=224\n",
      "4368: loss=0.382, reward_mean=0.230, reward_bound=0.254, batch=226\n",
      "4369: loss=0.381, reward_mean=0.310, reward_bound=0.349, batch=210\n",
      "4370: loss=0.380, reward_mean=0.340, reward_bound=0.338, batch=217\n",
      "4371: loss=0.378, reward_mean=0.270, reward_bound=0.308, batch=222\n",
      "4372: loss=0.377, reward_mean=0.350, reward_bound=0.349, batch=221\n",
      "4373: loss=0.371, reward_mean=0.260, reward_bound=0.387, batch=142\n",
      "4374: loss=0.360, reward_mean=0.360, reward_bound=0.020, batch=169\n",
      "4375: loss=0.356, reward_mean=0.360, reward_bound=0.058, batch=187\n",
      "4376: loss=0.351, reward_mean=0.310, reward_bound=0.080, batch=200\n",
      "4377: loss=0.366, reward_mean=0.350, reward_bound=0.098, batch=207\n",
      "4378: loss=0.377, reward_mean=0.350, reward_bound=0.122, batch=206\n",
      "4379: loss=0.379, reward_mean=0.400, reward_bound=0.135, batch=209\n",
      "4380: loss=0.371, reward_mean=0.370, reward_bound=0.167, batch=211\n",
      "4381: loss=0.376, reward_mean=0.450, reward_bound=0.206, batch=212\n",
      "4382: loss=0.376, reward_mean=0.370, reward_bound=0.229, batch=217\n",
      "4383: loss=0.375, reward_mean=0.390, reward_bound=0.229, batch=221\n",
      "4384: loss=0.384, reward_mean=0.370, reward_bound=0.254, batch=216\n",
      "4385: loss=0.381, reward_mean=0.390, reward_bound=0.282, batch=210\n",
      "4386: loss=0.383, reward_mean=0.330, reward_bound=0.314, batch=200\n",
      "4387: loss=0.380, reward_mean=0.350, reward_bound=0.185, batch=208\n",
      "4388: loss=0.378, reward_mean=0.380, reward_bound=0.229, batch=214\n",
      "4389: loss=0.377, reward_mean=0.350, reward_bound=0.254, batch=219\n",
      "4390: loss=0.379, reward_mean=0.360, reward_bound=0.282, batch=222\n",
      "4391: loss=0.383, reward_mean=0.290, reward_bound=0.314, batch=217\n",
      "4392: loss=0.385, reward_mean=0.340, reward_bound=0.342, batch=222\n",
      "4393: loss=0.385, reward_mean=0.380, reward_bound=0.349, batch=206\n",
      "4394: loss=0.382, reward_mean=0.380, reward_bound=0.217, batch=214\n",
      "4395: loss=0.385, reward_mean=0.250, reward_bound=0.221, batch=220\n",
      "4396: loss=0.379, reward_mean=0.350, reward_bound=0.282, batch=222\n",
      "4397: loss=0.379, reward_mean=0.350, reward_bound=0.292, batch=225\n",
      "4398: loss=0.382, reward_mean=0.390, reward_bound=0.349, batch=222\n",
      "4399: loss=0.383, reward_mean=0.340, reward_bound=0.314, batch=224\n",
      "4400: loss=0.383, reward_mean=0.280, reward_bound=0.314, batch=226\n",
      "4401: loss=0.381, reward_mean=0.340, reward_bound=0.368, batch=228\n",
      "4402: loss=0.372, reward_mean=0.340, reward_bound=0.387, batch=203\n",
      "4403: loss=0.368, reward_mean=0.320, reward_bound=0.282, batch=209\n",
      "4404: loss=0.371, reward_mean=0.240, reward_bound=0.157, batch=216\n",
      "4405: loss=0.370, reward_mean=0.330, reward_bound=0.206, batch=220\n",
      "4406: loss=0.371, reward_mean=0.300, reward_bound=0.254, batch=223\n",
      "4407: loss=0.370, reward_mean=0.380, reward_bound=0.301, batch=226\n",
      "4408: loss=0.367, reward_mean=0.260, reward_bound=0.314, batch=227\n",
      "4409: loss=0.367, reward_mean=0.330, reward_bound=0.349, batch=221\n",
      "4410: loss=0.366, reward_mean=0.390, reward_bound=0.314, batch=223\n",
      "4411: loss=0.366, reward_mean=0.310, reward_bound=0.314, batch=225\n",
      "4412: loss=0.365, reward_mean=0.400, reward_bound=0.356, batch=227\n",
      "4413: loss=0.368, reward_mean=0.410, reward_bound=0.380, batch=229\n",
      "4414: loss=0.371, reward_mean=0.300, reward_bound=0.387, batch=224\n",
      "4415: loss=0.368, reward_mean=0.350, reward_bound=0.282, batch=226\n",
      "4416: loss=0.371, reward_mean=0.360, reward_bound=0.409, batch=228\n",
      "4417: loss=0.373, reward_mean=0.380, reward_bound=0.430, batch=132\n",
      "4418: loss=0.385, reward_mean=0.460, reward_bound=0.045, batch=162\n",
      "4419: loss=0.361, reward_mean=0.240, reward_bound=0.002, batch=183\n",
      "4420: loss=0.367, reward_mean=0.410, reward_bound=0.052, batch=197\n",
      "4421: loss=0.353, reward_mean=0.270, reward_bound=0.072, batch=207\n",
      "4422: loss=0.360, reward_mean=0.360, reward_bound=0.107, batch=215\n",
      "4423: loss=0.362, reward_mean=0.350, reward_bound=0.135, batch=217\n",
      "4424: loss=0.375, reward_mean=0.390, reward_bound=0.167, batch=211\n",
      "4425: loss=0.377, reward_mean=0.330, reward_bound=0.185, batch=210\n",
      "4426: loss=0.376, reward_mean=0.300, reward_bound=0.189, batch=217\n",
      "4427: loss=0.371, reward_mean=0.450, reward_bound=0.206, batch=221\n",
      "4428: loss=0.371, reward_mean=0.290, reward_bound=0.229, batch=211\n",
      "4429: loss=0.372, reward_mean=0.360, reward_bound=0.254, batch=201\n",
      "4430: loss=0.371, reward_mean=0.200, reward_bound=0.052, batch=209\n",
      "4431: loss=0.374, reward_mean=0.320, reward_bound=0.194, batch=216\n",
      "4432: loss=0.372, reward_mean=0.250, reward_bound=0.206, batch=219\n",
      "4433: loss=0.369, reward_mean=0.330, reward_bound=0.254, batch=221\n",
      "4434: loss=0.365, reward_mean=0.290, reward_bound=0.282, batch=202\n",
      "4435: loss=0.366, reward_mean=0.310, reward_bound=0.135, batch=210\n",
      "4436: loss=0.367, reward_mean=0.300, reward_bound=0.222, batch=217\n",
      "4437: loss=0.369, reward_mean=0.380, reward_bound=0.229, batch=220\n",
      "4438: loss=0.369, reward_mean=0.360, reward_bound=0.282, batch=222\n",
      "4439: loss=0.371, reward_mean=0.390, reward_bound=0.314, batch=209\n",
      "4440: loss=0.373, reward_mean=0.380, reward_bound=0.150, batch=214\n",
      "4441: loss=0.366, reward_mean=0.360, reward_bound=0.282, batch=219\n",
      "4442: loss=0.368, reward_mean=0.340, reward_bound=0.314, batch=220\n",
      "4443: loss=0.367, reward_mean=0.320, reward_bound=0.338, batch=224\n",
      "4444: loss=0.368, reward_mean=0.400, reward_bound=0.349, batch=201\n",
      "4445: loss=0.363, reward_mean=0.350, reward_bound=0.206, batch=209\n",
      "4446: loss=0.360, reward_mean=0.310, reward_bound=0.185, batch=215\n",
      "4447: loss=0.359, reward_mean=0.330, reward_bound=0.282, batch=217\n",
      "4448: loss=0.356, reward_mean=0.300, reward_bound=0.272, batch=222\n",
      "4449: loss=0.355, reward_mean=0.320, reward_bound=0.263, batch=225\n",
      "4450: loss=0.362, reward_mean=0.340, reward_bound=0.314, batch=224\n",
      "4451: loss=0.361, reward_mean=0.450, reward_bound=0.282, batch=226\n",
      "4452: loss=0.361, reward_mean=0.280, reward_bound=0.298, batch=228\n",
      "4453: loss=0.360, reward_mean=0.360, reward_bound=0.349, batch=220\n",
      "4454: loss=0.359, reward_mean=0.270, reward_bound=0.259, batch=224\n",
      "4455: loss=0.358, reward_mean=0.320, reward_bound=0.345, batch=227\n",
      "4456: loss=0.359, reward_mean=0.270, reward_bound=0.349, batch=225\n",
      "4457: loss=0.359, reward_mean=0.460, reward_bound=0.356, batch=227\n",
      "4458: loss=0.359, reward_mean=0.380, reward_bound=0.387, batch=200\n",
      "4459: loss=0.354, reward_mean=0.340, reward_bound=0.150, batch=209\n",
      "4460: loss=0.360, reward_mean=0.370, reward_bound=0.229, batch=213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4461: loss=0.358, reward_mean=0.430, reward_bound=0.282, batch=217\n",
      "4462: loss=0.360, reward_mean=0.390, reward_bound=0.314, batch=217\n",
      "4463: loss=0.357, reward_mean=0.340, reward_bound=0.229, batch=221\n",
      "4464: loss=0.359, reward_mean=0.310, reward_bound=0.254, batch=224\n",
      "4465: loss=0.361, reward_mean=0.350, reward_bound=0.314, batch=225\n",
      "4466: loss=0.357, reward_mean=0.350, reward_bound=0.349, batch=221\n",
      "4467: loss=0.357, reward_mean=0.420, reward_bound=0.314, batch=223\n",
      "4468: loss=0.359, reward_mean=0.380, reward_bound=0.349, batch=224\n",
      "4469: loss=0.361, reward_mean=0.400, reward_bound=0.314, batch=226\n",
      "4470: loss=0.359, reward_mean=0.360, reward_bound=0.368, batch=228\n",
      "4471: loss=0.360, reward_mean=0.390, reward_bound=0.353, batch=229\n",
      "4472: loss=0.359, reward_mean=0.470, reward_bound=0.387, batch=221\n",
      "4473: loss=0.358, reward_mean=0.440, reward_bound=0.349, batch=224\n",
      "4474: loss=0.354, reward_mean=0.420, reward_bound=0.277, batch=227\n",
      "4475: loss=0.355, reward_mean=0.390, reward_bound=0.380, batch=229\n",
      "4476: loss=0.355, reward_mean=0.320, reward_bound=0.387, batch=229\n",
      "4477: loss=0.366, reward_mean=0.370, reward_bound=0.430, batch=190\n",
      "4478: loss=0.363, reward_mean=0.350, reward_bound=0.175, batch=203\n",
      "4479: loss=0.370, reward_mean=0.330, reward_bound=0.229, batch=209\n",
      "4480: loss=0.365, reward_mean=0.310, reward_bound=0.132, batch=216\n",
      "4481: loss=0.362, reward_mean=0.330, reward_bound=0.186, batch=221\n",
      "4482: loss=0.367, reward_mean=0.350, reward_bound=0.229, batch=223\n",
      "4483: loss=0.371, reward_mean=0.370, reward_bound=0.254, batch=224\n",
      "4484: loss=0.367, reward_mean=0.320, reward_bound=0.282, batch=219\n",
      "4485: loss=0.364, reward_mean=0.350, reward_bound=0.215, batch=223\n",
      "4486: loss=0.369, reward_mean=0.390, reward_bound=0.282, batch=225\n",
      "4487: loss=0.370, reward_mean=0.470, reward_bound=0.314, batch=222\n",
      "4488: loss=0.369, reward_mean=0.380, reward_bound=0.292, batch=225\n",
      "4489: loss=0.367, reward_mean=0.380, reward_bound=0.321, batch=227\n",
      "4490: loss=0.369, reward_mean=0.340, reward_bound=0.349, batch=221\n",
      "4491: loss=0.371, reward_mean=0.390, reward_bound=0.349, batch=224\n",
      "4492: loss=0.370, reward_mean=0.410, reward_bound=0.314, batch=226\n",
      "4493: loss=0.374, reward_mean=0.230, reward_bound=0.368, batch=228\n",
      "4494: loss=0.372, reward_mean=0.330, reward_bound=0.387, batch=216\n",
      "4495: loss=0.377, reward_mean=0.410, reward_bound=0.351, batch=221\n",
      "4496: loss=0.377, reward_mean=0.360, reward_bound=0.349, batch=224\n",
      "4497: loss=0.378, reward_mean=0.300, reward_bound=0.387, batch=224\n",
      "4498: loss=0.379, reward_mean=0.390, reward_bound=0.314, batch=226\n",
      "4499: loss=0.379, reward_mean=0.360, reward_bound=0.316, batch=228\n",
      "4500: loss=0.378, reward_mean=0.340, reward_bound=0.357, batch=229\n",
      "4501: loss=0.380, reward_mean=0.370, reward_bound=0.405, batch=230\n",
      "4502: loss=0.374, reward_mean=0.310, reward_bound=0.430, batch=215\n",
      "4503: loss=0.377, reward_mean=0.300, reward_bound=0.296, batch=220\n",
      "4504: loss=0.377, reward_mean=0.360, reward_bound=0.349, batch=222\n",
      "4505: loss=0.377, reward_mean=0.340, reward_bound=0.387, batch=223\n",
      "4506: loss=0.378, reward_mean=0.270, reward_bound=0.335, batch=226\n",
      "4507: loss=0.372, reward_mean=0.330, reward_bound=0.430, batch=221\n",
      "4508: loss=0.372, reward_mean=0.360, reward_bound=0.349, batch=223\n",
      "4509: loss=0.370, reward_mean=0.390, reward_bound=0.372, batch=226\n",
      "4510: loss=0.369, reward_mean=0.310, reward_bound=0.368, batch=228\n",
      "4511: loss=0.367, reward_mean=0.340, reward_bound=0.392, batch=229\n",
      "4512: loss=0.366, reward_mean=0.320, reward_bound=0.381, batch=230\n",
      "4513: loss=0.368, reward_mean=0.330, reward_bound=0.430, batch=227\n",
      "4514: loss=0.369, reward_mean=0.410, reward_bound=0.387, batch=228\n",
      "4515: loss=0.368, reward_mean=0.320, reward_bound=0.478, batch=231\n",
      "4516: loss=0.367, reward_mean=0.360, reward_bound=0.478, batch=95\n",
      "4517: loss=0.321, reward_mean=0.320, reward_bound=0.000, batch=127\n",
      "4518: loss=0.320, reward_mean=0.320, reward_bound=0.000, batch=159\n",
      "4519: loss=0.324, reward_mean=0.310, reward_bound=0.012, batch=181\n",
      "4520: loss=0.334, reward_mean=0.410, reward_bound=0.038, batch=194\n",
      "4521: loss=0.329, reward_mean=0.290, reward_bound=0.047, batch=202\n",
      "4522: loss=0.327, reward_mean=0.320, reward_bound=0.065, batch=209\n",
      "4523: loss=0.331, reward_mean=0.380, reward_bound=0.080, batch=215\n",
      "4524: loss=0.335, reward_mean=0.430, reward_bound=0.109, batch=227\n",
      "4525: loss=0.340, reward_mean=0.440, reward_bound=0.122, batch=220\n",
      "4526: loss=0.333, reward_mean=0.310, reward_bound=0.135, batch=222\n",
      "4527: loss=0.348, reward_mean=0.350, reward_bound=0.167, batch=214\n",
      "4528: loss=0.351, reward_mean=0.400, reward_bound=0.185, batch=204\n",
      "4529: loss=0.353, reward_mean=0.350, reward_bound=0.167, batch=212\n",
      "4530: loss=0.347, reward_mean=0.360, reward_bound=0.206, batch=221\n",
      "4531: loss=0.357, reward_mean=0.310, reward_bound=0.206, batch=212\n",
      "4532: loss=0.363, reward_mean=0.360, reward_bound=0.229, batch=201\n",
      "4533: loss=0.361, reward_mean=0.410, reward_bound=0.206, batch=209\n",
      "4534: loss=0.351, reward_mean=0.440, reward_bound=0.254, batch=201\n",
      "4535: loss=0.358, reward_mean=0.360, reward_bound=0.135, batch=210\n",
      "4536: loss=0.357, reward_mean=0.330, reward_bound=0.200, batch=217\n",
      "4537: loss=0.353, reward_mean=0.280, reward_bound=0.249, batch=222\n",
      "4538: loss=0.349, reward_mean=0.340, reward_bound=0.282, batch=208\n",
      "4539: loss=0.348, reward_mean=0.350, reward_bound=0.257, batch=215\n",
      "4540: loss=0.348, reward_mean=0.280, reward_bound=0.282, batch=217\n",
      "4541: loss=0.348, reward_mean=0.390, reward_bound=0.314, batch=200\n",
      "4542: loss=0.343, reward_mean=0.400, reward_bound=0.229, batch=209\n",
      "4543: loss=0.341, reward_mean=0.410, reward_bound=0.147, batch=216\n",
      "4544: loss=0.344, reward_mean=0.370, reward_bound=0.282, batch=219\n",
      "4545: loss=0.345, reward_mean=0.350, reward_bound=0.314, batch=218\n",
      "4546: loss=0.342, reward_mean=0.300, reward_bound=0.314, batch=221\n",
      "4547: loss=0.342, reward_mean=0.390, reward_bound=0.314, batch=224\n",
      "4548: loss=0.340, reward_mean=0.320, reward_bound=0.339, batch=227\n",
      "4549: loss=0.346, reward_mean=0.390, reward_bound=0.349, batch=194\n",
      "4550: loss=0.347, reward_mean=0.450, reward_bound=0.183, batch=206\n",
      "4551: loss=0.348, reward_mean=0.420, reward_bound=0.217, batch=214\n",
      "4552: loss=0.343, reward_mean=0.330, reward_bound=0.254, batch=218\n",
      "4553: loss=0.343, reward_mean=0.460, reward_bound=0.282, batch=217\n",
      "4554: loss=0.344, reward_mean=0.380, reward_bound=0.282, batch=219\n",
      "4555: loss=0.347, reward_mean=0.380, reward_bound=0.328, batch=223\n",
      "4556: loss=0.345, reward_mean=0.370, reward_bound=0.301, batch=226\n",
      "4557: loss=0.349, reward_mean=0.350, reward_bound=0.314, batch=227\n",
      "4558: loss=0.348, reward_mean=0.330, reward_bound=0.349, batch=219\n",
      "4559: loss=0.351, reward_mean=0.390, reward_bound=0.265, batch=223\n",
      "4560: loss=0.349, reward_mean=0.420, reward_bound=0.261, batch=226\n",
      "4561: loss=0.350, reward_mean=0.380, reward_bound=0.282, batch=227\n",
      "4562: loss=0.345, reward_mean=0.370, reward_bound=0.314, batch=228\n",
      "4563: loss=0.357, reward_mean=0.300, reward_bound=0.387, batch=180\n",
      "4564: loss=0.345, reward_mean=0.320, reward_bound=0.122, batch=195\n",
      "4565: loss=0.348, reward_mean=0.350, reward_bound=0.138, batch=206\n",
      "4566: loss=0.349, reward_mean=0.370, reward_bound=0.150, batch=212\n",
      "4567: loss=0.349, reward_mean=0.270, reward_bound=0.185, batch=217\n",
      "4568: loss=0.350, reward_mean=0.380, reward_bound=0.206, batch=220\n",
      "4569: loss=0.353, reward_mean=0.330, reward_bound=0.254, batch=214\n",
      "4570: loss=0.355, reward_mean=0.320, reward_bound=0.229, batch=219\n",
      "4571: loss=0.354, reward_mean=0.360, reward_bound=0.282, batch=218\n",
      "4572: loss=0.355, reward_mean=0.280, reward_bound=0.286, batch=222\n",
      "4573: loss=0.357, reward_mean=0.370, reward_bound=0.272, batch=225\n",
      "4574: loss=0.359, reward_mean=0.360, reward_bound=0.314, batch=221\n",
      "4575: loss=0.358, reward_mean=0.300, reward_bound=0.314, batch=224\n",
      "4576: loss=0.356, reward_mean=0.400, reward_bound=0.349, batch=212\n",
      "4577: loss=0.356, reward_mean=0.370, reward_bound=0.263, batch=218\n",
      "4578: loss=0.361, reward_mean=0.440, reward_bound=0.349, batch=220\n",
      "4579: loss=0.361, reward_mean=0.370, reward_bound=0.338, batch=224\n",
      "4580: loss=0.353, reward_mean=0.350, reward_bound=0.387, batch=208\n",
      "4581: loss=0.351, reward_mean=0.380, reward_bound=0.257, batch=215\n",
      "4582: loss=0.350, reward_mean=0.330, reward_bound=0.282, batch=219\n",
      "4583: loss=0.349, reward_mean=0.360, reward_bound=0.349, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4584: loss=0.349, reward_mean=0.310, reward_bound=0.254, batch=224\n",
      "4585: loss=0.348, reward_mean=0.330, reward_bound=0.349, batch=226\n",
      "4586: loss=0.349, reward_mean=0.400, reward_bound=0.368, batch=228\n",
      "4587: loss=0.350, reward_mean=0.360, reward_bound=0.353, batch=229\n",
      "4588: loss=0.354, reward_mean=0.360, reward_bound=0.387, batch=223\n",
      "4589: loss=0.353, reward_mean=0.320, reward_bound=0.322, batch=226\n",
      "4590: loss=0.356, reward_mean=0.310, reward_bound=0.390, batch=228\n",
      "4591: loss=0.358, reward_mean=0.420, reward_bound=0.430, batch=170\n",
      "4592: loss=0.354, reward_mean=0.460, reward_bound=0.185, batch=187\n",
      "4593: loss=0.348, reward_mean=0.420, reward_bound=0.109, batch=203\n",
      "4594: loss=0.343, reward_mean=0.350, reward_bound=0.135, batch=211\n",
      "4595: loss=0.343, reward_mean=0.270, reward_bound=0.150, batch=215\n",
      "4596: loss=0.345, reward_mean=0.370, reward_bound=0.206, batch=210\n",
      "4597: loss=0.342, reward_mean=0.380, reward_bound=0.222, batch=217\n",
      "4598: loss=0.356, reward_mean=0.360, reward_bound=0.229, batch=219\n",
      "4599: loss=0.353, reward_mean=0.350, reward_bound=0.254, batch=220\n",
      "4600: loss=0.353, reward_mean=0.360, reward_bound=0.254, batch=223\n",
      "4601: loss=0.351, reward_mean=0.330, reward_bound=0.261, batch=226\n",
      "4602: loss=0.350, reward_mean=0.310, reward_bound=0.282, batch=225\n",
      "4603: loss=0.352, reward_mean=0.320, reward_bound=0.314, batch=216\n",
      "4604: loss=0.350, reward_mean=0.340, reward_bound=0.185, batch=219\n",
      "4605: loss=0.348, reward_mean=0.320, reward_bound=0.254, batch=222\n",
      "4606: loss=0.351, reward_mean=0.370, reward_bound=0.263, batch=225\n",
      "4607: loss=0.355, reward_mean=0.420, reward_bound=0.349, batch=205\n",
      "4608: loss=0.358, reward_mean=0.310, reward_bound=0.254, batch=212\n",
      "4609: loss=0.357, reward_mean=0.330, reward_bound=0.245, batch=218\n",
      "4610: loss=0.359, reward_mean=0.350, reward_bound=0.282, batch=221\n",
      "4611: loss=0.357, reward_mean=0.330, reward_bound=0.349, batch=224\n",
      "4612: loss=0.357, reward_mean=0.320, reward_bound=0.349, batch=225\n",
      "4613: loss=0.345, reward_mean=0.410, reward_bound=0.387, batch=210\n",
      "4614: loss=0.345, reward_mean=0.310, reward_bound=0.229, batch=216\n",
      "4615: loss=0.345, reward_mean=0.280, reward_bound=0.254, batch=220\n",
      "4616: loss=0.340, reward_mean=0.300, reward_bound=0.304, batch=224\n",
      "4617: loss=0.340, reward_mean=0.350, reward_bound=0.314, batch=223\n",
      "4618: loss=0.341, reward_mean=0.370, reward_bound=0.349, batch=224\n",
      "4619: loss=0.341, reward_mean=0.370, reward_bound=0.349, batch=226\n",
      "4620: loss=0.341, reward_mean=0.360, reward_bound=0.254, batch=226\n",
      "4621: loss=0.339, reward_mean=0.380, reward_bound=0.331, batch=228\n",
      "4622: loss=0.338, reward_mean=0.330, reward_bound=0.317, batch=229\n",
      "4623: loss=0.341, reward_mean=0.300, reward_bound=0.387, batch=226\n",
      "4624: loss=0.339, reward_mean=0.380, reward_bound=0.351, batch=228\n",
      "4625: loss=0.354, reward_mean=0.400, reward_bound=0.430, batch=198\n",
      "4626: loss=0.353, reward_mean=0.340, reward_bound=0.154, batch=208\n",
      "4627: loss=0.355, reward_mean=0.310, reward_bound=0.208, batch=215\n",
      "4628: loss=0.353, reward_mean=0.350, reward_bound=0.210, batch=220\n",
      "4629: loss=0.352, reward_mean=0.390, reward_bound=0.254, batch=217\n",
      "4630: loss=0.354, reward_mean=0.330, reward_bound=0.308, batch=222\n",
      "4631: loss=0.350, reward_mean=0.350, reward_bound=0.314, batch=220\n",
      "4632: loss=0.349, reward_mean=0.360, reward_bound=0.304, batch=224\n",
      "4633: loss=0.349, reward_mean=0.310, reward_bound=0.349, batch=219\n",
      "4634: loss=0.350, reward_mean=0.380, reward_bound=0.309, batch=223\n",
      "4635: loss=0.349, reward_mean=0.310, reward_bound=0.387, batch=222\n",
      "4636: loss=0.348, reward_mean=0.300, reward_bound=0.324, batch=225\n",
      "4637: loss=0.356, reward_mean=0.450, reward_bound=0.430, batch=215\n",
      "4638: loss=0.354, reward_mean=0.360, reward_bound=0.387, batch=219\n",
      "4639: loss=0.353, reward_mean=0.380, reward_bound=0.295, batch=223\n",
      "4640: loss=0.352, reward_mean=0.340, reward_bound=0.314, batch=225\n",
      "4641: loss=0.352, reward_mean=0.380, reward_bound=0.387, batch=224\n",
      "4642: loss=0.354, reward_mean=0.370, reward_bound=0.314, batch=226\n",
      "4643: loss=0.356, reward_mean=0.340, reward_bound=0.430, batch=222\n",
      "4644: loss=0.356, reward_mean=0.430, reward_bound=0.445, batch=225\n",
      "4645: loss=0.356, reward_mean=0.400, reward_bound=0.430, batch=225\n",
      "4646: loss=0.356, reward_mean=0.260, reward_bound=0.349, batch=225\n",
      "4647: loss=0.355, reward_mean=0.410, reward_bound=0.396, batch=227\n",
      "4648: loss=0.354, reward_mean=0.390, reward_bound=0.342, batch=229\n",
      "4649: loss=0.354, reward_mean=0.370, reward_bound=0.349, batch=229\n",
      "4650: loss=0.355, reward_mean=0.360, reward_bound=0.430, batch=226\n",
      "4651: loss=0.355, reward_mean=0.350, reward_bound=0.390, batch=228\n",
      "4652: loss=0.355, reward_mean=0.310, reward_bound=0.325, batch=229\n",
      "4653: loss=0.354, reward_mean=0.440, reward_bound=0.343, batch=230\n",
      "4654: loss=0.356, reward_mean=0.200, reward_bound=0.418, batch=231\n",
      "4655: loss=0.356, reward_mean=0.270, reward_bound=0.430, batch=229\n",
      "4656: loss=0.357, reward_mean=0.350, reward_bound=0.450, batch=230\n",
      "4657: loss=0.350, reward_mean=0.390, reward_bound=0.478, batch=151\n",
      "4658: loss=0.332, reward_mean=0.330, reward_bound=0.005, batch=175\n",
      "4659: loss=0.331, reward_mean=0.310, reward_bound=0.029, batch=192\n",
      "4660: loss=0.325, reward_mean=0.380, reward_bound=0.058, batch=202\n",
      "4661: loss=0.344, reward_mean=0.340, reward_bound=0.089, batch=208\n",
      "4662: loss=0.349, reward_mean=0.340, reward_bound=0.150, batch=212\n",
      "4663: loss=0.358, reward_mean=0.340, reward_bound=0.185, batch=213\n",
      "4664: loss=0.356, reward_mean=0.280, reward_bound=0.206, batch=212\n",
      "4665: loss=0.359, reward_mean=0.350, reward_bound=0.213, batch=218\n",
      "4666: loss=0.360, reward_mean=0.320, reward_bound=0.229, batch=219\n",
      "4667: loss=0.364, reward_mean=0.350, reward_bound=0.254, batch=219\n",
      "4668: loss=0.362, reward_mean=0.350, reward_bound=0.265, batch=223\n",
      "4669: loss=0.360, reward_mean=0.350, reward_bound=0.282, batch=216\n",
      "4670: loss=0.356, reward_mean=0.330, reward_bound=0.256, batch=221\n",
      "4671: loss=0.359, reward_mean=0.380, reward_bound=0.282, batch=224\n",
      "4672: loss=0.367, reward_mean=0.420, reward_bound=0.314, batch=214\n",
      "4673: loss=0.364, reward_mean=0.330, reward_bound=0.224, batch=220\n",
      "4674: loss=0.363, reward_mean=0.280, reward_bound=0.229, batch=223\n",
      "4675: loss=0.367, reward_mean=0.310, reward_bound=0.301, batch=226\n",
      "4676: loss=0.366, reward_mean=0.340, reward_bound=0.349, batch=199\n",
      "4677: loss=0.368, reward_mean=0.280, reward_bound=0.119, batch=209\n",
      "4678: loss=0.367, reward_mean=0.260, reward_bound=0.194, batch=216\n",
      "4679: loss=0.362, reward_mean=0.380, reward_bound=0.229, batch=219\n",
      "4680: loss=0.358, reward_mean=0.270, reward_bound=0.254, batch=221\n",
      "4681: loss=0.358, reward_mean=0.310, reward_bound=0.282, batch=220\n",
      "4682: loss=0.363, reward_mean=0.340, reward_bound=0.314, batch=221\n",
      "4683: loss=0.363, reward_mean=0.440, reward_bound=0.349, batch=218\n",
      "4684: loss=0.363, reward_mean=0.210, reward_bound=0.268, batch=222\n",
      "4685: loss=0.364, reward_mean=0.340, reward_bound=0.387, batch=201\n",
      "4686: loss=0.369, reward_mean=0.350, reward_bound=0.229, batch=210\n",
      "4687: loss=0.366, reward_mean=0.280, reward_bound=0.254, batch=216\n",
      "4688: loss=0.364, reward_mean=0.300, reward_bound=0.229, batch=219\n",
      "4689: loss=0.361, reward_mean=0.340, reward_bound=0.282, batch=222\n",
      "4690: loss=0.362, reward_mean=0.430, reward_bound=0.314, batch=221\n",
      "4691: loss=0.362, reward_mean=0.330, reward_bound=0.349, batch=217\n",
      "4692: loss=0.363, reward_mean=0.340, reward_bound=0.349, batch=219\n",
      "4693: loss=0.365, reward_mean=0.370, reward_bound=0.328, batch=223\n",
      "4694: loss=0.367, reward_mean=0.300, reward_bound=0.335, batch=226\n",
      "4695: loss=0.368, reward_mean=0.250, reward_bound=0.298, batch=228\n",
      "4696: loss=0.366, reward_mean=0.410, reward_bound=0.349, batch=228\n",
      "4697: loss=0.372, reward_mean=0.350, reward_bound=0.387, batch=220\n",
      "4698: loss=0.374, reward_mean=0.270, reward_bound=0.406, batch=224\n",
      "4699: loss=0.373, reward_mean=0.320, reward_bound=0.349, batch=226\n",
      "4700: loss=0.373, reward_mean=0.360, reward_bound=0.387, batch=225\n",
      "4701: loss=0.373, reward_mean=0.260, reward_bound=0.387, batch=226\n",
      "4702: loss=0.373, reward_mean=0.300, reward_bound=0.387, batch=227\n",
      "4703: loss=0.381, reward_mean=0.250, reward_bound=0.430, batch=199\n",
      "4704: loss=0.378, reward_mean=0.300, reward_bound=0.265, batch=209\n",
      "4705: loss=0.373, reward_mean=0.350, reward_bound=0.215, batch=216\n",
      "4706: loss=0.374, reward_mean=0.340, reward_bound=0.256, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4707: loss=0.376, reward_mean=0.380, reward_bound=0.282, batch=217\n",
      "4708: loss=0.382, reward_mean=0.320, reward_bound=0.249, batch=222\n",
      "4709: loss=0.380, reward_mean=0.350, reward_bound=0.282, batch=224\n",
      "4710: loss=0.377, reward_mean=0.310, reward_bound=0.183, batch=227\n",
      "4711: loss=0.377, reward_mean=0.290, reward_bound=0.314, batch=226\n",
      "4712: loss=0.376, reward_mean=0.400, reward_bound=0.349, batch=227\n",
      "4713: loss=0.378, reward_mean=0.400, reward_bound=0.387, batch=218\n",
      "4714: loss=0.380, reward_mean=0.310, reward_bound=0.231, batch=222\n",
      "4715: loss=0.375, reward_mean=0.340, reward_bound=0.324, batch=225\n",
      "4716: loss=0.377, reward_mean=0.380, reward_bound=0.387, batch=223\n",
      "4717: loss=0.376, reward_mean=0.310, reward_bound=0.372, batch=226\n",
      "4718: loss=0.375, reward_mean=0.330, reward_bound=0.430, batch=218\n",
      "4719: loss=0.373, reward_mean=0.290, reward_bound=0.314, batch=220\n",
      "4720: loss=0.382, reward_mean=0.400, reward_bound=0.349, batch=220\n",
      "4721: loss=0.380, reward_mean=0.350, reward_bound=0.376, batch=224\n",
      "4722: loss=0.379, reward_mean=0.360, reward_bound=0.349, batch=226\n",
      "4723: loss=0.376, reward_mean=0.400, reward_bound=0.387, batch=226\n",
      "4724: loss=0.373, reward_mean=0.320, reward_bound=0.430, batch=224\n",
      "4725: loss=0.375, reward_mean=0.340, reward_bound=0.349, batch=226\n",
      "4726: loss=0.377, reward_mean=0.350, reward_bound=0.282, batch=227\n",
      "4727: loss=0.373, reward_mean=0.350, reward_bound=0.349, batch=228\n",
      "4728: loss=0.375, reward_mean=0.340, reward_bound=0.430, batch=227\n",
      "4729: loss=0.375, reward_mean=0.370, reward_bound=0.460, batch=229\n",
      "4730: loss=0.375, reward_mean=0.390, reward_bound=0.424, batch=230\n",
      "4731: loss=0.375, reward_mean=0.270, reward_bound=0.464, batch=231\n",
      "4732: loss=0.371, reward_mean=0.370, reward_bound=0.478, batch=186\n",
      "4733: loss=0.362, reward_mean=0.330, reward_bound=0.094, batch=200\n",
      "4734: loss=0.365, reward_mean=0.340, reward_bound=0.146, batch=210\n",
      "4735: loss=0.379, reward_mean=0.260, reward_bound=0.167, batch=215\n",
      "4736: loss=0.376, reward_mean=0.310, reward_bound=0.185, batch=219\n",
      "4737: loss=0.382, reward_mean=0.310, reward_bound=0.206, batch=219\n",
      "4738: loss=0.388, reward_mean=0.430, reward_bound=0.254, batch=220\n",
      "4739: loss=0.381, reward_mean=0.310, reward_bound=0.282, batch=217\n",
      "4740: loss=0.383, reward_mean=0.270, reward_bound=0.254, batch=221\n",
      "4741: loss=0.382, reward_mean=0.360, reward_bound=0.314, batch=213\n",
      "4742: loss=0.381, reward_mean=0.350, reward_bound=0.160, batch=219\n",
      "4743: loss=0.386, reward_mean=0.360, reward_bound=0.229, batch=221\n",
      "4744: loss=0.377, reward_mean=0.320, reward_bound=0.349, batch=215\n",
      "4745: loss=0.378, reward_mean=0.300, reward_bound=0.321, batch=220\n",
      "4746: loss=0.382, reward_mean=0.240, reward_bound=0.254, batch=223\n",
      "4747: loss=0.377, reward_mean=0.320, reward_bound=0.349, batch=225\n",
      "4748: loss=0.374, reward_mean=0.310, reward_bound=0.387, batch=218\n",
      "4749: loss=0.375, reward_mean=0.270, reward_bound=0.206, batch=221\n",
      "4750: loss=0.372, reward_mean=0.350, reward_bound=0.254, batch=224\n",
      "4751: loss=0.373, reward_mean=0.280, reward_bound=0.314, batch=224\n",
      "4752: loss=0.375, reward_mean=0.270, reward_bound=0.314, batch=226\n",
      "4753: loss=0.377, reward_mean=0.270, reward_bound=0.368, batch=228\n",
      "4754: loss=0.376, reward_mean=0.310, reward_bound=0.387, batch=228\n",
      "4755: loss=0.378, reward_mean=0.300, reward_bound=0.430, batch=209\n",
      "4756: loss=0.377, reward_mean=0.250, reward_bound=0.215, batch=216\n",
      "4757: loss=0.374, reward_mean=0.300, reward_bound=0.282, batch=220\n",
      "4758: loss=0.376, reward_mean=0.310, reward_bound=0.314, batch=221\n",
      "4759: loss=0.376, reward_mean=0.320, reward_bound=0.254, batch=224\n",
      "4760: loss=0.376, reward_mean=0.290, reward_bound=0.349, batch=222\n",
      "4761: loss=0.376, reward_mean=0.320, reward_bound=0.282, batch=224\n",
      "4762: loss=0.376, reward_mean=0.300, reward_bound=0.349, batch=226\n",
      "4763: loss=0.374, reward_mean=0.290, reward_bound=0.316, batch=228\n",
      "4764: loss=0.378, reward_mean=0.340, reward_bound=0.353, batch=229\n",
      "4765: loss=0.378, reward_mean=0.280, reward_bound=0.364, batch=230\n",
      "4766: loss=0.378, reward_mean=0.290, reward_bound=0.376, batch=231\n",
      "4767: loss=0.378, reward_mean=0.260, reward_bound=0.387, batch=219\n",
      "4768: loss=0.377, reward_mean=0.410, reward_bound=0.343, batch=223\n",
      "4769: loss=0.376, reward_mean=0.350, reward_bound=0.387, batch=224\n",
      "4770: loss=0.379, reward_mean=0.280, reward_bound=0.384, batch=227\n",
      "4771: loss=0.374, reward_mean=0.380, reward_bound=0.387, batch=226\n",
      "4772: loss=0.373, reward_mean=0.410, reward_bound=0.387, batch=227\n",
      "4773: loss=0.379, reward_mean=0.430, reward_bound=0.430, batch=222\n",
      "4774: loss=0.377, reward_mean=0.380, reward_bound=0.430, batch=224\n",
      "4775: loss=0.377, reward_mean=0.380, reward_bound=0.342, batch=227\n",
      "4776: loss=0.378, reward_mean=0.390, reward_bound=0.342, batch=229\n",
      "4777: loss=0.376, reward_mean=0.360, reward_bound=0.387, batch=229\n",
      "4778: loss=0.379, reward_mean=0.320, reward_bound=0.430, batch=229\n",
      "4779: loss=0.378, reward_mean=0.270, reward_bound=0.424, batch=230\n",
      "4780: loss=0.378, reward_mean=0.300, reward_bound=0.464, batch=231\n",
      "4781: loss=0.377, reward_mean=0.320, reward_bound=0.478, batch=208\n",
      "4782: loss=0.380, reward_mean=0.340, reward_bound=0.286, batch=215\n",
      "4783: loss=0.380, reward_mean=0.310, reward_bound=0.314, batch=218\n",
      "4784: loss=0.379, reward_mean=0.370, reward_bound=0.349, batch=220\n",
      "4785: loss=0.379, reward_mean=0.290, reward_bound=0.387, batch=221\n",
      "4786: loss=0.379, reward_mean=0.360, reward_bound=0.314, batch=224\n",
      "4787: loss=0.377, reward_mean=0.280, reward_bound=0.349, batch=226\n",
      "4788: loss=0.378, reward_mean=0.290, reward_bound=0.368, batch=228\n",
      "4789: loss=0.378, reward_mean=0.330, reward_bound=0.353, batch=229\n",
      "4790: loss=0.380, reward_mean=0.330, reward_bound=0.387, batch=229\n",
      "4791: loss=0.380, reward_mean=0.350, reward_bound=0.405, batch=230\n",
      "4792: loss=0.380, reward_mean=0.360, reward_bound=0.406, batch=231\n",
      "4793: loss=0.374, reward_mean=0.270, reward_bound=0.430, batch=220\n",
      "4794: loss=0.373, reward_mean=0.430, reward_bound=0.314, batch=223\n",
      "4795: loss=0.372, reward_mean=0.390, reward_bound=0.335, batch=226\n",
      "4796: loss=0.375, reward_mean=0.300, reward_bound=0.349, batch=225\n",
      "4797: loss=0.377, reward_mean=0.340, reward_bound=0.314, batch=226\n",
      "4798: loss=0.373, reward_mean=0.440, reward_bound=0.387, batch=225\n",
      "4799: loss=0.373, reward_mean=0.430, reward_bound=0.396, batch=227\n",
      "4800: loss=0.373, reward_mean=0.390, reward_bound=0.430, batch=226\n",
      "4801: loss=0.372, reward_mean=0.340, reward_bound=0.409, batch=228\n",
      "4802: loss=0.372, reward_mean=0.400, reward_bound=0.435, batch=229\n",
      "4803: loss=0.372, reward_mean=0.260, reward_bound=0.478, batch=231\n",
      "4804: loss=0.373, reward_mean=0.300, reward_bound=0.478, batch=220\n",
      "4805: loss=0.372, reward_mean=0.300, reward_bound=0.234, batch=224\n",
      "4806: loss=0.372, reward_mean=0.290, reward_bound=0.252, batch=227\n",
      "4807: loss=0.371, reward_mean=0.330, reward_bound=0.387, batch=227\n",
      "4808: loss=0.371, reward_mean=0.320, reward_bound=0.335, batch=229\n",
      "4809: loss=0.370, reward_mean=0.320, reward_bound=0.381, batch=230\n",
      "4810: loss=0.369, reward_mean=0.310, reward_bound=0.430, batch=228\n",
      "4811: loss=0.370, reward_mean=0.380, reward_bound=0.435, batch=229\n",
      "4812: loss=0.370, reward_mean=0.330, reward_bound=0.405, batch=230\n",
      "4813: loss=0.371, reward_mean=0.390, reward_bound=0.430, batch=230\n",
      "4814: loss=0.371, reward_mean=0.340, reward_bound=0.439, batch=231\n",
      "4815: loss=0.373, reward_mean=0.300, reward_bound=0.478, batch=224\n",
      "4816: loss=0.371, reward_mean=0.380, reward_bound=0.426, batch=227\n",
      "4817: loss=0.370, reward_mean=0.380, reward_bound=0.387, batch=228\n",
      "4818: loss=0.373, reward_mean=0.330, reward_bound=0.430, batch=228\n",
      "4819: loss=0.372, reward_mean=0.370, reward_bound=0.478, batch=230\n",
      "4820: loss=0.371, reward_mean=0.340, reward_bound=0.418, batch=231\n",
      "4821: loss=0.371, reward_mean=0.350, reward_bound=0.430, batch=231\n",
      "4822: loss=0.372, reward_mean=0.290, reward_bound=0.478, batch=225\n",
      "4823: loss=0.371, reward_mean=0.320, reward_bound=0.387, batch=226\n",
      "4824: loss=0.369, reward_mean=0.320, reward_bound=0.316, batch=228\n",
      "4825: loss=0.370, reward_mean=0.320, reward_bound=0.392, batch=229\n",
      "4826: loss=0.369, reward_mean=0.300, reward_bound=0.405, batch=230\n",
      "4827: loss=0.368, reward_mean=0.440, reward_bound=0.406, batch=231\n",
      "4828: loss=0.370, reward_mean=0.320, reward_bound=0.430, batch=229\n",
      "4829: loss=0.370, reward_mean=0.330, reward_bound=0.360, batch=230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4830: loss=0.370, reward_mean=0.320, reward_bound=0.430, batch=229\n",
      "4831: loss=0.370, reward_mean=0.360, reward_bound=0.424, batch=230\n",
      "4832: loss=0.370, reward_mean=0.270, reward_bound=0.418, batch=231\n",
      "4833: loss=0.371, reward_mean=0.360, reward_bound=0.478, batch=229\n",
      "4834: loss=0.371, reward_mean=0.300, reward_bound=0.381, batch=230\n",
      "4835: loss=0.372, reward_mean=0.400, reward_bound=0.464, batch=231\n",
      "4836: loss=0.373, reward_mean=0.340, reward_bound=0.478, batch=231\n",
      "4837: loss=0.373, reward_mean=0.290, reward_bound=0.430, batch=231\n",
      "4838: loss=0.373, reward_mean=0.410, reward_bound=0.430, batch=231\n",
      "4839: loss=0.373, reward_mean=0.370, reward_bound=0.430, batch=231\n",
      "4841: loss=0.323, reward_mean=0.400, reward_bound=0.000, batch=40\n",
      "4842: loss=0.310, reward_mean=0.300, reward_bound=0.000, batch=70\n",
      "4843: loss=0.307, reward_mean=0.230, reward_bound=0.000, batch=93\n",
      "4844: loss=0.313, reward_mean=0.300, reward_bound=0.000, batch=123\n",
      "4845: loss=0.324, reward_mean=0.400, reward_bound=0.002, batch=155\n",
      "4846: loss=0.323, reward_mean=0.370, reward_bound=0.007, batch=178\n",
      "4847: loss=0.324, reward_mean=0.370, reward_bound=0.021, batch=194\n",
      "4848: loss=0.323, reward_mean=0.390, reward_bound=0.034, batch=202\n",
      "4849: loss=0.323, reward_mean=0.330, reward_bound=0.042, batch=204\n",
      "4850: loss=0.318, reward_mean=0.350, reward_bound=0.058, batch=206\n",
      "4851: loss=0.321, reward_mean=0.360, reward_bound=0.065, batch=210\n",
      "4852: loss=0.323, reward_mean=0.380, reward_bound=0.080, batch=200\n",
      "4853: loss=0.319, reward_mean=0.400, reward_bound=0.098, batch=206\n",
      "4854: loss=0.315, reward_mean=0.370, reward_bound=0.109, batch=204\n",
      "4855: loss=0.321, reward_mean=0.370, reward_bound=0.122, batch=200\n",
      "4856: loss=0.327, reward_mean=0.400, reward_bound=0.135, batch=200\n",
      "4857: loss=0.313, reward_mean=0.410, reward_bound=0.150, batch=193\n",
      "4858: loss=0.314, reward_mean=0.400, reward_bound=0.167, batch=191\n",
      "4859: loss=0.318, reward_mean=0.420, reward_bound=0.185, batch=184\n",
      "4860: loss=0.318, reward_mean=0.300, reward_bound=0.119, batch=199\n",
      "4861: loss=0.323, reward_mean=0.340, reward_bound=0.194, batch=209\n",
      "4862: loss=0.322, reward_mean=0.480, reward_bound=0.206, batch=191\n",
      "4863: loss=0.323, reward_mean=0.320, reward_bound=0.167, batch=201\n",
      "4864: loss=0.324, reward_mean=0.400, reward_bound=0.167, batch=210\n",
      "4865: loss=0.324, reward_mean=0.360, reward_bound=0.185, batch=213\n",
      "4866: loss=0.322, reward_mean=0.380, reward_bound=0.206, batch=218\n",
      "4867: loss=0.322, reward_mean=0.380, reward_bound=0.229, batch=192\n",
      "4868: loss=0.327, reward_mean=0.380, reward_bound=0.172, batch=204\n",
      "4869: loss=0.326, reward_mean=0.380, reward_bound=0.185, batch=211\n",
      "4870: loss=0.329, reward_mean=0.380, reward_bound=0.229, batch=216\n",
      "4871: loss=0.340, reward_mean=0.430, reward_bound=0.254, batch=180\n",
      "4872: loss=0.344, reward_mean=0.370, reward_bound=0.118, batch=196\n",
      "4873: loss=0.340, reward_mean=0.280, reward_bound=0.158, batch=207\n",
      "4874: loss=0.337, reward_mean=0.370, reward_bound=0.185, batch=211\n",
      "4875: loss=0.338, reward_mean=0.410, reward_bound=0.206, batch=211\n",
      "4876: loss=0.338, reward_mean=0.410, reward_bound=0.229, batch=217\n",
      "4877: loss=0.339, reward_mean=0.410, reward_bound=0.254, batch=217\n",
      "4878: loss=0.345, reward_mean=0.410, reward_bound=0.282, batch=177\n",
      "4879: loss=0.340, reward_mean=0.410, reward_bound=0.095, batch=194\n",
      "4880: loss=0.343, reward_mean=0.290, reward_bound=0.098, batch=204\n",
      "4881: loss=0.340, reward_mean=0.370, reward_bound=0.149, batch=213\n",
      "4882: loss=0.339, reward_mean=0.400, reward_bound=0.185, batch=214\n",
      "4883: loss=0.340, reward_mean=0.370, reward_bound=0.226, batch=220\n",
      "4884: loss=0.341, reward_mean=0.380, reward_bound=0.229, batch=222\n",
      "4885: loss=0.346, reward_mean=0.440, reward_bound=0.254, batch=223\n",
      "4886: loss=0.342, reward_mean=0.390, reward_bound=0.314, batch=183\n",
      "4887: loss=0.332, reward_mean=0.310, reward_bound=0.109, batch=197\n",
      "4888: loss=0.329, reward_mean=0.360, reward_bound=0.163, batch=208\n",
      "4889: loss=0.326, reward_mean=0.370, reward_bound=0.167, batch=212\n",
      "4890: loss=0.329, reward_mean=0.340, reward_bound=0.185, batch=217\n",
      "4891: loss=0.336, reward_mean=0.440, reward_bound=0.229, batch=221\n",
      "4892: loss=0.337, reward_mean=0.450, reward_bound=0.254, batch=224\n",
      "4893: loss=0.338, reward_mean=0.440, reward_bound=0.282, batch=225\n",
      "4894: loss=0.342, reward_mean=0.340, reward_bound=0.314, batch=216\n",
      "4895: loss=0.342, reward_mean=0.380, reward_bound=0.282, batch=219\n",
      "4896: loss=0.352, reward_mean=0.350, reward_bound=0.349, batch=157\n",
      "4897: loss=0.331, reward_mean=0.380, reward_bound=0.016, batch=180\n",
      "4898: loss=0.341, reward_mean=0.370, reward_bound=0.065, batch=195\n",
      "4899: loss=0.341, reward_mean=0.320, reward_bound=0.089, batch=208\n",
      "4900: loss=0.340, reward_mean=0.330, reward_bound=0.109, batch=214\n",
      "4901: loss=0.350, reward_mean=0.340, reward_bound=0.135, batch=217\n",
      "4902: loss=0.349, reward_mean=0.350, reward_bound=0.185, batch=209\n",
      "4903: loss=0.350, reward_mean=0.370, reward_bound=0.206, batch=209\n",
      "4904: loss=0.352, reward_mean=0.390, reward_bound=0.203, batch=216\n",
      "4905: loss=0.343, reward_mean=0.370, reward_bound=0.254, batch=216\n",
      "4906: loss=0.350, reward_mean=0.390, reward_bound=0.282, batch=214\n",
      "4907: loss=0.348, reward_mean=0.400, reward_bound=0.229, batch=219\n",
      "4908: loss=0.348, reward_mean=0.380, reward_bound=0.254, batch=222\n",
      "4909: loss=0.345, reward_mean=0.390, reward_bound=0.314, batch=215\n",
      "4910: loss=0.345, reward_mean=0.360, reward_bound=0.349, batch=207\n",
      "4911: loss=0.350, reward_mean=0.420, reward_bound=0.308, batch=215\n",
      "4912: loss=0.349, reward_mean=0.290, reward_bound=0.349, batch=217\n",
      "4913: loss=0.347, reward_mean=0.410, reward_bound=0.302, batch=222\n",
      "4914: loss=0.347, reward_mean=0.380, reward_bound=0.229, batch=225\n",
      "4915: loss=0.346, reward_mean=0.370, reward_bound=0.282, batch=226\n",
      "4916: loss=0.346, reward_mean=0.510, reward_bound=0.314, batch=227\n",
      "4917: loss=0.345, reward_mean=0.400, reward_bound=0.380, batch=229\n",
      "4918: loss=0.341, reward_mean=0.320, reward_bound=0.387, batch=150\n",
      "4919: loss=0.319, reward_mean=0.440, reward_bound=0.057, batch=175\n",
      "4920: loss=0.324, reward_mean=0.470, reward_bound=0.101, batch=192\n",
      "4921: loss=0.320, reward_mean=0.360, reward_bound=0.078, batch=204\n",
      "4922: loss=0.316, reward_mean=0.370, reward_bound=0.122, batch=209\n",
      "4923: loss=0.309, reward_mean=0.360, reward_bound=0.141, batch=216\n",
      "4924: loss=0.314, reward_mean=0.330, reward_bound=0.167, batch=214\n",
      "4925: loss=0.314, reward_mean=0.390, reward_bound=0.185, batch=214\n",
      "4926: loss=0.323, reward_mean=0.380, reward_bound=0.226, batch=220\n",
      "4927: loss=0.324, reward_mean=0.400, reward_bound=0.229, batch=216\n",
      "4928: loss=0.322, reward_mean=0.380, reward_bound=0.254, batch=211\n",
      "4929: loss=0.321, reward_mean=0.360, reward_bound=0.229, batch=217\n",
      "4930: loss=0.325, reward_mean=0.370, reward_bound=0.254, batch=218\n",
      "4931: loss=0.324, reward_mean=0.360, reward_bound=0.257, batch=222\n",
      "4932: loss=0.325, reward_mean=0.440, reward_bound=0.282, batch=221\n",
      "4933: loss=0.326, reward_mean=0.350, reward_bound=0.282, batch=224\n",
      "4934: loss=0.330, reward_mean=0.470, reward_bound=0.314, batch=207\n",
      "4935: loss=0.330, reward_mean=0.400, reward_bound=0.254, batch=213\n",
      "4936: loss=0.329, reward_mean=0.480, reward_bound=0.314, batch=217\n",
      "4937: loss=0.331, reward_mean=0.360, reward_bound=0.349, batch=212\n",
      "4938: loss=0.329, reward_mean=0.510, reward_bound=0.206, batch=219\n",
      "4939: loss=0.330, reward_mean=0.330, reward_bound=0.206, batch=222\n",
      "4940: loss=0.330, reward_mean=0.440, reward_bound=0.254, batch=224\n",
      "4941: loss=0.331, reward_mean=0.340, reward_bound=0.249, batch=227\n",
      "4942: loss=0.328, reward_mean=0.330, reward_bound=0.308, batch=229\n",
      "4943: loss=0.327, reward_mean=0.380, reward_bound=0.328, batch=230\n",
      "4944: loss=0.326, reward_mean=0.430, reward_bound=0.349, batch=229\n",
      "4945: loss=0.326, reward_mean=0.420, reward_bound=0.349, batch=229\n",
      "4946: loss=0.322, reward_mean=0.450, reward_bound=0.387, batch=204\n",
      "4947: loss=0.318, reward_mean=0.410, reward_bound=0.182, batch=213\n",
      "4948: loss=0.315, reward_mean=0.310, reward_bound=0.185, batch=217\n",
      "4949: loss=0.315, reward_mean=0.410, reward_bound=0.229, batch=219\n",
      "4950: loss=0.323, reward_mean=0.340, reward_bound=0.265, batch=223\n",
      "4951: loss=0.321, reward_mean=0.470, reward_bound=0.282, batch=222\n",
      "4952: loss=0.318, reward_mean=0.350, reward_bound=0.324, batch=225\n",
      "4953: loss=0.320, reward_mean=0.460, reward_bound=0.349, batch=225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4954: loss=0.322, reward_mean=0.350, reward_bound=0.329, batch=227\n",
      "4955: loss=0.324, reward_mean=0.450, reward_bound=0.387, batch=225\n",
      "4956: loss=0.324, reward_mean=0.320, reward_bound=0.349, batch=226\n",
      "4957: loss=0.315, reward_mean=0.430, reward_bound=0.430, batch=114\n",
      "4958: loss=0.295, reward_mean=0.470, reward_bound=0.013, batch=150\n",
      "4959: loss=0.292, reward_mean=0.410, reward_bound=0.028, batch=174\n",
      "4960: loss=0.292, reward_mean=0.380, reward_bound=0.037, batch=192\n",
      "4961: loss=0.299, reward_mean=0.380, reward_bound=0.042, batch=203\n",
      "4962: loss=0.305, reward_mean=0.360, reward_bound=0.072, batch=208\n",
      "4963: loss=0.307, reward_mean=0.350, reward_bound=0.089, batch=209\n",
      "4964: loss=0.315, reward_mean=0.400, reward_bound=0.135, batch=208\n",
      "4965: loss=0.313, reward_mean=0.360, reward_bound=0.150, batch=214\n",
      "4966: loss=0.314, reward_mean=0.470, reward_bound=0.183, batch=220\n",
      "4967: loss=0.312, reward_mean=0.390, reward_bound=0.185, batch=223\n",
      "4968: loss=0.315, reward_mean=0.430, reward_bound=0.206, batch=219\n",
      "4969: loss=0.317, reward_mean=0.390, reward_bound=0.229, batch=220\n",
      "4970: loss=0.316, reward_mean=0.380, reward_bound=0.254, batch=209\n",
      "4971: loss=0.319, reward_mean=0.380, reward_bound=0.282, batch=190\n",
      "4972: loss=0.318, reward_mean=0.400, reward_bound=0.206, batch=204\n",
      "4973: loss=0.318, reward_mean=0.400, reward_bound=0.204, batch=213\n",
      "4974: loss=0.314, reward_mean=0.410, reward_bound=0.206, batch=218\n",
      "4975: loss=0.318, reward_mean=0.350, reward_bound=0.254, batch=216\n",
      "4976: loss=0.317, reward_mean=0.330, reward_bound=0.282, batch=220\n",
      "4977: loss=0.313, reward_mean=0.420, reward_bound=0.314, batch=209\n",
      "4978: loss=0.312, reward_mean=0.400, reward_bound=0.206, batch=215\n",
      "4979: loss=0.312, reward_mean=0.380, reward_bound=0.234, batch=220\n",
      "4980: loss=0.314, reward_mean=0.400, reward_bound=0.282, batch=223\n",
      "4981: loss=0.313, reward_mean=0.330, reward_bound=0.261, batch=226\n",
      "4982: loss=0.313, reward_mean=0.330, reward_bound=0.282, batch=227\n",
      "4983: loss=0.313, reward_mean=0.410, reward_bound=0.314, batch=227\n",
      "4984: loss=0.312, reward_mean=0.390, reward_bound=0.342, batch=229\n",
      "4985: loss=0.302, reward_mean=0.360, reward_bound=0.349, batch=198\n",
      "4986: loss=0.300, reward_mean=0.420, reward_bound=0.234, batch=208\n",
      "4987: loss=0.301, reward_mean=0.420, reward_bound=0.257, batch=215\n",
      "4988: loss=0.298, reward_mean=0.420, reward_bound=0.289, batch=220\n",
      "4989: loss=0.298, reward_mean=0.400, reward_bound=0.314, batch=217\n",
      "4990: loss=0.299, reward_mean=0.380, reward_bound=0.277, batch=222\n",
      "4991: loss=0.298, reward_mean=0.400, reward_bound=0.282, batch=223\n",
      "4992: loss=0.293, reward_mean=0.410, reward_bound=0.314, batch=225\n",
      "4993: loss=0.296, reward_mean=0.360, reward_bound=0.349, batch=222\n",
      "4994: loss=0.298, reward_mean=0.450, reward_bound=0.272, batch=225\n",
      "4995: loss=0.295, reward_mean=0.450, reward_bound=0.321, batch=227\n",
      "4996: loss=0.293, reward_mean=0.520, reward_bound=0.349, batch=228\n",
      "4997: loss=0.301, reward_mean=0.400, reward_bound=0.387, batch=185\n",
      "4998: loss=0.301, reward_mean=0.380, reward_bound=0.122, batch=197\n",
      "4999: loss=0.304, reward_mean=0.390, reward_bound=0.163, batch=208\n",
      "5000: loss=0.302, reward_mean=0.450, reward_bound=0.185, batch=213\n",
      "5001: loss=0.303, reward_mean=0.480, reward_bound=0.229, batch=215\n",
      "5002: loss=0.304, reward_mean=0.300, reward_bound=0.194, batch=220\n",
      "5003: loss=0.303, reward_mean=0.390, reward_bound=0.254, batch=219\n",
      "5004: loss=0.305, reward_mean=0.450, reward_bound=0.282, batch=216\n",
      "5005: loss=0.304, reward_mean=0.360, reward_bound=0.268, batch=221\n",
      "5006: loss=0.301, reward_mean=0.430, reward_bound=0.314, batch=215\n",
      "5007: loss=0.299, reward_mean=0.430, reward_bound=0.289, batch=220\n",
      "5008: loss=0.300, reward_mean=0.400, reward_bound=0.314, batch=222\n",
      "5009: loss=0.304, reward_mean=0.460, reward_bound=0.349, batch=211\n",
      "5010: loss=0.303, reward_mean=0.410, reward_bound=0.229, batch=217\n",
      "5011: loss=0.300, reward_mean=0.370, reward_bound=0.249, batch=222\n",
      "5012: loss=0.301, reward_mean=0.390, reward_bound=0.254, batch=224\n",
      "5013: loss=0.299, reward_mean=0.330, reward_bound=0.314, batch=224\n",
      "5014: loss=0.302, reward_mean=0.450, reward_bound=0.349, batch=224\n",
      "5015: loss=0.299, reward_mean=0.340, reward_bound=0.345, batch=227\n",
      "5016: loss=0.299, reward_mean=0.440, reward_bound=0.380, batch=229\n",
      "5017: loss=0.298, reward_mean=0.420, reward_bound=0.343, batch=230\n",
      "5018: loss=0.302, reward_mean=0.400, reward_bound=0.387, batch=211\n",
      "5019: loss=0.304, reward_mean=0.540, reward_bound=0.314, batch=217\n",
      "5020: loss=0.302, reward_mean=0.390, reward_bound=0.249, batch=222\n",
      "5021: loss=0.303, reward_mean=0.440, reward_bound=0.263, batch=225\n",
      "5022: loss=0.303, reward_mean=0.420, reward_bound=0.289, batch=227\n",
      "5023: loss=0.302, reward_mean=0.410, reward_bound=0.387, batch=227\n",
      "5024: loss=0.308, reward_mean=0.370, reward_bound=0.366, batch=229\n",
      "5025: loss=0.308, reward_mean=0.350, reward_bound=0.364, batch=230\n",
      "5026: loss=0.305, reward_mean=0.390, reward_bound=0.387, batch=230\n",
      "5027: loss=0.307, reward_mean=0.390, reward_bound=0.430, batch=174\n",
      "5028: loss=0.304, reward_mean=0.380, reward_bound=0.079, batch=192\n",
      "5029: loss=0.298, reward_mean=0.390, reward_bound=0.122, batch=200\n",
      "5030: loss=0.300, reward_mean=0.410, reward_bound=0.162, batch=210\n",
      "5031: loss=0.308, reward_mean=0.450, reward_bound=0.185, batch=213\n",
      "5032: loss=0.306, reward_mean=0.440, reward_bound=0.206, batch=213\n",
      "5033: loss=0.305, reward_mean=0.460, reward_bound=0.229, batch=216\n",
      "5034: loss=0.302, reward_mean=0.450, reward_bound=0.254, batch=217\n",
      "5035: loss=0.301, reward_mean=0.320, reward_bound=0.254, batch=220\n",
      "5036: loss=0.305, reward_mean=0.440, reward_bound=0.282, batch=215\n",
      "5037: loss=0.304, reward_mean=0.410, reward_bound=0.289, batch=220\n",
      "5038: loss=0.305, reward_mean=0.500, reward_bound=0.314, batch=217\n",
      "5039: loss=0.304, reward_mean=0.450, reward_bound=0.314, batch=221\n",
      "5040: loss=0.307, reward_mean=0.430, reward_bound=0.349, batch=215\n",
      "5041: loss=0.302, reward_mean=0.420, reward_bound=0.260, batch=220\n",
      "5042: loss=0.305, reward_mean=0.450, reward_bound=0.314, batch=222\n",
      "5043: loss=0.304, reward_mean=0.400, reward_bound=0.282, batch=224\n",
      "5044: loss=0.304, reward_mean=0.390, reward_bound=0.349, batch=223\n",
      "5045: loss=0.307, reward_mean=0.380, reward_bound=0.387, batch=211\n",
      "5046: loss=0.307, reward_mean=0.420, reward_bound=0.314, batch=217\n",
      "5047: loss=0.305, reward_mean=0.380, reward_bound=0.342, batch=222\n",
      "5048: loss=0.305, reward_mean=0.430, reward_bound=0.349, batch=220\n",
      "5049: loss=0.306, reward_mean=0.430, reward_bound=0.314, batch=223\n",
      "5050: loss=0.307, reward_mean=0.340, reward_bound=0.282, batch=225\n",
      "5051: loss=0.308, reward_mean=0.420, reward_bound=0.356, batch=227\n",
      "5052: loss=0.309, reward_mean=0.370, reward_bound=0.387, batch=224\n",
      "5053: loss=0.308, reward_mean=0.370, reward_bound=0.349, batch=226\n",
      "5054: loss=0.307, reward_mean=0.450, reward_bound=0.430, batch=209\n",
      "5055: loss=0.308, reward_mean=0.500, reward_bound=0.282, batch=215\n",
      "5056: loss=0.309, reward_mean=0.360, reward_bound=0.282, batch=218\n",
      "5057: loss=0.310, reward_mean=0.400, reward_bound=0.286, batch=222\n",
      "5058: loss=0.309, reward_mean=0.380, reward_bound=0.314, batch=219\n",
      "5059: loss=0.307, reward_mean=0.390, reward_bound=0.349, batch=220\n",
      "5060: loss=0.307, reward_mean=0.410, reward_bound=0.254, batch=223\n",
      "5061: loss=0.312, reward_mean=0.280, reward_bound=0.301, batch=226\n",
      "5062: loss=0.307, reward_mean=0.380, reward_bound=0.349, batch=227\n",
      "5063: loss=0.310, reward_mean=0.320, reward_bound=0.387, batch=225\n",
      "5064: loss=0.306, reward_mean=0.420, reward_bound=0.430, batch=221\n",
      "5065: loss=0.308, reward_mean=0.420, reward_bound=0.349, batch=224\n",
      "5066: loss=0.306, reward_mean=0.440, reward_bound=0.282, batch=226\n",
      "5067: loss=0.307, reward_mean=0.360, reward_bound=0.409, batch=228\n",
      "5068: loss=0.307, reward_mean=0.390, reward_bound=0.357, batch=229\n",
      "5069: loss=0.306, reward_mean=0.370, reward_bound=0.405, batch=230\n",
      "5070: loss=0.306, reward_mean=0.370, reward_bound=0.418, batch=231\n",
      "5071: loss=0.303, reward_mean=0.440, reward_bound=0.430, batch=227\n",
      "5072: loss=0.302, reward_mean=0.270, reward_bound=0.373, batch=229\n",
      "5073: loss=0.305, reward_mean=0.430, reward_bound=0.430, batch=229\n",
      "5074: loss=0.305, reward_mean=0.440, reward_bound=0.381, batch=230\n",
      "5075: loss=0.305, reward_mean=0.410, reward_bound=0.406, batch=231\n",
      "5076: loss=0.309, reward_mean=0.350, reward_bound=0.430, batch=230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5077: loss=0.293, reward_mean=0.410, reward_bound=0.478, batch=90\n",
      "5078: loss=0.257, reward_mean=0.380, reward_bound=0.000, batch=128\n",
      "5079: loss=0.258, reward_mean=0.460, reward_bound=0.007, batch=159\n",
      "5080: loss=0.260, reward_mean=0.430, reward_bound=0.025, batch=179\n",
      "5081: loss=0.264, reward_mean=0.430, reward_bound=0.047, batch=193\n",
      "5082: loss=0.257, reward_mean=0.330, reward_bound=0.072, batch=203\n",
      "5083: loss=0.263, reward_mean=0.330, reward_bound=0.089, batch=209\n",
      "5084: loss=0.262, reward_mean=0.380, reward_bound=0.122, batch=212\n",
      "5085: loss=0.265, reward_mean=0.420, reward_bound=0.150, batch=208\n",
      "5086: loss=0.272, reward_mean=0.440, reward_bound=0.167, batch=210\n",
      "5087: loss=0.277, reward_mean=0.370, reward_bound=0.185, batch=205\n",
      "5088: loss=0.274, reward_mean=0.450, reward_bound=0.175, batch=213\n",
      "5089: loss=0.281, reward_mean=0.410, reward_bound=0.206, batch=203\n",
      "5090: loss=0.282, reward_mean=0.440, reward_bound=0.198, batch=212\n",
      "5091: loss=0.279, reward_mean=0.390, reward_bound=0.206, batch=219\n",
      "5092: loss=0.285, reward_mean=0.490, reward_bound=0.229, batch=205\n",
      "5093: loss=0.282, reward_mean=0.380, reward_bound=0.229, batch=212\n",
      "5094: loss=0.282, reward_mean=0.400, reward_bound=0.191, batch=218\n",
      "5095: loss=0.280, reward_mean=0.450, reward_bound=0.254, batch=208\n",
      "5096: loss=0.283, reward_mean=0.430, reward_bound=0.257, batch=215\n",
      "5097: loss=0.281, reward_mean=0.300, reward_bound=0.210, batch=220\n",
      "5098: loss=0.284, reward_mean=0.420, reward_bound=0.254, batch=222\n",
      "5099: loss=0.285, reward_mean=0.410, reward_bound=0.282, batch=199\n",
      "5100: loss=0.281, reward_mean=0.400, reward_bound=0.135, batch=208\n",
      "5101: loss=0.284, reward_mean=0.510, reward_bound=0.229, batch=214\n",
      "5102: loss=0.286, reward_mean=0.400, reward_bound=0.254, batch=219\n",
      "5103: loss=0.288, reward_mean=0.480, reward_bound=0.282, batch=222\n",
      "5104: loss=0.287, reward_mean=0.300, reward_bound=0.282, batch=224\n",
      "5105: loss=0.287, reward_mean=0.350, reward_bound=0.311, batch=227\n",
      "5106: loss=0.293, reward_mean=0.350, reward_bound=0.314, batch=200\n",
      "5107: loss=0.293, reward_mean=0.400, reward_bound=0.185, batch=209\n",
      "5108: loss=0.290, reward_mean=0.420, reward_bound=0.194, batch=216\n",
      "5109: loss=0.293, reward_mean=0.450, reward_bound=0.206, batch=220\n",
      "5110: loss=0.296, reward_mean=0.400, reward_bound=0.282, batch=218\n",
      "5111: loss=0.294, reward_mean=0.400, reward_bound=0.314, batch=218\n",
      "5112: loss=0.293, reward_mean=0.440, reward_bound=0.282, batch=221\n",
      "5113: loss=0.292, reward_mean=0.480, reward_bound=0.254, batch=224\n",
      "5114: loss=0.288, reward_mean=0.350, reward_bound=0.349, batch=186\n",
      "5115: loss=0.284, reward_mean=0.340, reward_bound=0.135, batch=199\n",
      "5116: loss=0.285, reward_mean=0.410, reward_bound=0.167, batch=207\n",
      "5117: loss=0.289, reward_mean=0.420, reward_bound=0.182, batch=215\n",
      "5118: loss=0.290, reward_mean=0.390, reward_bound=0.206, batch=219\n",
      "5119: loss=0.293, reward_mean=0.440, reward_bound=0.254, batch=222\n",
      "5120: loss=0.293, reward_mean=0.280, reward_bound=0.229, batch=224\n",
      "5121: loss=0.288, reward_mean=0.400, reward_bound=0.282, batch=219\n",
      "5122: loss=0.285, reward_mean=0.410, reward_bound=0.314, batch=216\n",
      "5123: loss=0.286, reward_mean=0.390, reward_bound=0.254, batch=220\n",
      "5124: loss=0.288, reward_mean=0.340, reward_bound=0.222, batch=224\n",
      "5125: loss=0.284, reward_mean=0.430, reward_bound=0.314, batch=226\n",
      "5126: loss=0.291, reward_mean=0.420, reward_bound=0.349, batch=218\n",
      "5127: loss=0.292, reward_mean=0.340, reward_bound=0.229, batch=221\n",
      "5128: loss=0.292, reward_mean=0.380, reward_bound=0.349, batch=224\n",
      "5129: loss=0.292, reward_mean=0.330, reward_bound=0.349, batch=225\n",
      "5130: loss=0.296, reward_mean=0.500, reward_bound=0.387, batch=180\n",
      "5131: loss=0.297, reward_mean=0.510, reward_bound=0.150, batch=195\n",
      "5132: loss=0.289, reward_mean=0.330, reward_bound=0.141, batch=206\n",
      "5133: loss=0.286, reward_mean=0.440, reward_bound=0.206, batch=211\n",
      "5134: loss=0.287, reward_mean=0.360, reward_bound=0.206, batch=217\n",
      "5135: loss=0.286, reward_mean=0.370, reward_bound=0.182, batch=222\n",
      "5136: loss=0.290, reward_mean=0.460, reward_bound=0.229, batch=224\n",
      "5137: loss=0.296, reward_mean=0.450, reward_bound=0.254, batch=222\n",
      "5138: loss=0.293, reward_mean=0.380, reward_bound=0.282, batch=216\n",
      "5139: loss=0.293, reward_mean=0.440, reward_bound=0.256, batch=221\n",
      "5140: loss=0.295, reward_mean=0.310, reward_bound=0.254, batch=224\n",
      "5141: loss=0.295, reward_mean=0.380, reward_bound=0.314, batch=221\n",
      "5142: loss=0.293, reward_mean=0.430, reward_bound=0.314, batch=224\n",
      "5143: loss=0.293, reward_mean=0.470, reward_bound=0.349, batch=220\n",
      "5144: loss=0.292, reward_mean=0.420, reward_bound=0.288, batch=224\n",
      "5145: loss=0.290, reward_mean=0.390, reward_bound=0.311, batch=227\n",
      "5146: loss=0.289, reward_mean=0.460, reward_bound=0.314, batch=228\n",
      "5147: loss=0.290, reward_mean=0.360, reward_bound=0.349, batch=226\n",
      "5148: loss=0.297, reward_mean=0.330, reward_bound=0.387, batch=211\n",
      "5149: loss=0.297, reward_mean=0.400, reward_bound=0.254, batch=215\n",
      "5150: loss=0.295, reward_mean=0.460, reward_bound=0.314, batch=218\n",
      "5151: loss=0.298, reward_mean=0.420, reward_bound=0.185, batch=221\n",
      "5152: loss=0.296, reward_mean=0.340, reward_bound=0.282, batch=224\n",
      "5153: loss=0.296, reward_mean=0.370, reward_bound=0.349, batch=224\n",
      "5154: loss=0.294, reward_mean=0.450, reward_bound=0.387, batch=219\n",
      "5155: loss=0.295, reward_mean=0.450, reward_bound=0.430, batch=170\n",
      "5156: loss=0.281, reward_mean=0.510, reward_bound=0.135, batch=188\n",
      "5157: loss=0.277, reward_mean=0.400, reward_bound=0.150, batch=200\n",
      "5158: loss=0.283, reward_mean=0.430, reward_bound=0.185, batch=208\n",
      "5159: loss=0.286, reward_mean=0.490, reward_bound=0.208, batch=215\n",
      "5160: loss=0.293, reward_mean=0.340, reward_bound=0.229, batch=218\n",
      "5161: loss=0.305, reward_mean=0.360, reward_bound=0.254, batch=210\n",
      "5162: loss=0.297, reward_mean=0.430, reward_bound=0.282, batch=208\n",
      "5163: loss=0.296, reward_mean=0.340, reward_bound=0.282, batch=213\n",
      "5164: loss=0.301, reward_mean=0.320, reward_bound=0.211, batch=219\n",
      "5165: loss=0.297, reward_mean=0.430, reward_bound=0.254, batch=222\n",
      "5166: loss=0.292, reward_mean=0.440, reward_bound=0.292, batch=225\n",
      "5167: loss=0.292, reward_mean=0.360, reward_bound=0.282, batch=226\n",
      "5168: loss=0.295, reward_mean=0.430, reward_bound=0.314, batch=216\n",
      "5169: loss=0.299, reward_mean=0.380, reward_bound=0.244, batch=221\n",
      "5170: loss=0.295, reward_mean=0.360, reward_bound=0.282, batch=224\n",
      "5171: loss=0.293, reward_mean=0.460, reward_bound=0.314, batch=225\n",
      "5172: loss=0.296, reward_mean=0.340, reward_bound=0.349, batch=212\n",
      "5173: loss=0.303, reward_mean=0.490, reward_bound=0.282, batch=216\n",
      "5174: loss=0.299, reward_mean=0.450, reward_bound=0.314, batch=219\n",
      "5175: loss=0.296, reward_mean=0.460, reward_bound=0.387, batch=207\n",
      "5176: loss=0.293, reward_mean=0.400, reward_bound=0.277, batch=215\n",
      "5177: loss=0.300, reward_mean=0.530, reward_bound=0.349, batch=217\n",
      "5178: loss=0.302, reward_mean=0.430, reward_bound=0.387, batch=217\n",
      "5179: loss=0.302, reward_mean=0.350, reward_bound=0.163, batch=222\n",
      "5180: loss=0.297, reward_mean=0.410, reward_bound=0.314, batch=224\n",
      "5181: loss=0.298, reward_mean=0.450, reward_bound=0.349, batch=226\n",
      "5182: loss=0.298, reward_mean=0.430, reward_bound=0.335, batch=228\n",
      "5183: loss=0.298, reward_mean=0.430, reward_bound=0.387, batch=224\n",
      "5184: loss=0.299, reward_mean=0.340, reward_bound=0.311, batch=227\n",
      "5185: loss=0.298, reward_mean=0.430, reward_bound=0.387, batch=228\n",
      "5186: loss=0.298, reward_mean=0.440, reward_bound=0.387, batch=228\n",
      "5187: loss=0.298, reward_mean=0.500, reward_bound=0.357, batch=229\n",
      "5188: loss=0.298, reward_mean=0.400, reward_bound=0.387, batch=229\n",
      "5189: loss=0.297, reward_mean=0.420, reward_bound=0.405, batch=230\n",
      "5190: loss=0.294, reward_mean=0.430, reward_bound=0.430, batch=206\n",
      "5191: loss=0.295, reward_mean=0.410, reward_bound=0.206, batch=213\n",
      "5192: loss=0.303, reward_mean=0.380, reward_bound=0.254, batch=216\n",
      "5193: loss=0.301, reward_mean=0.430, reward_bound=0.268, batch=221\n",
      "5194: loss=0.297, reward_mean=0.400, reward_bound=0.349, batch=217\n",
      "5195: loss=0.297, reward_mean=0.480, reward_bound=0.342, batch=222\n",
      "5196: loss=0.296, reward_mean=0.370, reward_bound=0.387, batch=221\n",
      "5197: loss=0.293, reward_mean=0.420, reward_bound=0.254, batch=224\n",
      "5198: loss=0.291, reward_mean=0.300, reward_bound=0.280, batch=227\n",
      "5199: loss=0.296, reward_mean=0.500, reward_bound=0.314, batch=227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200: loss=0.297, reward_mean=0.460, reward_bound=0.380, batch=229\n",
      "5201: loss=0.295, reward_mean=0.360, reward_bound=0.387, batch=225\n",
      "5202: loss=0.294, reward_mean=0.280, reward_bound=0.282, batch=226\n",
      "5203: loss=0.294, reward_mean=0.400, reward_bound=0.430, batch=222\n",
      "5204: loss=0.295, reward_mean=0.370, reward_bound=0.387, batch=224\n",
      "5205: loss=0.297, reward_mean=0.410, reward_bound=0.349, batch=226\n",
      "5206: loss=0.296, reward_mean=0.430, reward_bound=0.430, batch=226\n",
      "5207: loss=0.297, reward_mean=0.420, reward_bound=0.387, batch=227\n",
      "5208: loss=0.296, reward_mean=0.450, reward_bound=0.387, batch=228\n",
      "5209: loss=0.298, reward_mean=0.310, reward_bound=0.357, batch=229\n",
      "5210: loss=0.297, reward_mean=0.410, reward_bound=0.405, batch=230\n",
      "5211: loss=0.297, reward_mean=0.360, reward_bound=0.406, batch=231\n",
      "5212: loss=0.295, reward_mean=0.440, reward_bound=0.430, batch=231\n",
      "5213: loss=0.295, reward_mean=0.350, reward_bound=0.478, batch=140\n",
      "5214: loss=0.291, reward_mean=0.370, reward_bound=0.021, batch=168\n",
      "5215: loss=0.305, reward_mean=0.390, reward_bound=0.058, batch=184\n",
      "5216: loss=0.287, reward_mean=0.400, reward_bound=0.079, batch=199\n",
      "5217: loss=0.294, reward_mean=0.490, reward_bound=0.114, batch=209\n",
      "5218: loss=0.282, reward_mean=0.350, reward_bound=0.141, batch=216\n",
      "5219: loss=0.284, reward_mean=0.380, reward_bound=0.167, batch=214\n",
      "5220: loss=0.290, reward_mean=0.430, reward_bound=0.185, batch=216\n",
      "5221: loss=0.291, reward_mean=0.350, reward_bound=0.206, batch=215\n",
      "5222: loss=0.287, reward_mean=0.480, reward_bound=0.229, batch=216\n",
      "5223: loss=0.292, reward_mean=0.440, reward_bound=0.254, batch=211\n",
      "5224: loss=0.300, reward_mean=0.390, reward_bound=0.229, batch=217\n",
      "5225: loss=0.301, reward_mean=0.370, reward_bound=0.282, batch=204\n",
      "5226: loss=0.299, reward_mean=0.390, reward_bound=0.280, batch=213\n",
      "5227: loss=0.297, reward_mean=0.480, reward_bound=0.271, batch=219\n",
      "5228: loss=0.297, reward_mean=0.440, reward_bound=0.282, batch=222\n",
      "5229: loss=0.295, reward_mean=0.470, reward_bound=0.314, batch=206\n",
      "5230: loss=0.295, reward_mean=0.420, reward_bound=0.241, batch=214\n",
      "5231: loss=0.298, reward_mean=0.440, reward_bound=0.311, batch=220\n",
      "5232: loss=0.297, reward_mean=0.350, reward_bound=0.304, batch=224\n",
      "5233: loss=0.294, reward_mean=0.330, reward_bound=0.308, batch=227\n",
      "5234: loss=0.296, reward_mean=0.390, reward_bound=0.314, batch=224\n",
      "5235: loss=0.300, reward_mean=0.300, reward_bound=0.349, batch=211\n",
      "5236: loss=0.298, reward_mean=0.370, reward_bound=0.185, batch=216\n",
      "5237: loss=0.303, reward_mean=0.390, reward_bound=0.284, batch=221\n",
      "5238: loss=0.301, reward_mean=0.410, reward_bound=0.254, batch=224\n",
      "5239: loss=0.303, reward_mean=0.400, reward_bound=0.314, batch=225\n",
      "5240: loss=0.296, reward_mean=0.430, reward_bound=0.349, batch=225\n",
      "5241: loss=0.296, reward_mean=0.330, reward_bound=0.329, batch=227\n",
      "5242: loss=0.301, reward_mean=0.390, reward_bound=0.387, batch=196\n",
      "5243: loss=0.302, reward_mean=0.460, reward_bound=0.167, batch=205\n",
      "5244: loss=0.303, reward_mean=0.450, reward_bound=0.206, batch=212\n",
      "5245: loss=0.305, reward_mean=0.390, reward_bound=0.229, batch=215\n",
      "5246: loss=0.302, reward_mean=0.400, reward_bound=0.254, batch=219\n",
      "5247: loss=0.300, reward_mean=0.360, reward_bound=0.295, batch=223\n",
      "5248: loss=0.298, reward_mean=0.330, reward_bound=0.271, batch=226\n",
      "5249: loss=0.301, reward_mean=0.400, reward_bound=0.331, batch=228\n",
      "5250: loss=0.305, reward_mean=0.350, reward_bound=0.349, batch=220\n",
      "5251: loss=0.306, reward_mean=0.270, reward_bound=0.320, batch=224\n",
      "5252: loss=0.306, reward_mean=0.430, reward_bound=0.345, batch=227\n",
      "5253: loss=0.303, reward_mean=0.430, reward_bound=0.380, batch=229\n",
      "5254: loss=0.301, reward_mean=0.370, reward_bound=0.387, batch=216\n",
      "5255: loss=0.298, reward_mean=0.430, reward_bound=0.241, batch=221\n",
      "5256: loss=0.304, reward_mean=0.400, reward_bound=0.314, batch=222\n",
      "5257: loss=0.303, reward_mean=0.420, reward_bound=0.324, batch=225\n",
      "5258: loss=0.302, reward_mean=0.350, reward_bound=0.349, batch=226\n",
      "5259: loss=0.304, reward_mean=0.370, reward_bound=0.349, batch=227\n",
      "5260: loss=0.306, reward_mean=0.350, reward_bound=0.342, batch=229\n",
      "5261: loss=0.300, reward_mean=0.340, reward_bound=0.387, batch=227\n",
      "5262: loss=0.300, reward_mean=0.480, reward_bound=0.422, batch=229\n",
      "5263: loss=0.308, reward_mean=0.460, reward_bound=0.430, batch=189\n",
      "5264: loss=0.304, reward_mean=0.440, reward_bound=0.135, batch=201\n",
      "5265: loss=0.299, reward_mean=0.460, reward_bound=0.185, batch=208\n",
      "5266: loss=0.297, reward_mean=0.470, reward_bound=0.231, batch=215\n",
      "5267: loss=0.295, reward_mean=0.360, reward_bound=0.254, batch=217\n",
      "5268: loss=0.296, reward_mean=0.350, reward_bound=0.277, batch=222\n",
      "5269: loss=0.297, reward_mean=0.450, reward_bound=0.282, batch=219\n",
      "5270: loss=0.296, reward_mean=0.390, reward_bound=0.295, batch=223\n",
      "5271: loss=0.305, reward_mean=0.460, reward_bound=0.314, batch=221\n",
      "5272: loss=0.305, reward_mean=0.400, reward_bound=0.349, batch=214\n",
      "5273: loss=0.301, reward_mean=0.410, reward_bound=0.229, batch=219\n",
      "5274: loss=0.300, reward_mean=0.360, reward_bound=0.203, batch=223\n",
      "5275: loss=0.300, reward_mean=0.410, reward_bound=0.254, batch=223\n",
      "5276: loss=0.300, reward_mean=0.350, reward_bound=0.314, batch=224\n",
      "5277: loss=0.301, reward_mean=0.360, reward_bound=0.349, batch=224\n",
      "5278: loss=0.300, reward_mean=0.410, reward_bound=0.349, batch=226\n",
      "5279: loss=0.305, reward_mean=0.360, reward_bound=0.387, batch=218\n",
      "5280: loss=0.304, reward_mean=0.320, reward_bound=0.286, batch=222\n",
      "5281: loss=0.303, reward_mean=0.410, reward_bound=0.314, batch=224\n",
      "5282: loss=0.300, reward_mean=0.370, reward_bound=0.349, batch=226\n",
      "5283: loss=0.303, reward_mean=0.380, reward_bound=0.409, batch=228\n",
      "5284: loss=0.302, reward_mean=0.420, reward_bound=0.392, batch=229\n",
      "5285: loss=0.309, reward_mean=0.340, reward_bound=0.430, batch=205\n",
      "5286: loss=0.313, reward_mean=0.380, reward_bound=0.153, batch=213\n",
      "5287: loss=0.314, reward_mean=0.360, reward_bound=0.244, batch=219\n",
      "5288: loss=0.308, reward_mean=0.340, reward_bound=0.265, batch=223\n",
      "5289: loss=0.305, reward_mean=0.440, reward_bound=0.301, batch=226\n",
      "5290: loss=0.310, reward_mean=0.290, reward_bound=0.314, batch=222\n",
      "5291: loss=0.304, reward_mean=0.380, reward_bound=0.349, batch=221\n",
      "5292: loss=0.303, reward_mean=0.410, reward_bound=0.349, batch=224\n",
      "5293: loss=0.301, reward_mean=0.450, reward_bound=0.387, batch=225\n",
      "5294: loss=0.306, reward_mean=0.370, reward_bound=0.430, batch=217\n",
      "5295: loss=0.302, reward_mean=0.350, reward_bound=0.282, batch=221\n",
      "5296: loss=0.305, reward_mean=0.350, reward_bound=0.349, batch=223\n",
      "5297: loss=0.306, reward_mean=0.370, reward_bound=0.387, batch=224\n",
      "5298: loss=0.308, reward_mean=0.390, reward_bound=0.349, batch=226\n",
      "5299: loss=0.308, reward_mean=0.410, reward_bound=0.368, batch=228\n",
      "5300: loss=0.308, reward_mean=0.360, reward_bound=0.387, batch=227\n",
      "5301: loss=0.306, reward_mean=0.320, reward_bound=0.430, batch=221\n",
      "5302: loss=0.307, reward_mean=0.420, reward_bound=0.349, batch=223\n",
      "5303: loss=0.308, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "5304: loss=0.308, reward_mean=0.340, reward_bound=0.282, batch=226\n",
      "5305: loss=0.307, reward_mean=0.340, reward_bound=0.387, batch=223\n",
      "5306: loss=0.306, reward_mean=0.390, reward_bound=0.314, batch=225\n",
      "5307: loss=0.305, reward_mean=0.310, reward_bound=0.329, batch=227\n",
      "5308: loss=0.305, reward_mean=0.400, reward_bound=0.387, batch=228\n",
      "5309: loss=0.306, reward_mean=0.370, reward_bound=0.430, batch=227\n",
      "5310: loss=0.306, reward_mean=0.380, reward_bound=0.460, batch=229\n",
      "5311: loss=0.305, reward_mean=0.360, reward_bound=0.450, batch=230\n",
      "5312: loss=0.305, reward_mean=0.360, reward_bound=0.478, batch=177\n",
      "5313: loss=0.305, reward_mean=0.340, reward_bound=0.072, batch=193\n",
      "5314: loss=0.306, reward_mean=0.440, reward_bound=0.144, batch=205\n",
      "5315: loss=0.309, reward_mean=0.400, reward_bound=0.185, batch=211\n",
      "5316: loss=0.310, reward_mean=0.380, reward_bound=0.206, batch=216\n",
      "5317: loss=0.309, reward_mean=0.390, reward_bound=0.229, batch=217\n",
      "5318: loss=0.308, reward_mean=0.430, reward_bound=0.277, batch=222\n",
      "5319: loss=0.310, reward_mean=0.430, reward_bound=0.282, batch=217\n",
      "5320: loss=0.309, reward_mean=0.410, reward_bound=0.308, batch=222\n",
      "5321: loss=0.319, reward_mean=0.340, reward_bound=0.314, batch=216\n",
      "5322: loss=0.318, reward_mean=0.400, reward_bound=0.254, batch=220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5323: loss=0.319, reward_mean=0.390, reward_bound=0.247, batch=224\n",
      "5324: loss=0.318, reward_mean=0.340, reward_bound=0.282, batch=224\n",
      "5325: loss=0.318, reward_mean=0.350, reward_bound=0.349, batch=213\n",
      "5326: loss=0.316, reward_mean=0.350, reward_bound=0.271, batch=219\n",
      "5327: loss=0.318, reward_mean=0.390, reward_bound=0.239, batch=223\n",
      "5328: loss=0.315, reward_mean=0.340, reward_bound=0.282, batch=224\n",
      "5329: loss=0.316, reward_mean=0.420, reward_bound=0.311, batch=227\n",
      "5330: loss=0.318, reward_mean=0.360, reward_bound=0.314, batch=228\n",
      "5331: loss=0.315, reward_mean=0.430, reward_bound=0.349, batch=227\n",
      "5332: loss=0.315, reward_mean=0.420, reward_bound=0.387, batch=215\n",
      "5333: loss=0.317, reward_mean=0.490, reward_bound=0.282, batch=219\n",
      "5334: loss=0.316, reward_mean=0.430, reward_bound=0.314, batch=222\n",
      "5335: loss=0.316, reward_mean=0.320, reward_bound=0.292, batch=225\n",
      "5336: loss=0.313, reward_mean=0.280, reward_bound=0.321, batch=227\n",
      "5337: loss=0.313, reward_mean=0.400, reward_bound=0.349, batch=225\n",
      "5338: loss=0.314, reward_mean=0.370, reward_bound=0.387, batch=226\n",
      "5339: loss=0.309, reward_mean=0.360, reward_bound=0.430, batch=205\n",
      "5340: loss=0.311, reward_mean=0.430, reward_bound=0.229, batch=212\n",
      "5341: loss=0.309, reward_mean=0.490, reward_bound=0.254, batch=216\n",
      "5342: loss=0.308, reward_mean=0.340, reward_bound=0.282, batch=218\n",
      "5343: loss=0.306, reward_mean=0.300, reward_bound=0.206, batch=221\n",
      "5344: loss=0.308, reward_mean=0.390, reward_bound=0.282, batch=224\n",
      "5345: loss=0.307, reward_mean=0.410, reward_bound=0.314, batch=226\n",
      "5346: loss=0.303, reward_mean=0.410, reward_bound=0.349, batch=227\n",
      "5347: loss=0.303, reward_mean=0.440, reward_bound=0.387, batch=223\n",
      "5348: loss=0.305, reward_mean=0.360, reward_bound=0.387, batch=225\n",
      "5349: loss=0.306, reward_mean=0.390, reward_bound=0.329, batch=227\n",
      "5350: loss=0.307, reward_mean=0.330, reward_bound=0.314, batch=228\n",
      "5351: loss=0.303, reward_mean=0.390, reward_bound=0.392, batch=229\n",
      "5352: loss=0.305, reward_mean=0.340, reward_bound=0.430, batch=221\n",
      "5353: loss=0.306, reward_mean=0.320, reward_bound=0.314, batch=224\n",
      "5354: loss=0.305, reward_mean=0.460, reward_bound=0.426, batch=227\n",
      "5355: loss=0.304, reward_mean=0.370, reward_bound=0.380, batch=229\n",
      "5356: loss=0.304, reward_mean=0.380, reward_bound=0.387, batch=229\n",
      "5357: loss=0.308, reward_mean=0.360, reward_bound=0.430, batch=226\n",
      "5358: loss=0.310, reward_mean=0.300, reward_bound=0.349, batch=227\n",
      "5359: loss=0.309, reward_mean=0.440, reward_bound=0.414, batch=229\n",
      "5360: loss=0.307, reward_mean=0.400, reward_bound=0.430, batch=228\n",
      "5361: loss=0.311, reward_mean=0.400, reward_bound=0.478, batch=230\n",
      "5362: loss=0.312, reward_mean=0.450, reward_bound=0.464, batch=231\n",
      "5363: loss=0.313, reward_mean=0.420, reward_bound=0.478, batch=199\n",
      "5364: loss=0.309, reward_mean=0.370, reward_bound=0.295, batch=209\n",
      "5365: loss=0.308, reward_mean=0.400, reward_bound=0.182, batch=216\n",
      "5366: loss=0.311, reward_mean=0.410, reward_bound=0.314, batch=216\n",
      "5367: loss=0.310, reward_mean=0.460, reward_bound=0.284, batch=221\n",
      "5368: loss=0.310, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "5369: loss=0.311, reward_mean=0.390, reward_bound=0.349, batch=222\n",
      "5370: loss=0.309, reward_mean=0.420, reward_bound=0.292, batch=225\n",
      "5371: loss=0.309, reward_mean=0.430, reward_bound=0.282, batch=226\n",
      "5372: loss=0.308, reward_mean=0.350, reward_bound=0.314, batch=227\n",
      "5373: loss=0.307, reward_mean=0.350, reward_bound=0.282, batch=228\n",
      "5374: loss=0.307, reward_mean=0.360, reward_bound=0.317, batch=229\n",
      "5375: loss=0.308, reward_mean=0.480, reward_bound=0.364, batch=230\n",
      "5376: loss=0.315, reward_mean=0.480, reward_bound=0.387, batch=219\n",
      "5377: loss=0.316, reward_mean=0.350, reward_bound=0.314, batch=221\n",
      "5378: loss=0.315, reward_mean=0.410, reward_bound=0.254, batch=224\n",
      "5379: loss=0.314, reward_mean=0.410, reward_bound=0.387, batch=226\n",
      "5380: loss=0.312, reward_mean=0.350, reward_bound=0.430, batch=218\n",
      "5381: loss=0.313, reward_mean=0.360, reward_bound=0.349, batch=221\n",
      "5382: loss=0.310, reward_mean=0.390, reward_bound=0.387, batch=222\n",
      "5383: loss=0.310, reward_mean=0.450, reward_bound=0.292, batch=225\n",
      "5384: loss=0.311, reward_mean=0.310, reward_bound=0.349, batch=226\n",
      "5385: loss=0.311, reward_mean=0.390, reward_bound=0.349, batch=227\n",
      "5386: loss=0.312, reward_mean=0.370, reward_bound=0.349, batch=228\n",
      "5387: loss=0.312, reward_mean=0.390, reward_bound=0.353, batch=229\n",
      "5388: loss=0.311, reward_mean=0.390, reward_bound=0.387, batch=229\n",
      "5389: loss=0.311, reward_mean=0.400, reward_bound=0.430, batch=223\n",
      "5390: loss=0.309, reward_mean=0.380, reward_bound=0.229, batch=225\n",
      "5391: loss=0.313, reward_mean=0.350, reward_bound=0.349, batch=226\n",
      "5392: loss=0.312, reward_mean=0.350, reward_bound=0.387, batch=227\n",
      "5393: loss=0.310, reward_mean=0.310, reward_bound=0.335, batch=229\n",
      "5394: loss=0.310, reward_mean=0.340, reward_bound=0.328, batch=230\n",
      "5395: loss=0.310, reward_mean=0.390, reward_bound=0.338, batch=231\n",
      "5396: loss=0.310, reward_mean=0.410, reward_bound=0.430, batch=231\n",
      "5397: loss=0.313, reward_mean=0.350, reward_bound=0.478, batch=207\n",
      "5398: loss=0.316, reward_mean=0.390, reward_bound=0.282, batch=212\n",
      "5399: loss=0.310, reward_mean=0.350, reward_bound=0.263, batch=218\n",
      "5400: loss=0.311, reward_mean=0.330, reward_bound=0.286, batch=222\n",
      "5401: loss=0.310, reward_mean=0.360, reward_bound=0.314, batch=217\n",
      "5402: loss=0.308, reward_mean=0.260, reward_bound=0.229, batch=221\n",
      "5403: loss=0.312, reward_mean=0.390, reward_bound=0.349, batch=220\n",
      "5404: loss=0.312, reward_mean=0.430, reward_bound=0.349, batch=221\n",
      "5405: loss=0.310, reward_mean=0.430, reward_bound=0.282, batch=224\n",
      "5406: loss=0.310, reward_mean=0.310, reward_bound=0.249, batch=227\n",
      "5407: loss=0.310, reward_mean=0.390, reward_bound=0.380, batch=229\n",
      "5408: loss=0.312, reward_mean=0.490, reward_bound=0.387, batch=219\n",
      "5409: loss=0.311, reward_mean=0.420, reward_bound=0.349, batch=222\n",
      "5410: loss=0.312, reward_mean=0.350, reward_bound=0.360, batch=225\n",
      "5411: loss=0.311, reward_mean=0.360, reward_bound=0.296, batch=227\n",
      "5412: loss=0.310, reward_mean=0.350, reward_bound=0.349, batch=228\n",
      "5413: loss=0.310, reward_mean=0.430, reward_bound=0.387, batch=227\n",
      "5414: loss=0.312, reward_mean=0.400, reward_bound=0.430, batch=217\n",
      "5415: loss=0.311, reward_mean=0.390, reward_bound=0.349, batch=221\n",
      "5416: loss=0.309, reward_mean=0.350, reward_bound=0.254, batch=224\n",
      "5417: loss=0.309, reward_mean=0.290, reward_bound=0.314, batch=226\n",
      "5418: loss=0.308, reward_mean=0.370, reward_bound=0.387, batch=227\n",
      "5419: loss=0.310, reward_mean=0.410, reward_bound=0.430, batch=222\n",
      "5420: loss=0.310, reward_mean=0.320, reward_bound=0.349, batch=224\n",
      "5421: loss=0.310, reward_mean=0.420, reward_bound=0.314, batch=225\n",
      "5422: loss=0.310, reward_mean=0.360, reward_bound=0.478, batch=214\n",
      "5423: loss=0.308, reward_mean=0.320, reward_bound=0.167, batch=219\n",
      "5424: loss=0.308, reward_mean=0.410, reward_bound=0.254, batch=222\n",
      "5425: loss=0.309, reward_mean=0.440, reward_bound=0.282, batch=224\n",
      "5426: loss=0.310, reward_mean=0.360, reward_bound=0.345, batch=227\n",
      "5427: loss=0.309, reward_mean=0.450, reward_bound=0.349, batch=226\n",
      "5428: loss=0.309, reward_mean=0.390, reward_bound=0.368, batch=228\n",
      "5429: loss=0.308, reward_mean=0.400, reward_bound=0.387, batch=225\n",
      "5430: loss=0.307, reward_mean=0.380, reward_bound=0.365, batch=227\n",
      "5431: loss=0.307, reward_mean=0.450, reward_bound=0.349, batch=228\n",
      "5432: loss=0.309, reward_mean=0.380, reward_bound=0.392, batch=229\n",
      "5433: loss=0.311, reward_mean=0.400, reward_bound=0.430, batch=225\n",
      "5434: loss=0.310, reward_mean=0.360, reward_bound=0.296, batch=227\n",
      "5435: loss=0.313, reward_mean=0.420, reward_bound=0.387, batch=227\n",
      "5436: loss=0.313, reward_mean=0.480, reward_bound=0.430, batch=228\n",
      "5437: loss=0.313, reward_mean=0.410, reward_bound=0.397, batch=229\n",
      "5438: loss=0.314, reward_mean=0.380, reward_bound=0.424, batch=230\n",
      "5439: loss=0.308, reward_mean=0.370, reward_bound=0.478, batch=220\n",
      "5440: loss=0.307, reward_mean=0.300, reward_bound=0.282, batch=223\n",
      "5441: loss=0.305, reward_mean=0.350, reward_bound=0.335, batch=226\n",
      "5442: loss=0.304, reward_mean=0.370, reward_bound=0.368, batch=228\n",
      "5443: loss=0.305, reward_mean=0.370, reward_bound=0.387, batch=227\n",
      "5444: loss=0.307, reward_mean=0.370, reward_bound=0.430, batch=225\n",
      "5445: loss=0.307, reward_mean=0.420, reward_bound=0.478, batch=226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5446: loss=0.307, reward_mean=0.270, reward_bound=0.298, batch=228\n",
      "5447: loss=0.305, reward_mean=0.380, reward_bound=0.357, batch=229\n",
      "5448: loss=0.304, reward_mean=0.320, reward_bound=0.405, batch=230\n",
      "5449: loss=0.307, reward_mean=0.400, reward_bound=0.464, batch=231\n",
      "5450: loss=0.307, reward_mean=0.380, reward_bound=0.430, batch=231\n",
      "5451: loss=0.305, reward_mean=0.390, reward_bound=0.478, batch=229\n",
      "5452: loss=0.305, reward_mean=0.370, reward_bound=0.349, batch=229\n",
      "5453: loss=0.305, reward_mean=0.370, reward_bound=0.430, batch=229\n",
      "5454: loss=0.305, reward_mean=0.440, reward_bound=0.450, batch=230\n",
      "5455: loss=0.305, reward_mean=0.400, reward_bound=0.451, batch=231\n",
      "5456: loss=0.305, reward_mean=0.410, reward_bound=0.478, batch=230\n",
      "5457: loss=0.304, reward_mean=0.450, reward_bound=0.376, batch=231\n",
      "5458: loss=0.305, reward_mean=0.430, reward_bound=0.430, batch=231\n",
      "5459: loss=0.305, reward_mean=0.300, reward_bound=0.430, batch=231\n",
      "5460: loss=0.305, reward_mean=0.350, reward_bound=0.478, batch=231\n",
      "5462: loss=0.260, reward_mean=0.400, reward_bound=0.000, batch=40\n",
      "5463: loss=0.258, reward_mean=0.430, reward_bound=0.000, batch=83\n",
      "5464: loss=0.260, reward_mean=0.390, reward_bound=0.000, batch=122\n",
      "5465: loss=0.259, reward_mean=0.360, reward_bound=0.001, batch=155\n",
      "5466: loss=0.261, reward_mean=0.450, reward_bound=0.007, batch=176\n",
      "5467: loss=0.263, reward_mean=0.410, reward_bound=0.016, batch=192\n",
      "5468: loss=0.272, reward_mean=0.420, reward_bound=0.031, batch=199\n",
      "5469: loss=0.271, reward_mean=0.470, reward_bound=0.052, batch=203\n",
      "5470: loss=0.271, reward_mean=0.440, reward_bound=0.065, batch=211\n",
      "5471: loss=0.267, reward_mean=0.390, reward_bound=0.080, batch=200\n",
      "5472: loss=0.278, reward_mean=0.470, reward_bound=0.089, batch=204\n",
      "5473: loss=0.285, reward_mean=0.410, reward_bound=0.098, batch=208\n",
      "5474: loss=0.287, reward_mean=0.410, reward_bound=0.109, batch=212\n",
      "5475: loss=0.291, reward_mean=0.390, reward_bound=0.122, batch=216\n",
      "5476: loss=0.289, reward_mean=0.370, reward_bound=0.135, batch=209\n",
      "5477: loss=0.284, reward_mean=0.450, reward_bound=0.150, batch=212\n",
      "5478: loss=0.278, reward_mean=0.480, reward_bound=0.167, batch=202\n",
      "5479: loss=0.278, reward_mean=0.490, reward_bound=0.185, batch=190\n",
      "5480: loss=0.279, reward_mean=0.330, reward_bound=0.093, batch=203\n",
      "5481: loss=0.279, reward_mean=0.450, reward_bound=0.190, batch=212\n",
      "5482: loss=0.279, reward_mean=0.530, reward_bound=0.206, batch=228\n",
      "5483: loss=0.282, reward_mean=0.400, reward_bound=0.206, batch=210\n",
      "5484: loss=0.281, reward_mean=0.440, reward_bound=0.229, batch=200\n",
      "5485: loss=0.278, reward_mean=0.360, reward_bound=0.229, batch=209\n",
      "5486: loss=0.277, reward_mean=0.420, reward_bound=0.157, batch=216\n",
      "5487: loss=0.278, reward_mean=0.460, reward_bound=0.254, batch=187\n",
      "5488: loss=0.276, reward_mean=0.420, reward_bound=0.132, batch=201\n",
      "5489: loss=0.275, reward_mean=0.370, reward_bound=0.167, batch=210\n",
      "5490: loss=0.275, reward_mean=0.360, reward_bound=0.162, batch=217\n",
      "5491: loss=0.277, reward_mean=0.510, reward_bound=0.229, batch=218\n",
      "5492: loss=0.283, reward_mean=0.420, reward_bound=0.254, batch=221\n",
      "5493: loss=0.286, reward_mean=0.430, reward_bound=0.282, batch=177\n",
      "5494: loss=0.281, reward_mean=0.400, reward_bound=0.087, batch=194\n",
      "5495: loss=0.281, reward_mean=0.360, reward_bound=0.122, batch=204\n",
      "5496: loss=0.282, reward_mean=0.350, reward_bound=0.135, batch=209\n",
      "5497: loss=0.285, reward_mean=0.360, reward_bound=0.167, batch=210\n",
      "5498: loss=0.283, reward_mean=0.420, reward_bound=0.146, batch=217\n",
      "5499: loss=0.283, reward_mean=0.410, reward_bound=0.185, batch=217\n",
      "5500: loss=0.290, reward_mean=0.400, reward_bound=0.206, batch=220\n",
      "5501: loss=0.291, reward_mean=0.330, reward_bound=0.229, batch=216\n",
      "5502: loss=0.292, reward_mean=0.390, reward_bound=0.241, batch=221\n",
      "5503: loss=0.290, reward_mean=0.420, reward_bound=0.254, batch=211\n",
      "5504: loss=0.288, reward_mean=0.410, reward_bound=0.185, batch=216\n",
      "5505: loss=0.288, reward_mean=0.370, reward_bound=0.254, batch=220\n",
      "5506: loss=0.288, reward_mean=0.410, reward_bound=0.282, batch=217\n",
      "5507: loss=0.290, reward_mean=0.390, reward_bound=0.206, batch=221\n",
      "5508: loss=0.285, reward_mean=0.430, reward_bound=0.314, batch=176\n",
      "5509: loss=0.278, reward_mean=0.470, reward_bound=0.122, batch=191\n",
      "5510: loss=0.280, reward_mean=0.430, reward_bound=0.185, batch=198\n",
      "5511: loss=0.278, reward_mean=0.390, reward_bound=0.206, batch=207\n",
      "5512: loss=0.281, reward_mean=0.400, reward_bound=0.229, batch=207\n",
      "5513: loss=0.282, reward_mean=0.410, reward_bound=0.249, batch=215\n",
      "5514: loss=0.280, reward_mean=0.440, reward_bound=0.254, batch=218\n",
      "5515: loss=0.282, reward_mean=0.480, reward_bound=0.282, batch=219\n",
      "5516: loss=0.285, reward_mean=0.420, reward_bound=0.265, batch=223\n",
      "5517: loss=0.280, reward_mean=0.450, reward_bound=0.314, batch=214\n",
      "5518: loss=0.281, reward_mean=0.400, reward_bound=0.280, batch=220\n",
      "5519: loss=0.281, reward_mean=0.310, reward_bound=0.282, batch=221\n",
      "5520: loss=0.281, reward_mean=0.340, reward_bound=0.314, batch=222\n",
      "5521: loss=0.283, reward_mean=0.400, reward_bound=0.349, batch=152\n",
      "5522: loss=0.265, reward_mean=0.470, reward_bound=0.082, batch=176\n",
      "5523: loss=0.270, reward_mean=0.420, reward_bound=0.098, batch=190\n",
      "5524: loss=0.270, reward_mean=0.460, reward_bound=0.109, batch=199\n",
      "5525: loss=0.273, reward_mean=0.490, reward_bound=0.150, batch=204\n",
      "5526: loss=0.278, reward_mean=0.470, reward_bound=0.167, batch=212\n",
      "5527: loss=0.280, reward_mean=0.490, reward_bound=0.206, batch=223\n",
      "5528: loss=0.279, reward_mean=0.480, reward_bound=0.206, batch=225\n",
      "5529: loss=0.281, reward_mean=0.430, reward_bound=0.229, batch=215\n",
      "5530: loss=0.286, reward_mean=0.410, reward_bound=0.254, batch=210\n",
      "5531: loss=0.293, reward_mean=0.490, reward_bound=0.282, batch=202\n",
      "5532: loss=0.292, reward_mean=0.420, reward_bound=0.167, batch=211\n",
      "5533: loss=0.292, reward_mean=0.430, reward_bound=0.229, batch=215\n",
      "5534: loss=0.292, reward_mean=0.440, reward_bound=0.282, batch=218\n",
      "5535: loss=0.285, reward_mean=0.450, reward_bound=0.314, batch=209\n",
      "5536: loss=0.286, reward_mean=0.460, reward_bound=0.167, batch=215\n",
      "5537: loss=0.286, reward_mean=0.420, reward_bound=0.289, batch=220\n",
      "5538: loss=0.289, reward_mean=0.410, reward_bound=0.349, batch=212\n",
      "5539: loss=0.287, reward_mean=0.380, reward_bound=0.229, batch=217\n",
      "5540: loss=0.286, reward_mean=0.500, reward_bound=0.342, batch=222\n",
      "5541: loss=0.287, reward_mean=0.420, reward_bound=0.272, batch=225\n",
      "5542: loss=0.287, reward_mean=0.370, reward_bound=0.314, batch=225\n",
      "5543: loss=0.290, reward_mean=0.350, reward_bound=0.349, batch=225\n",
      "5544: loss=0.290, reward_mean=0.430, reward_bound=0.266, batch=227\n",
      "5545: loss=0.289, reward_mean=0.500, reward_bound=0.349, batch=227\n",
      "5546: loss=0.289, reward_mean=0.420, reward_bound=0.380, batch=229\n",
      "5547: loss=0.294, reward_mean=0.380, reward_bound=0.387, batch=156\n",
      "5548: loss=0.293, reward_mean=0.450, reward_bound=0.076, batch=179\n",
      "5549: loss=0.289, reward_mean=0.440, reward_bound=0.114, batch=195\n",
      "5550: loss=0.292, reward_mean=0.420, reward_bound=0.112, batch=206\n",
      "5551: loss=0.297, reward_mean=0.440, reward_bound=0.150, batch=207\n",
      "5552: loss=0.292, reward_mean=0.350, reward_bound=0.167, batch=214\n",
      "5553: loss=0.290, reward_mean=0.490, reward_bound=0.185, batch=218\n",
      "5554: loss=0.289, reward_mean=0.430, reward_bound=0.206, batch=219\n",
      "5555: loss=0.287, reward_mean=0.390, reward_bound=0.229, batch=217\n",
      "5556: loss=0.290, reward_mean=0.410, reward_bound=0.254, batch=214\n",
      "5557: loss=0.294, reward_mean=0.440, reward_bound=0.226, batch=220\n",
      "5558: loss=0.290, reward_mean=0.410, reward_bound=0.274, batch=224\n",
      "5559: loss=0.287, reward_mean=0.440, reward_bound=0.282, batch=224\n",
      "5560: loss=0.286, reward_mean=0.420, reward_bound=0.311, batch=227\n",
      "5561: loss=0.287, reward_mean=0.500, reward_bound=0.314, batch=215\n",
      "5562: loss=0.285, reward_mean=0.380, reward_bound=0.266, batch=220\n",
      "5563: loss=0.286, reward_mean=0.360, reward_bound=0.222, batch=224\n",
      "5564: loss=0.285, reward_mean=0.440, reward_bound=0.345, batch=227\n",
      "5565: loss=0.289, reward_mean=0.420, reward_bound=0.349, batch=212\n",
      "5566: loss=0.285, reward_mean=0.370, reward_bound=0.206, batch=219\n",
      "5567: loss=0.289, reward_mean=0.430, reward_bound=0.254, batch=219\n",
      "5568: loss=0.289, reward_mean=0.460, reward_bound=0.295, batch=223\n",
      "5569: loss=0.289, reward_mean=0.460, reward_bound=0.301, batch=226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5570: loss=0.289, reward_mean=0.490, reward_bound=0.331, batch=228\n",
      "5571: loss=0.288, reward_mean=0.410, reward_bound=0.349, batch=221\n",
      "5572: loss=0.296, reward_mean=0.400, reward_bound=0.387, batch=197\n",
      "5573: loss=0.290, reward_mean=0.300, reward_bound=0.089, batch=209\n",
      "5574: loss=0.293, reward_mean=0.440, reward_bound=0.239, batch=216\n",
      "5575: loss=0.291, reward_mean=0.480, reward_bound=0.282, batch=218\n",
      "5576: loss=0.294, reward_mean=0.430, reward_bound=0.314, batch=218\n",
      "5577: loss=0.293, reward_mean=0.420, reward_bound=0.257, batch=222\n",
      "5578: loss=0.292, reward_mean=0.440, reward_bound=0.263, batch=225\n",
      "5579: loss=0.294, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "5580: loss=0.293, reward_mean=0.390, reward_bound=0.345, batch=227\n",
      "5581: loss=0.294, reward_mean=0.440, reward_bound=0.349, batch=221\n",
      "5582: loss=0.296, reward_mean=0.410, reward_bound=0.282, batch=224\n",
      "5583: loss=0.296, reward_mean=0.420, reward_bound=0.345, batch=227\n",
      "5584: loss=0.293, reward_mean=0.480, reward_bound=0.349, batch=228\n",
      "5585: loss=0.295, reward_mean=0.360, reward_bound=0.387, batch=221\n",
      "5586: loss=0.294, reward_mean=0.430, reward_bound=0.349, batch=224\n",
      "5587: loss=0.292, reward_mean=0.350, reward_bound=0.229, batch=226\n",
      "5588: loss=0.292, reward_mean=0.480, reward_bound=0.368, batch=228\n",
      "5589: loss=0.291, reward_mean=0.310, reward_bound=0.289, batch=229\n",
      "5590: loss=0.294, reward_mean=0.400, reward_bound=0.387, batch=226\n",
      "5591: loss=0.293, reward_mean=0.390, reward_bound=0.298, batch=228\n",
      "5592: loss=0.296, reward_mean=0.500, reward_bound=0.430, batch=117\n",
      "5593: loss=0.273, reward_mean=0.450, reward_bound=0.020, batch=152\n",
      "5594: loss=0.270, reward_mean=0.500, reward_bound=0.052, batch=175\n",
      "5595: loss=0.266, reward_mean=0.450, reward_bound=0.065, batch=191\n",
      "5596: loss=0.266, reward_mean=0.410, reward_bound=0.080, batch=199\n",
      "5597: loss=0.275, reward_mean=0.460, reward_bound=0.109, batch=202\n",
      "5598: loss=0.275, reward_mean=0.380, reward_bound=0.122, batch=207\n",
      "5599: loss=0.278, reward_mean=0.380, reward_bound=0.150, batch=206\n",
      "5600: loss=0.283, reward_mean=0.400, reward_bound=0.167, batch=209\n",
      "5601: loss=0.285, reward_mean=0.490, reward_bound=0.185, batch=214\n",
      "5602: loss=0.292, reward_mean=0.340, reward_bound=0.206, batch=214\n",
      "5603: loss=0.294, reward_mean=0.400, reward_bound=0.229, batch=206\n",
      "5604: loss=0.294, reward_mean=0.500, reward_bound=0.254, batch=203\n",
      "5605: loss=0.290, reward_mean=0.510, reward_bound=0.282, batch=195\n",
      "5606: loss=0.285, reward_mean=0.420, reward_bound=0.135, batch=205\n",
      "5607: loss=0.284, reward_mean=0.410, reward_bound=0.189, batch=213\n",
      "5608: loss=0.287, reward_mean=0.410, reward_bound=0.206, batch=216\n",
      "5609: loss=0.288, reward_mean=0.460, reward_bound=0.282, batch=216\n",
      "5610: loss=0.293, reward_mean=0.500, reward_bound=0.314, batch=203\n",
      "5611: loss=0.287, reward_mean=0.430, reward_bound=0.271, batch=212\n",
      "5612: loss=0.287, reward_mean=0.340, reward_bound=0.282, batch=217\n",
      "5613: loss=0.287, reward_mean=0.440, reward_bound=0.272, batch=222\n",
      "5614: loss=0.287, reward_mean=0.400, reward_bound=0.314, batch=222\n",
      "5615: loss=0.292, reward_mean=0.510, reward_bound=0.349, batch=190\n",
      "5616: loss=0.285, reward_mean=0.450, reward_bound=0.109, batch=201\n",
      "5617: loss=0.282, reward_mean=0.530, reward_bound=0.229, batch=209\n",
      "5618: loss=0.281, reward_mean=0.420, reward_bound=0.141, batch=216\n",
      "5619: loss=0.282, reward_mean=0.360, reward_bound=0.254, batch=215\n",
      "5620: loss=0.284, reward_mean=0.460, reward_bound=0.314, batch=215\n",
      "5621: loss=0.284, reward_mean=0.420, reward_bound=0.240, batch=220\n",
      "5622: loss=0.283, reward_mean=0.480, reward_bound=0.314, batch=222\n",
      "5623: loss=0.282, reward_mean=0.430, reward_bound=0.292, batch=225\n",
      "5624: loss=0.283, reward_mean=0.460, reward_bound=0.321, batch=227\n",
      "5625: loss=0.288, reward_mean=0.400, reward_bound=0.349, batch=218\n",
      "5626: loss=0.288, reward_mean=0.490, reward_bound=0.314, batch=221\n",
      "5627: loss=0.286, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "5628: loss=0.286, reward_mean=0.420, reward_bound=0.349, batch=226\n",
      "5629: loss=0.287, reward_mean=0.520, reward_bound=0.331, batch=228\n",
      "5630: loss=0.299, reward_mean=0.380, reward_bound=0.387, batch=185\n",
      "5631: loss=0.292, reward_mean=0.400, reward_bound=0.138, batch=199\n",
      "5632: loss=0.299, reward_mean=0.430, reward_bound=0.194, batch=209\n",
      "5633: loss=0.298, reward_mean=0.410, reward_bound=0.215, batch=216\n",
      "5634: loss=0.293, reward_mean=0.450, reward_bound=0.229, batch=220\n",
      "5635: loss=0.292, reward_mean=0.470, reward_bound=0.247, batch=224\n",
      "5636: loss=0.291, reward_mean=0.340, reward_bound=0.254, batch=223\n",
      "5637: loss=0.286, reward_mean=0.520, reward_bound=0.282, batch=223\n",
      "5638: loss=0.288, reward_mean=0.500, reward_bound=0.314, batch=218\n",
      "5639: loss=0.287, reward_mean=0.360, reward_bound=0.289, batch=222\n",
      "5640: loss=0.290, reward_mean=0.450, reward_bound=0.349, batch=214\n",
      "5641: loss=0.290, reward_mean=0.390, reward_bound=0.254, batch=218\n",
      "5642: loss=0.290, reward_mean=0.410, reward_bound=0.349, batch=221\n",
      "5643: loss=0.292, reward_mean=0.400, reward_bound=0.387, batch=213\n",
      "5644: loss=0.291, reward_mean=0.410, reward_bound=0.301, batch=219\n",
      "5645: loss=0.295, reward_mean=0.430, reward_bound=0.249, batch=223\n",
      "5646: loss=0.293, reward_mean=0.450, reward_bound=0.220, batch=226\n",
      "5647: loss=0.292, reward_mean=0.510, reward_bound=0.314, batch=227\n",
      "5648: loss=0.292, reward_mean=0.420, reward_bound=0.349, batch=227\n",
      "5649: loss=0.291, reward_mean=0.460, reward_bound=0.349, batch=228\n",
      "5650: loss=0.291, reward_mean=0.490, reward_bound=0.353, batch=229\n",
      "5651: loss=0.290, reward_mean=0.450, reward_bound=0.387, batch=223\n",
      "5652: loss=0.290, reward_mean=0.370, reward_bound=0.387, batch=224\n",
      "5653: loss=0.291, reward_mean=0.400, reward_bound=0.426, batch=227\n",
      "5654: loss=0.290, reward_mean=0.410, reward_bound=0.414, batch=229\n",
      "5655: loss=0.290, reward_mean=0.380, reward_bound=0.387, batch=229\n",
      "5656: loss=0.293, reward_mean=0.340, reward_bound=0.430, batch=168\n",
      "5657: loss=0.280, reward_mean=0.310, reward_bound=0.025, batch=187\n",
      "5658: loss=0.285, reward_mean=0.360, reward_bound=0.065, batch=200\n",
      "5659: loss=0.283, reward_mean=0.400, reward_bound=0.109, batch=209\n",
      "5660: loss=0.282, reward_mean=0.390, reward_bound=0.185, batch=214\n",
      "5661: loss=0.284, reward_mean=0.450, reward_bound=0.206, batch=217\n",
      "5662: loss=0.285, reward_mean=0.430, reward_bound=0.229, batch=214\n",
      "5663: loss=0.283, reward_mean=0.410, reward_bound=0.254, batch=211\n",
      "5664: loss=0.280, reward_mean=0.440, reward_bound=0.254, batch=217\n",
      "5665: loss=0.279, reward_mean=0.400, reward_bound=0.282, batch=211\n",
      "5666: loss=0.282, reward_mean=0.490, reward_bound=0.314, batch=205\n",
      "5667: loss=0.281, reward_mean=0.440, reward_bound=0.229, batch=212\n",
      "5668: loss=0.280, reward_mean=0.390, reward_bound=0.229, batch=215\n",
      "5669: loss=0.281, reward_mean=0.470, reward_bound=0.254, batch=218\n",
      "5670: loss=0.285, reward_mean=0.470, reward_bound=0.282, batch=221\n",
      "5671: loss=0.283, reward_mean=0.400, reward_bound=0.314, batch=223\n",
      "5672: loss=0.282, reward_mean=0.510, reward_bound=0.335, batch=226\n",
      "5673: loss=0.287, reward_mean=0.450, reward_bound=0.349, batch=210\n",
      "5674: loss=0.285, reward_mean=0.470, reward_bound=0.338, batch=217\n",
      "5675: loss=0.283, reward_mean=0.530, reward_bound=0.330, batch=222\n",
      "5676: loss=0.284, reward_mean=0.400, reward_bound=0.349, batch=221\n",
      "5677: loss=0.284, reward_mean=0.460, reward_bound=0.349, batch=224\n",
      "5678: loss=0.285, reward_mean=0.450, reward_bound=0.384, batch=227\n",
      "5679: loss=0.285, reward_mean=0.380, reward_bound=0.282, batch=228\n",
      "5680: loss=0.287, reward_mean=0.400, reward_bound=0.387, batch=208\n",
      "5681: loss=0.283, reward_mean=0.430, reward_bound=0.229, batch=213\n",
      "5682: loss=0.288, reward_mean=0.390, reward_bound=0.254, batch=216\n",
      "5683: loss=0.286, reward_mean=0.460, reward_bound=0.331, batch=221\n",
      "5684: loss=0.284, reward_mean=0.430, reward_bound=0.349, batch=224\n",
      "5685: loss=0.284, reward_mean=0.340, reward_bound=0.282, batch=226\n",
      "5686: loss=0.285, reward_mean=0.490, reward_bound=0.387, batch=222\n",
      "5687: loss=0.284, reward_mean=0.410, reward_bound=0.349, batch=224\n",
      "5688: loss=0.286, reward_mean=0.400, reward_bound=0.314, batch=226\n",
      "5689: loss=0.285, reward_mean=0.420, reward_bound=0.351, batch=228\n",
      "5690: loss=0.286, reward_mean=0.360, reward_bound=0.430, batch=207\n",
      "5691: loss=0.285, reward_mean=0.400, reward_bound=0.160, batch=215\n",
      "5692: loss=0.291, reward_mean=0.490, reward_bound=0.254, batch=219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5693: loss=0.288, reward_mean=0.440, reward_bound=0.349, batch=218\n",
      "5694: loss=0.287, reward_mean=0.530, reward_bound=0.234, batch=222\n",
      "5695: loss=0.285, reward_mean=0.390, reward_bound=0.254, batch=224\n",
      "5696: loss=0.285, reward_mean=0.550, reward_bound=0.314, batch=226\n",
      "5697: loss=0.286, reward_mean=0.420, reward_bound=0.349, batch=227\n",
      "5698: loss=0.286, reward_mean=0.490, reward_bound=0.387, batch=223\n",
      "5699: loss=0.286, reward_mean=0.330, reward_bound=0.387, batch=225\n",
      "5700: loss=0.285, reward_mean=0.460, reward_bound=0.387, batch=226\n",
      "5701: loss=0.285, reward_mean=0.370, reward_bound=0.368, batch=228\n",
      "5702: loss=0.287, reward_mean=0.500, reward_bound=0.353, batch=229\n",
      "5703: loss=0.285, reward_mean=0.390, reward_bound=0.387, batch=228\n",
      "5704: loss=0.285, reward_mean=0.410, reward_bound=0.430, batch=217\n",
      "5705: loss=0.283, reward_mean=0.380, reward_bound=0.182, batch=222\n",
      "5706: loss=0.283, reward_mean=0.460, reward_bound=0.254, batch=224\n",
      "5707: loss=0.284, reward_mean=0.410, reward_bound=0.314, batch=224\n",
      "5708: loss=0.283, reward_mean=0.410, reward_bound=0.280, batch=227\n",
      "5709: loss=0.284, reward_mean=0.460, reward_bound=0.380, batch=229\n",
      "5710: loss=0.284, reward_mean=0.370, reward_bound=0.387, batch=222\n",
      "5711: loss=0.284, reward_mean=0.430, reward_bound=0.327, batch=225\n",
      "5712: loss=0.284, reward_mean=0.490, reward_bound=0.430, batch=223\n",
      "5713: loss=0.283, reward_mean=0.320, reward_bound=0.220, batch=226\n",
      "5714: loss=0.282, reward_mean=0.440, reward_bound=0.268, batch=228\n",
      "5715: loss=0.283, reward_mean=0.500, reward_bound=0.317, batch=229\n",
      "5716: loss=0.284, reward_mean=0.470, reward_bound=0.328, batch=230\n",
      "5717: loss=0.283, reward_mean=0.440, reward_bound=0.349, batch=230\n",
      "5718: loss=0.284, reward_mean=0.500, reward_bound=0.418, batch=231\n",
      "5719: loss=0.284, reward_mean=0.520, reward_bound=0.430, batch=227\n",
      "5720: loss=0.284, reward_mean=0.310, reward_bound=0.342, batch=229\n",
      "5721: loss=0.284, reward_mean=0.450, reward_bound=0.387, batch=229\n",
      "5722: loss=0.284, reward_mean=0.480, reward_bound=0.424, batch=230\n",
      "5723: loss=0.284, reward_mean=0.400, reward_bound=0.430, batch=230\n",
      "5724: loss=0.283, reward_mean=0.560, reward_bound=0.464, batch=231\n",
      "5725: loss=0.278, reward_mean=0.470, reward_bound=0.478, batch=104\n",
      "5726: loss=0.235, reward_mean=0.450, reward_bound=0.005, batch=143\n",
      "5727: loss=0.245, reward_mean=0.480, reward_bound=0.019, batch=170\n",
      "5728: loss=0.243, reward_mean=0.380, reward_bound=0.036, batch=189\n",
      "5729: loss=0.246, reward_mean=0.460, reward_bound=0.052, batch=200\n",
      "5730: loss=0.246, reward_mean=0.430, reward_bound=0.086, batch=210\n",
      "5731: loss=0.248, reward_mean=0.450, reward_bound=0.109, batch=215\n",
      "5732: loss=0.255, reward_mean=0.360, reward_bound=0.135, batch=213\n",
      "5733: loss=0.257, reward_mean=0.380, reward_bound=0.150, batch=215\n",
      "5734: loss=0.259, reward_mean=0.440, reward_bound=0.167, batch=218\n",
      "5735: loss=0.261, reward_mean=0.390, reward_bound=0.185, batch=213\n",
      "5736: loss=0.266, reward_mean=0.450, reward_bound=0.206, batch=208\n",
      "5737: loss=0.262, reward_mean=0.380, reward_bound=0.206, batch=213\n",
      "5738: loss=0.264, reward_mean=0.470, reward_bound=0.229, batch=211\n",
      "5739: loss=0.263, reward_mean=0.360, reward_bound=0.206, batch=216\n",
      "5740: loss=0.265, reward_mean=0.440, reward_bound=0.230, batch=221\n",
      "5741: loss=0.269, reward_mean=0.400, reward_bound=0.254, batch=207\n",
      "5742: loss=0.267, reward_mean=0.470, reward_bound=0.150, batch=214\n",
      "5743: loss=0.271, reward_mean=0.450, reward_bound=0.282, batch=196\n",
      "5744: loss=0.270, reward_mean=0.440, reward_bound=0.185, batch=206\n",
      "5745: loss=0.273, reward_mean=0.480, reward_bound=0.217, batch=214\n",
      "5746: loss=0.275, reward_mean=0.430, reward_bound=0.229, batch=216\n",
      "5747: loss=0.274, reward_mean=0.440, reward_bound=0.241, batch=221\n",
      "5748: loss=0.277, reward_mean=0.410, reward_bound=0.254, batch=224\n",
      "5749: loss=0.273, reward_mean=0.430, reward_bound=0.282, batch=225\n",
      "5750: loss=0.265, reward_mean=0.550, reward_bound=0.314, batch=207\n",
      "5751: loss=0.263, reward_mean=0.480, reward_bound=0.224, batch=215\n",
      "5752: loss=0.265, reward_mean=0.470, reward_bound=0.234, batch=220\n",
      "5753: loss=0.269, reward_mean=0.350, reward_bound=0.254, batch=222\n",
      "5754: loss=0.264, reward_mean=0.450, reward_bound=0.314, batch=224\n",
      "5755: loss=0.264, reward_mean=0.520, reward_bound=0.311, batch=227\n",
      "5756: loss=0.272, reward_mean=0.470, reward_bound=0.349, batch=193\n",
      "5757: loss=0.272, reward_mean=0.500, reward_bound=0.198, batch=205\n",
      "5758: loss=0.269, reward_mean=0.440, reward_bound=0.210, batch=213\n",
      "5759: loss=0.269, reward_mean=0.430, reward_bound=0.244, batch=219\n",
      "5760: loss=0.270, reward_mean=0.390, reward_bound=0.254, batch=220\n",
      "5761: loss=0.270, reward_mean=0.440, reward_bound=0.282, batch=219\n",
      "5762: loss=0.275, reward_mean=0.430, reward_bound=0.314, batch=214\n",
      "5763: loss=0.274, reward_mean=0.420, reward_bound=0.345, batch=220\n",
      "5764: loss=0.275, reward_mean=0.430, reward_bound=0.304, batch=224\n",
      "5765: loss=0.272, reward_mean=0.460, reward_bound=0.314, batch=225\n",
      "5766: loss=0.271, reward_mean=0.500, reward_bound=0.349, batch=221\n",
      "5767: loss=0.276, reward_mean=0.510, reward_bound=0.387, batch=187\n",
      "5768: loss=0.277, reward_mean=0.430, reward_bound=0.167, batch=200\n",
      "5769: loss=0.278, reward_mean=0.400, reward_bound=0.167, batch=207\n",
      "5770: loss=0.279, reward_mean=0.440, reward_bound=0.229, batch=213\n",
      "5771: loss=0.279, reward_mean=0.440, reward_bound=0.254, batch=214\n",
      "5772: loss=0.278, reward_mean=0.370, reward_bound=0.226, batch=220\n",
      "5773: loss=0.279, reward_mean=0.460, reward_bound=0.247, batch=224\n",
      "5774: loss=0.272, reward_mean=0.440, reward_bound=0.282, batch=220\n",
      "5775: loss=0.274, reward_mean=0.440, reward_bound=0.314, batch=220\n",
      "5776: loss=0.273, reward_mean=0.510, reward_bound=0.349, batch=214\n",
      "5777: loss=0.274, reward_mean=0.440, reward_bound=0.277, batch=220\n",
      "5778: loss=0.274, reward_mean=0.480, reward_bound=0.304, batch=224\n",
      "5779: loss=0.274, reward_mean=0.440, reward_bound=0.282, batch=226\n",
      "5780: loss=0.278, reward_mean=0.440, reward_bound=0.314, batch=225\n",
      "5781: loss=0.275, reward_mean=0.510, reward_bound=0.387, batch=205\n",
      "5782: loss=0.274, reward_mean=0.500, reward_bound=0.260, batch=213\n",
      "5783: loss=0.271, reward_mean=0.520, reward_bound=0.282, batch=215\n",
      "5784: loss=0.278, reward_mean=0.440, reward_bound=0.314, batch=218\n",
      "5785: loss=0.276, reward_mean=0.550, reward_bound=0.314, batch=221\n",
      "5786: loss=0.278, reward_mean=0.410, reward_bound=0.349, batch=222\n",
      "5787: loss=0.275, reward_mean=0.400, reward_bound=0.387, batch=220\n",
      "5788: loss=0.274, reward_mean=0.430, reward_bound=0.329, batch=224\n",
      "5789: loss=0.273, reward_mean=0.520, reward_bound=0.349, batch=225\n",
      "5790: loss=0.274, reward_mean=0.430, reward_bound=0.387, batch=224\n",
      "5791: loss=0.274, reward_mean=0.450, reward_bound=0.349, batch=226\n",
      "5792: loss=0.274, reward_mean=0.370, reward_bound=0.314, batch=227\n",
      "5793: loss=0.275, reward_mean=0.520, reward_bound=0.422, batch=229\n",
      "5794: loss=0.275, reward_mean=0.470, reward_bound=0.430, batch=173\n",
      "5795: loss=0.275, reward_mean=0.460, reward_bound=0.130, batch=191\n",
      "5796: loss=0.270, reward_mean=0.390, reward_bound=0.135, batch=202\n",
      "5797: loss=0.264, reward_mean=0.440, reward_bound=0.150, batch=208\n",
      "5798: loss=0.268, reward_mean=0.430, reward_bound=0.167, batch=211\n",
      "5799: loss=0.270, reward_mean=0.430, reward_bound=0.206, batch=213\n",
      "5800: loss=0.271, reward_mean=0.360, reward_bound=0.229, batch=211\n",
      "5801: loss=0.270, reward_mean=0.360, reward_bound=0.206, batch=216\n",
      "5802: loss=0.266, reward_mean=0.470, reward_bound=0.254, batch=215\n",
      "5803: loss=0.265, reward_mean=0.430, reward_bound=0.229, batch=219\n",
      "5804: loss=0.267, reward_mean=0.360, reward_bound=0.282, batch=216\n",
      "5805: loss=0.270, reward_mean=0.410, reward_bound=0.206, batch=220\n",
      "5806: loss=0.266, reward_mean=0.430, reward_bound=0.304, batch=224\n",
      "5807: loss=0.266, reward_mean=0.450, reward_bound=0.280, batch=227\n",
      "5808: loss=0.268, reward_mean=0.410, reward_bound=0.308, batch=229\n",
      "5809: loss=0.268, reward_mean=0.300, reward_bound=0.314, batch=222\n",
      "5810: loss=0.268, reward_mean=0.440, reward_bound=0.272, batch=225\n",
      "5811: loss=0.267, reward_mean=0.400, reward_bound=0.314, batch=226\n",
      "5812: loss=0.268, reward_mean=0.400, reward_bound=0.349, batch=208\n",
      "5813: loss=0.268, reward_mean=0.460, reward_bound=0.282, batch=212\n",
      "5814: loss=0.271, reward_mean=0.440, reward_bound=0.213, batch=218\n",
      "5815: loss=0.266, reward_mean=0.430, reward_bound=0.314, batch=217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5816: loss=0.265, reward_mean=0.550, reward_bound=0.335, batch=222\n",
      "5817: loss=0.264, reward_mean=0.510, reward_bound=0.360, batch=225\n",
      "5818: loss=0.268, reward_mean=0.390, reward_bound=0.387, batch=201\n",
      "5819: loss=0.270, reward_mean=0.410, reward_bound=0.254, batch=210\n",
      "5820: loss=0.269, reward_mean=0.430, reward_bound=0.282, batch=216\n",
      "5821: loss=0.271, reward_mean=0.450, reward_bound=0.284, batch=221\n",
      "5822: loss=0.272, reward_mean=0.390, reward_bound=0.314, batch=220\n",
      "5823: loss=0.269, reward_mean=0.400, reward_bound=0.304, batch=224\n",
      "5824: loss=0.274, reward_mean=0.440, reward_bound=0.311, batch=227\n",
      "5825: loss=0.270, reward_mean=0.490, reward_bound=0.349, batch=224\n",
      "5826: loss=0.270, reward_mean=0.420, reward_bound=0.349, batch=226\n",
      "5827: loss=0.267, reward_mean=0.400, reward_bound=0.387, batch=222\n",
      "5828: loss=0.267, reward_mean=0.520, reward_bound=0.400, batch=225\n",
      "5829: loss=0.267, reward_mean=0.460, reward_bound=0.396, batch=227\n",
      "5830: loss=0.270, reward_mean=0.400, reward_bound=0.430, batch=201\n",
      "5831: loss=0.267, reward_mean=0.390, reward_bound=0.185, batch=209\n",
      "5832: loss=0.266, reward_mean=0.440, reward_bound=0.250, batch=216\n",
      "5833: loss=0.266, reward_mean=0.440, reward_bound=0.282, batch=220\n",
      "5834: loss=0.267, reward_mean=0.390, reward_bound=0.314, batch=223\n",
      "5835: loss=0.268, reward_mean=0.450, reward_bound=0.349, batch=220\n",
      "5836: loss=0.271, reward_mean=0.370, reward_bound=0.376, batch=224\n",
      "5837: loss=0.273, reward_mean=0.440, reward_bound=0.349, batch=226\n",
      "5838: loss=0.273, reward_mean=0.450, reward_bound=0.351, batch=228\n",
      "5839: loss=0.271, reward_mean=0.500, reward_bound=0.387, batch=224\n",
      "5840: loss=0.269, reward_mean=0.410, reward_bound=0.311, batch=227\n",
      "5841: loss=0.268, reward_mean=0.450, reward_bound=0.314, batch=228\n",
      "5842: loss=0.269, reward_mean=0.410, reward_bound=0.387, batch=228\n",
      "5843: loss=0.269, reward_mean=0.440, reward_bound=0.264, batch=229\n",
      "5844: loss=0.269, reward_mean=0.470, reward_bound=0.405, batch=230\n",
      "5845: loss=0.268, reward_mean=0.450, reward_bound=0.430, batch=215\n",
      "5846: loss=0.271, reward_mean=0.420, reward_bound=0.260, batch=220\n",
      "5847: loss=0.268, reward_mean=0.420, reward_bound=0.304, batch=224\n",
      "5848: loss=0.267, reward_mean=0.420, reward_bound=0.311, batch=227\n",
      "5849: loss=0.267, reward_mean=0.430, reward_bound=0.314, batch=226\n",
      "5850: loss=0.268, reward_mean=0.360, reward_bound=0.349, batch=225\n",
      "5851: loss=0.268, reward_mean=0.440, reward_bound=0.314, batch=225\n",
      "5852: loss=0.268, reward_mean=0.350, reward_bound=0.314, batch=225\n",
      "5853: loss=0.269, reward_mean=0.370, reward_bound=0.229, batch=226\n",
      "5854: loss=0.271, reward_mean=0.480, reward_bound=0.331, batch=228\n",
      "5855: loss=0.268, reward_mean=0.520, reward_bound=0.349, batch=227\n",
      "5856: loss=0.267, reward_mean=0.400, reward_bound=0.342, batch=229\n",
      "5857: loss=0.268, reward_mean=0.430, reward_bound=0.364, batch=230\n",
      "5858: loss=0.268, reward_mean=0.400, reward_bound=0.365, batch=231\n",
      "5859: loss=0.267, reward_mean=0.430, reward_bound=0.387, batch=225\n",
      "5860: loss=0.266, reward_mean=0.370, reward_bound=0.273, batch=227\n",
      "5861: loss=0.267, reward_mean=0.610, reward_bound=0.422, batch=229\n",
      "5862: loss=0.267, reward_mean=0.400, reward_bound=0.405, batch=230\n",
      "5863: loss=0.266, reward_mean=0.480, reward_bound=0.365, batch=231\n",
      "5864: loss=0.266, reward_mean=0.560, reward_bound=0.387, batch=231\n",
      "5865: loss=0.267, reward_mean=0.390, reward_bound=0.430, batch=224\n",
      "5866: loss=0.266, reward_mean=0.500, reward_bound=0.387, batch=226\n",
      "5867: loss=0.266, reward_mean=0.400, reward_bound=0.430, batch=227\n",
      "5868: loss=0.266, reward_mean=0.470, reward_bound=0.373, batch=229\n",
      "5869: loss=0.266, reward_mean=0.450, reward_bound=0.387, batch=228\n",
      "5870: loss=0.266, reward_mean=0.420, reward_bound=0.430, batch=228\n",
      "5871: loss=0.266, reward_mean=0.520, reward_bound=0.478, batch=230\n",
      "5872: loss=0.266, reward_mean=0.480, reward_bound=0.464, batch=231\n",
      "5873: loss=0.272, reward_mean=0.370, reward_bound=0.478, batch=156\n",
      "5874: loss=0.256, reward_mean=0.490, reward_bound=0.128, batch=179\n",
      "5875: loss=0.253, reward_mean=0.390, reward_bound=0.075, batch=195\n",
      "5876: loss=0.254, reward_mean=0.460, reward_bound=0.124, batch=206\n",
      "5877: loss=0.256, reward_mean=0.500, reward_bound=0.150, batch=213\n",
      "5878: loss=0.262, reward_mean=0.420, reward_bound=0.185, batch=211\n",
      "5879: loss=0.264, reward_mean=0.440, reward_bound=0.206, batch=215\n",
      "5880: loss=0.263, reward_mean=0.510, reward_bound=0.229, batch=216\n",
      "5881: loss=0.264, reward_mean=0.480, reward_bound=0.254, batch=211\n",
      "5882: loss=0.264, reward_mean=0.520, reward_bound=0.254, batch=216\n",
      "5883: loss=0.266, reward_mean=0.410, reward_bound=0.282, batch=211\n",
      "5884: loss=0.267, reward_mean=0.540, reward_bound=0.314, batch=207\n",
      "5885: loss=0.265, reward_mean=0.380, reward_bound=0.206, batch=213\n",
      "5886: loss=0.266, reward_mean=0.380, reward_bound=0.254, batch=216\n",
      "5887: loss=0.264, reward_mean=0.430, reward_bound=0.268, batch=221\n",
      "5888: loss=0.264, reward_mean=0.490, reward_bound=0.282, batch=224\n",
      "5889: loss=0.267, reward_mean=0.510, reward_bound=0.345, batch=227\n",
      "5890: loss=0.272, reward_mean=0.360, reward_bound=0.349, batch=213\n",
      "5891: loss=0.269, reward_mean=0.380, reward_bound=0.229, batch=218\n",
      "5892: loss=0.270, reward_mean=0.430, reward_bound=0.282, batch=220\n",
      "5893: loss=0.274, reward_mean=0.450, reward_bound=0.314, batch=223\n",
      "5894: loss=0.276, reward_mean=0.450, reward_bound=0.335, batch=226\n",
      "5895: loss=0.275, reward_mean=0.420, reward_bound=0.316, batch=228\n",
      "5896: loss=0.276, reward_mean=0.480, reward_bound=0.353, batch=229\n",
      "5897: loss=0.277, reward_mean=0.360, reward_bound=0.364, batch=230\n",
      "5898: loss=0.275, reward_mean=0.430, reward_bound=0.387, batch=204\n",
      "5899: loss=0.275, reward_mean=0.420, reward_bound=0.206, batch=212\n",
      "5900: loss=0.270, reward_mean=0.430, reward_bound=0.236, batch=218\n",
      "5901: loss=0.271, reward_mean=0.500, reward_bound=0.282, batch=219\n",
      "5902: loss=0.269, reward_mean=0.440, reward_bound=0.254, batch=222\n",
      "5903: loss=0.272, reward_mean=0.420, reward_bound=0.349, batch=220\n",
      "5904: loss=0.270, reward_mean=0.360, reward_bound=0.304, batch=224\n",
      "5905: loss=0.270, reward_mean=0.370, reward_bound=0.349, batch=226\n",
      "5906: loss=0.270, reward_mean=0.510, reward_bound=0.368, batch=228\n",
      "5907: loss=0.271, reward_mean=0.520, reward_bound=0.387, batch=223\n",
      "5908: loss=0.272, reward_mean=0.360, reward_bound=0.254, batch=225\n",
      "5909: loss=0.274, reward_mean=0.360, reward_bound=0.365, batch=227\n",
      "5910: loss=0.274, reward_mean=0.370, reward_bound=0.349, batch=228\n",
      "5911: loss=0.278, reward_mean=0.480, reward_bound=0.430, batch=196\n",
      "5912: loss=0.279, reward_mean=0.490, reward_bound=0.217, batch=207\n",
      "5913: loss=0.275, reward_mean=0.510, reward_bound=0.282, batch=213\n",
      "5914: loss=0.277, reward_mean=0.450, reward_bound=0.271, batch=219\n",
      "5915: loss=0.277, reward_mean=0.370, reward_bound=0.314, batch=221\n",
      "5916: loss=0.277, reward_mean=0.460, reward_bound=0.282, batch=223\n",
      "5917: loss=0.275, reward_mean=0.440, reward_bound=0.301, batch=226\n",
      "5918: loss=0.276, reward_mean=0.450, reward_bound=0.349, batch=217\n",
      "5919: loss=0.276, reward_mean=0.390, reward_bound=0.349, batch=219\n",
      "5920: loss=0.281, reward_mean=0.470, reward_bound=0.387, batch=214\n",
      "5921: loss=0.280, reward_mean=0.470, reward_bound=0.384, batch=220\n",
      "5922: loss=0.280, reward_mean=0.400, reward_bound=0.349, batch=223\n",
      "5923: loss=0.279, reward_mean=0.420, reward_bound=0.322, batch=226\n",
      "5924: loss=0.280, reward_mean=0.430, reward_bound=0.316, batch=228\n",
      "5925: loss=0.281, reward_mean=0.410, reward_bound=0.387, batch=226\n",
      "5926: loss=0.280, reward_mean=0.470, reward_bound=0.409, batch=228\n",
      "5927: loss=0.281, reward_mean=0.440, reward_bound=0.430, batch=217\n",
      "5928: loss=0.279, reward_mean=0.390, reward_bound=0.342, batch=222\n",
      "5929: loss=0.279, reward_mean=0.460, reward_bound=0.314, batch=224\n",
      "5930: loss=0.279, reward_mean=0.530, reward_bound=0.384, batch=227\n",
      "5931: loss=0.279, reward_mean=0.370, reward_bound=0.349, batch=228\n",
      "5932: loss=0.279, reward_mean=0.410, reward_bound=0.387, batch=226\n",
      "5933: loss=0.279, reward_mean=0.380, reward_bound=0.409, batch=228\n",
      "5934: loss=0.280, reward_mean=0.470, reward_bound=0.430, batch=222\n",
      "5935: loss=0.281, reward_mean=0.510, reward_bound=0.415, batch=225\n",
      "5936: loss=0.280, reward_mean=0.390, reward_bound=0.440, batch=227\n",
      "5937: loss=0.283, reward_mean=0.460, reward_bound=0.422, batch=229\n",
      "5938: loss=0.281, reward_mean=0.510, reward_bound=0.450, batch=230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5939: loss=0.281, reward_mean=0.440, reward_bound=0.430, batch=230\n",
      "5940: loss=0.280, reward_mean=0.410, reward_bound=0.418, batch=231\n",
      "5941: loss=0.280, reward_mean=0.410, reward_bound=0.349, batch=231\n",
      "5942: loss=0.280, reward_mean=0.470, reward_bound=0.478, batch=187\n",
      "5943: loss=0.277, reward_mean=0.500, reward_bound=0.182, batch=201\n",
      "5944: loss=0.277, reward_mean=0.380, reward_bound=0.185, batch=209\n",
      "5945: loss=0.284, reward_mean=0.540, reward_bound=0.229, batch=212\n",
      "5946: loss=0.287, reward_mean=0.450, reward_bound=0.135, batch=217\n",
      "5947: loss=0.285, reward_mean=0.410, reward_bound=0.220, batch=222\n",
      "5948: loss=0.282, reward_mean=0.420, reward_bound=0.282, batch=222\n",
      "5949: loss=0.281, reward_mean=0.450, reward_bound=0.314, batch=220\n",
      "5950: loss=0.279, reward_mean=0.490, reward_bound=0.314, batch=223\n",
      "5951: loss=0.286, reward_mean=0.390, reward_bound=0.349, batch=215\n",
      "5952: loss=0.286, reward_mean=0.530, reward_bound=0.289, batch=220\n",
      "5953: loss=0.283, reward_mean=0.350, reward_bound=0.314, batch=222\n",
      "5954: loss=0.286, reward_mean=0.480, reward_bound=0.349, batch=222\n",
      "5955: loss=0.287, reward_mean=0.390, reward_bound=0.314, batch=224\n",
      "5956: loss=0.288, reward_mean=0.470, reward_bound=0.308, batch=227\n",
      "5957: loss=0.288, reward_mean=0.400, reward_bound=0.314, batch=227\n",
      "5958: loss=0.286, reward_mean=0.450, reward_bound=0.380, batch=229\n",
      "5959: loss=0.284, reward_mean=0.350, reward_bound=0.387, batch=220\n",
      "5960: loss=0.291, reward_mean=0.370, reward_bound=0.247, batch=224\n",
      "5961: loss=0.287, reward_mean=0.430, reward_bound=0.345, batch=227\n",
      "5962: loss=0.288, reward_mean=0.430, reward_bound=0.245, batch=229\n",
      "5963: loss=0.290, reward_mean=0.440, reward_bound=0.349, batch=229\n",
      "5964: loss=0.290, reward_mean=0.360, reward_bound=0.364, batch=230\n",
      "5965: loss=0.284, reward_mean=0.460, reward_bound=0.387, batch=225\n",
      "5966: loss=0.279, reward_mean=0.400, reward_bound=0.430, batch=208\n",
      "5967: loss=0.279, reward_mean=0.450, reward_bound=0.229, batch=214\n",
      "5968: loss=0.279, reward_mean=0.450, reward_bound=0.229, batch=219\n",
      "5969: loss=0.279, reward_mean=0.420, reward_bound=0.282, batch=220\n",
      "5970: loss=0.278, reward_mean=0.390, reward_bound=0.314, batch=222\n",
      "5971: loss=0.279, reward_mean=0.410, reward_bound=0.349, batch=223\n",
      "5972: loss=0.281, reward_mean=0.450, reward_bound=0.349, batch=224\n",
      "5973: loss=0.281, reward_mean=0.410, reward_bound=0.311, batch=227\n",
      "5974: loss=0.281, reward_mean=0.420, reward_bound=0.314, batch=228\n",
      "5975: loss=0.280, reward_mean=0.480, reward_bound=0.387, batch=225\n",
      "5976: loss=0.279, reward_mean=0.450, reward_bound=0.387, batch=226\n",
      "5977: loss=0.278, reward_mean=0.440, reward_bound=0.409, batch=228\n",
      "5978: loss=0.281, reward_mean=0.530, reward_bound=0.430, batch=214\n",
      "5979: loss=0.279, reward_mean=0.530, reward_bound=0.277, batch=220\n",
      "5980: loss=0.281, reward_mean=0.470, reward_bound=0.314, batch=222\n",
      "5981: loss=0.279, reward_mean=0.420, reward_bound=0.349, batch=220\n",
      "5982: loss=0.283, reward_mean=0.430, reward_bound=0.338, batch=224\n",
      "5983: loss=0.278, reward_mean=0.410, reward_bound=0.349, batch=224\n",
      "5984: loss=0.279, reward_mean=0.370, reward_bound=0.387, batch=223\n",
      "5985: loss=0.279, reward_mean=0.440, reward_bound=0.387, batch=225\n",
      "5986: loss=0.278, reward_mean=0.430, reward_bound=0.349, batch=226\n",
      "5987: loss=0.278, reward_mean=0.440, reward_bound=0.368, batch=228\n",
      "5988: loss=0.278, reward_mean=0.510, reward_bound=0.430, batch=224\n",
      "5989: loss=0.278, reward_mean=0.400, reward_bound=0.349, batch=226\n",
      "5990: loss=0.277, reward_mean=0.420, reward_bound=0.331, batch=228\n",
      "5991: loss=0.278, reward_mean=0.440, reward_bound=0.387, batch=227\n",
      "5992: loss=0.277, reward_mean=0.470, reward_bound=0.254, batch=228\n",
      "5993: loss=0.276, reward_mean=0.400, reward_bound=0.286, batch=229\n",
      "5994: loss=0.278, reward_mean=0.460, reward_bound=0.405, batch=230\n",
      "5995: loss=0.277, reward_mean=0.440, reward_bound=0.418, batch=231\n",
      "5996: loss=0.277, reward_mean=0.520, reward_bound=0.430, batch=229\n",
      "5997: loss=0.277, reward_mean=0.420, reward_bound=0.478, batch=231\n",
      "5998: loss=0.277, reward_mean=0.460, reward_bound=0.478, batch=200\n",
      "5999: loss=0.270, reward_mean=0.430, reward_bound=0.157, batch=210\n",
      "6000: loss=0.270, reward_mean=0.420, reward_bound=0.167, batch=216\n",
      "6001: loss=0.271, reward_mean=0.420, reward_bound=0.196, batch=221\n",
      "6002: loss=0.277, reward_mean=0.440, reward_bound=0.254, batch=222\n",
      "6003: loss=0.276, reward_mean=0.380, reward_bound=0.282, batch=221\n",
      "6004: loss=0.278, reward_mean=0.370, reward_bound=0.314, batch=224\n",
      "6005: loss=0.279, reward_mean=0.500, reward_bound=0.342, batch=227\n",
      "6006: loss=0.280, reward_mean=0.450, reward_bound=0.349, batch=222\n",
      "6007: loss=0.277, reward_mean=0.500, reward_bound=0.387, batch=221\n",
      "6008: loss=0.276, reward_mean=0.430, reward_bound=0.254, batch=224\n",
      "6009: loss=0.276, reward_mean=0.430, reward_bound=0.345, batch=227\n",
      "6010: loss=0.280, reward_mean=0.360, reward_bound=0.325, batch=229\n",
      "6011: loss=0.279, reward_mean=0.430, reward_bound=0.387, batch=229\n",
      "6012: loss=0.278, reward_mean=0.480, reward_bound=0.430, batch=214\n",
      "6013: loss=0.280, reward_mean=0.450, reward_bound=0.349, batch=218\n",
      "6014: loss=0.283, reward_mean=0.440, reward_bound=0.387, batch=219\n",
      "6015: loss=0.284, reward_mean=0.420, reward_bound=0.295, batch=223\n",
      "6016: loss=0.282, reward_mean=0.390, reward_bound=0.335, batch=226\n",
      "6017: loss=0.282, reward_mean=0.460, reward_bound=0.349, batch=227\n",
      "6018: loss=0.281, reward_mean=0.450, reward_bound=0.308, batch=229\n",
      "6019: loss=0.281, reward_mean=0.390, reward_bound=0.349, batch=229\n",
      "6020: loss=0.279, reward_mean=0.410, reward_bound=0.405, batch=230\n",
      "6021: loss=0.282, reward_mean=0.320, reward_bound=0.418, batch=231\n",
      "6022: loss=0.280, reward_mean=0.420, reward_bound=0.430, batch=221\n",
      "6023: loss=0.279, reward_mean=0.440, reward_bound=0.349, batch=222\n",
      "6024: loss=0.281, reward_mean=0.410, reward_bound=0.324, batch=225\n",
      "6025: loss=0.283, reward_mean=0.380, reward_bound=0.296, batch=227\n",
      "6026: loss=0.285, reward_mean=0.510, reward_bound=0.349, batch=228\n",
      "6027: loss=0.283, reward_mean=0.460, reward_bound=0.392, batch=229\n",
      "6028: loss=0.283, reward_mean=0.380, reward_bound=0.343, batch=230\n",
      "6029: loss=0.282, reward_mean=0.450, reward_bound=0.418, batch=231\n",
      "6030: loss=0.282, reward_mean=0.530, reward_bound=0.430, batch=227\n",
      "6031: loss=0.283, reward_mean=0.420, reward_bound=0.407, batch=229\n",
      "6032: loss=0.282, reward_mean=0.440, reward_bound=0.405, batch=230\n",
      "6033: loss=0.282, reward_mean=0.430, reward_bound=0.282, batch=230\n",
      "6034: loss=0.282, reward_mean=0.480, reward_bound=0.418, batch=231\n",
      "6035: loss=0.282, reward_mean=0.410, reward_bound=0.430, batch=229\n",
      "6036: loss=0.282, reward_mean=0.390, reward_bound=0.349, batch=229\n",
      "6037: loss=0.281, reward_mean=0.490, reward_bound=0.478, batch=232\n",
      "6038: loss=0.281, reward_mean=0.470, reward_bound=0.478, batch=209\n",
      "6039: loss=0.279, reward_mean=0.430, reward_bound=0.194, batch=216\n",
      "6040: loss=0.281, reward_mean=0.370, reward_bound=0.217, batch=221\n",
      "6041: loss=0.281, reward_mean=0.450, reward_bound=0.314, batch=223\n",
      "6042: loss=0.279, reward_mean=0.390, reward_bound=0.349, batch=225\n",
      "6043: loss=0.280, reward_mean=0.480, reward_bound=0.387, batch=221\n",
      "6044: loss=0.279, reward_mean=0.470, reward_bound=0.349, batch=224\n",
      "6045: loss=0.278, reward_mean=0.430, reward_bound=0.426, batch=227\n",
      "6046: loss=0.278, reward_mean=0.410, reward_bound=0.308, batch=229\n",
      "6047: loss=0.279, reward_mean=0.470, reward_bound=0.343, batch=230\n",
      "6048: loss=0.278, reward_mean=0.520, reward_bound=0.387, batch=229\n",
      "6049: loss=0.279, reward_mean=0.460, reward_bound=0.430, batch=223\n",
      "6050: loss=0.278, reward_mean=0.410, reward_bound=0.387, batch=224\n",
      "6051: loss=0.278, reward_mean=0.450, reward_bound=0.345, batch=227\n",
      "6052: loss=0.282, reward_mean=0.410, reward_bound=0.380, batch=229\n",
      "6053: loss=0.280, reward_mean=0.430, reward_bound=0.430, batch=228\n",
      "6054: loss=0.279, reward_mean=0.390, reward_bound=0.435, batch=229\n",
      "6055: loss=0.279, reward_mean=0.510, reward_bound=0.381, batch=230\n",
      "6056: loss=0.279, reward_mean=0.410, reward_bound=0.418, batch=231\n",
      "6057: loss=0.279, reward_mean=0.410, reward_bound=0.430, batch=230\n",
      "6058: loss=0.280, reward_mean=0.460, reward_bound=0.478, batch=218\n",
      "6059: loss=0.277, reward_mean=0.520, reward_bound=0.254, batch=221\n",
      "6060: loss=0.277, reward_mean=0.500, reward_bound=0.314, batch=224\n",
      "6061: loss=0.277, reward_mean=0.370, reward_bound=0.349, batch=225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6062: loss=0.279, reward_mean=0.370, reward_bound=0.356, batch=227\n",
      "6063: loss=0.279, reward_mean=0.450, reward_bound=0.349, batch=227\n",
      "6064: loss=0.279, reward_mean=0.480, reward_bound=0.249, batch=229\n",
      "6065: loss=0.279, reward_mean=0.380, reward_bound=0.387, batch=225\n",
      "6066: loss=0.278, reward_mean=0.320, reward_bound=0.321, batch=227\n",
      "6067: loss=0.281, reward_mean=0.440, reward_bound=0.349, batch=228\n",
      "6068: loss=0.283, reward_mean=0.470, reward_bound=0.430, batch=225\n",
      "6069: loss=0.283, reward_mean=0.390, reward_bound=0.396, batch=227\n",
      "6070: loss=0.287, reward_mean=0.420, reward_bound=0.335, batch=229\n",
      "6071: loss=0.284, reward_mean=0.470, reward_bound=0.349, batch=228\n",
      "6072: loss=0.283, reward_mean=0.360, reward_bound=0.353, batch=229\n",
      "6073: loss=0.283, reward_mean=0.370, reward_bound=0.364, batch=230\n",
      "6074: loss=0.284, reward_mean=0.360, reward_bound=0.376, batch=231\n",
      "6075: loss=0.285, reward_mean=0.480, reward_bound=0.387, batch=230\n",
      "6076: loss=0.284, reward_mean=0.500, reward_bound=0.430, batch=230\n",
      "6077: loss=0.283, reward_mean=0.470, reward_bound=0.430, batch=230\n",
      "6078: loss=0.283, reward_mean=0.430, reward_bound=0.418, batch=231\n",
      "6079: loss=0.283, reward_mean=0.420, reward_bound=0.430, batch=231\n",
      "6080: loss=0.280, reward_mean=0.410, reward_bound=0.478, batch=225\n",
      "6081: loss=0.280, reward_mean=0.440, reward_bound=0.387, batch=226\n",
      "6082: loss=0.285, reward_mean=0.410, reward_bound=0.430, batch=227\n",
      "6083: loss=0.285, reward_mean=0.390, reward_bound=0.366, batch=229\n",
      "6084: loss=0.284, reward_mean=0.380, reward_bound=0.387, batch=229\n",
      "6085: loss=0.285, reward_mean=0.520, reward_bound=0.430, batch=229\n",
      "6086: loss=0.284, reward_mean=0.400, reward_bound=0.430, batch=229\n",
      "6087: loss=0.285, reward_mean=0.480, reward_bound=0.450, batch=230\n",
      "6088: loss=0.285, reward_mean=0.460, reward_bound=0.451, batch=231\n",
      "6089: loss=0.284, reward_mean=0.450, reward_bound=0.478, batch=229\n",
      "6090: loss=0.284, reward_mean=0.490, reward_bound=0.430, batch=229\n",
      "6091: loss=0.284, reward_mean=0.460, reward_bound=0.430, batch=229\n",
      "6092: loss=0.284, reward_mean=0.390, reward_bound=0.478, batch=231\n",
      "6093: loss=0.284, reward_mean=0.440, reward_bound=0.478, batch=230\n",
      "6094: loss=0.284, reward_mean=0.450, reward_bound=0.515, batch=231\n",
      "6095: loss=0.284, reward_mean=0.500, reward_bound=0.430, batch=231\n",
      "6097: loss=0.244, reward_mean=0.420, reward_bound=0.000, batch=42\n",
      "6098: loss=0.247, reward_mean=0.390, reward_bound=0.000, batch=81\n",
      "6099: loss=0.237, reward_mean=0.500, reward_bound=0.000, batch=126\n",
      "6100: loss=0.242, reward_mean=0.460, reward_bound=0.002, batch=158\n",
      "6101: loss=0.241, reward_mean=0.450, reward_bound=0.006, batch=180\n",
      "6102: loss=0.248, reward_mean=0.450, reward_bound=0.016, batch=195\n",
      "6103: loss=0.248, reward_mean=0.470, reward_bound=0.035, batch=206\n",
      "6104: loss=0.252, reward_mean=0.420, reward_bound=0.047, batch=207\n",
      "6105: loss=0.252, reward_mean=0.450, reward_bound=0.065, batch=209\n",
      "6106: loss=0.253, reward_mean=0.510, reward_bound=0.089, batch=206\n",
      "6107: loss=0.255, reward_mean=0.410, reward_bound=0.109, batch=206\n",
      "6108: loss=0.253, reward_mean=0.530, reward_bound=0.122, batch=208\n",
      "6109: loss=0.248, reward_mean=0.440, reward_bound=0.135, batch=205\n",
      "6110: loss=0.255, reward_mean=0.420, reward_bound=0.150, batch=201\n",
      "6111: loss=0.265, reward_mean=0.430, reward_bound=0.167, batch=199\n",
      "6112: loss=0.272, reward_mean=0.480, reward_bound=0.185, batch=187\n",
      "6113: loss=0.276, reward_mean=0.460, reward_bound=0.150, batch=200\n",
      "6114: loss=0.270, reward_mean=0.440, reward_bound=0.167, batch=209\n",
      "6115: loss=0.268, reward_mean=0.420, reward_bound=0.206, batch=193\n",
      "6116: loss=0.266, reward_mean=0.380, reward_bound=0.178, batch=205\n",
      "6117: loss=0.266, reward_mean=0.380, reward_bound=0.210, batch=213\n",
      "6118: loss=0.266, reward_mean=0.350, reward_bound=0.229, batch=183\n",
      "6119: loss=0.274, reward_mean=0.450, reward_bound=0.254, batch=177\n",
      "6120: loss=0.277, reward_mean=0.360, reward_bound=0.102, batch=194\n",
      "6121: loss=0.272, reward_mean=0.450, reward_bound=0.135, batch=204\n",
      "6122: loss=0.270, reward_mean=0.410, reward_bound=0.167, batch=210\n",
      "6123: loss=0.274, reward_mean=0.350, reward_bound=0.185, batch=215\n",
      "6124: loss=0.272, reward_mean=0.470, reward_bound=0.206, batch=218\n",
      "6125: loss=0.272, reward_mean=0.470, reward_bound=0.254, batch=215\n",
      "6126: loss=0.274, reward_mean=0.380, reward_bound=0.240, batch=220\n",
      "6127: loss=0.266, reward_mean=0.410, reward_bound=0.282, batch=185\n",
      "6128: loss=0.262, reward_mean=0.440, reward_bound=0.109, batch=200\n",
      "6129: loss=0.268, reward_mean=0.520, reward_bound=0.150, batch=209\n",
      "6130: loss=0.265, reward_mean=0.480, reward_bound=0.185, batch=213\n",
      "6131: loss=0.262, reward_mean=0.330, reward_bound=0.206, batch=216\n",
      "6132: loss=0.265, reward_mean=0.390, reward_bound=0.254, batch=215\n",
      "6133: loss=0.267, reward_mean=0.320, reward_bound=0.240, batch=220\n",
      "6134: loss=0.269, reward_mean=0.440, reward_bound=0.282, batch=217\n",
      "6135: loss=0.269, reward_mean=0.390, reward_bound=0.254, batch=220\n",
      "6136: loss=0.276, reward_mean=0.490, reward_bound=0.314, batch=166\n",
      "6137: loss=0.271, reward_mean=0.410, reward_bound=0.109, batch=185\n",
      "6138: loss=0.265, reward_mean=0.450, reward_bound=0.135, batch=198\n",
      "6139: loss=0.261, reward_mean=0.440, reward_bound=0.112, batch=208\n",
      "6140: loss=0.262, reward_mean=0.420, reward_bound=0.185, batch=213\n",
      "6141: loss=0.263, reward_mean=0.530, reward_bound=0.206, batch=218\n",
      "6142: loss=0.263, reward_mean=0.390, reward_bound=0.229, batch=217\n",
      "6143: loss=0.266, reward_mean=0.410, reward_bound=0.254, batch=218\n",
      "6144: loss=0.266, reward_mean=0.410, reward_bound=0.282, batch=220\n",
      "6145: loss=0.263, reward_mean=0.490, reward_bound=0.314, batch=218\n",
      "6146: loss=0.276, reward_mean=0.390, reward_bound=0.349, batch=164\n",
      "6147: loss=0.266, reward_mean=0.520, reward_bound=0.072, batch=184\n",
      "6148: loss=0.265, reward_mean=0.490, reward_bound=0.150, batch=198\n",
      "6149: loss=0.268, reward_mean=0.370, reward_bound=0.167, batch=205\n",
      "6150: loss=0.266, reward_mean=0.400, reward_bound=0.170, batch=213\n",
      "6151: loss=0.273, reward_mean=0.560, reward_bound=0.229, batch=211\n",
      "6152: loss=0.273, reward_mean=0.370, reward_bound=0.206, batch=216\n",
      "6153: loss=0.278, reward_mean=0.390, reward_bound=0.254, batch=214\n",
      "6154: loss=0.275, reward_mean=0.560, reward_bound=0.277, batch=220\n",
      "6155: loss=0.275, reward_mean=0.510, reward_bound=0.282, batch=220\n",
      "6156: loss=0.279, reward_mean=0.520, reward_bound=0.314, batch=216\n",
      "6157: loss=0.278, reward_mean=0.460, reward_bound=0.298, batch=221\n",
      "6158: loss=0.277, reward_mean=0.510, reward_bound=0.314, batch=223\n",
      "6159: loss=0.277, reward_mean=0.390, reward_bound=0.314, batch=225\n",
      "6160: loss=0.275, reward_mean=0.490, reward_bound=0.349, batch=211\n",
      "6161: loss=0.274, reward_mean=0.340, reward_bound=0.206, batch=215\n",
      "6162: loss=0.275, reward_mean=0.460, reward_bound=0.314, batch=219\n",
      "6163: loss=0.275, reward_mean=0.470, reward_bound=0.349, batch=220\n",
      "6164: loss=0.277, reward_mean=0.440, reward_bound=0.349, batch=223\n",
      "6165: loss=0.276, reward_mean=0.460, reward_bound=0.282, batch=225\n",
      "6166: loss=0.259, reward_mean=0.500, reward_bound=0.387, batch=147\n",
      "6167: loss=0.247, reward_mean=0.420, reward_bound=0.032, batch=173\n",
      "6168: loss=0.259, reward_mean=0.420, reward_bound=0.072, batch=190\n",
      "6169: loss=0.260, reward_mean=0.440, reward_bound=0.135, batch=200\n",
      "6170: loss=0.259, reward_mean=0.420, reward_bound=0.150, batch=208\n",
      "6171: loss=0.255, reward_mean=0.380, reward_bound=0.124, batch=215\n",
      "6172: loss=0.250, reward_mean=0.450, reward_bound=0.185, batch=213\n",
      "6173: loss=0.257, reward_mean=0.450, reward_bound=0.206, batch=209\n",
      "6174: loss=0.256, reward_mean=0.410, reward_bound=0.206, batch=215\n",
      "6175: loss=0.251, reward_mean=0.430, reward_bound=0.229, batch=214\n",
      "6176: loss=0.246, reward_mean=0.380, reward_bound=0.254, batch=208\n",
      "6177: loss=0.244, reward_mean=0.480, reward_bound=0.282, batch=207\n",
      "6178: loss=0.245, reward_mean=0.470, reward_bound=0.249, batch=215\n",
      "6179: loss=0.245, reward_mean=0.340, reward_bound=0.179, batch=220\n",
      "6180: loss=0.247, reward_mean=0.410, reward_bound=0.247, batch=224\n",
      "6181: loss=0.243, reward_mean=0.480, reward_bound=0.254, batch=226\n",
      "6182: loss=0.243, reward_mean=0.460, reward_bound=0.282, batch=227\n",
      "6183: loss=0.244, reward_mean=0.440, reward_bound=0.314, batch=212\n",
      "6184: loss=0.246, reward_mean=0.480, reward_bound=0.236, batch=218\n",
      "6185: loss=0.242, reward_mean=0.420, reward_bound=0.257, batch=222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6186: loss=0.241, reward_mean=0.480, reward_bound=0.292, batch=225\n",
      "6187: loss=0.246, reward_mean=0.510, reward_bound=0.349, batch=203\n",
      "6188: loss=0.247, reward_mean=0.420, reward_bound=0.254, batch=210\n",
      "6189: loss=0.249, reward_mean=0.470, reward_bound=0.274, batch=217\n",
      "6190: loss=0.247, reward_mean=0.480, reward_bound=0.229, batch=221\n",
      "6191: loss=0.247, reward_mean=0.400, reward_bound=0.254, batch=223\n",
      "6192: loss=0.246, reward_mean=0.480, reward_bound=0.282, batch=224\n",
      "6193: loss=0.245, reward_mean=0.470, reward_bound=0.314, batch=226\n",
      "6194: loss=0.244, reward_mean=0.470, reward_bound=0.349, batch=219\n",
      "6195: loss=0.244, reward_mean=0.470, reward_bound=0.314, batch=222\n",
      "6196: loss=0.242, reward_mean=0.460, reward_bound=0.349, batch=224\n",
      "6197: loss=0.242, reward_mean=0.500, reward_bound=0.349, batch=226\n",
      "6198: loss=0.241, reward_mean=0.490, reward_bound=0.316, batch=228\n",
      "6199: loss=0.243, reward_mean=0.420, reward_bound=0.349, batch=228\n",
      "6200: loss=0.243, reward_mean=0.580, reward_bound=0.387, batch=212\n",
      "6201: loss=0.242, reward_mean=0.410, reward_bound=0.229, batch=217\n",
      "6202: loss=0.242, reward_mean=0.410, reward_bound=0.387, batch=219\n",
      "6203: loss=0.240, reward_mean=0.490, reward_bound=0.265, batch=223\n",
      "6204: loss=0.241, reward_mean=0.440, reward_bound=0.301, batch=226\n",
      "6205: loss=0.243, reward_mean=0.530, reward_bound=0.314, batch=227\n",
      "6206: loss=0.244, reward_mean=0.420, reward_bound=0.349, batch=228\n",
      "6207: loss=0.260, reward_mean=0.370, reward_bound=0.430, batch=121\n",
      "6208: loss=0.233, reward_mean=0.330, reward_bound=0.000, batch=154\n",
      "6209: loss=0.233, reward_mean=0.550, reward_bound=0.020, batch=178\n",
      "6210: loss=0.251, reward_mean=0.570, reward_bound=0.065, batch=191\n",
      "6211: loss=0.259, reward_mean=0.480, reward_bound=0.109, batch=199\n",
      "6212: loss=0.265, reward_mean=0.380, reward_bound=0.135, batch=203\n",
      "6213: loss=0.262, reward_mean=0.440, reward_bound=0.160, batch=212\n",
      "6214: loss=0.268, reward_mean=0.480, reward_bound=0.167, batch=213\n",
      "6215: loss=0.265, reward_mean=0.490, reward_bound=0.185, batch=212\n",
      "6216: loss=0.266, reward_mean=0.340, reward_bound=0.206, batch=221\n",
      "6217: loss=0.267, reward_mean=0.440, reward_bound=0.206, batch=220\n",
      "6218: loss=0.272, reward_mean=0.430, reward_bound=0.229, batch=204\n",
      "6219: loss=0.277, reward_mean=0.420, reward_bound=0.167, batch=212\n",
      "6220: loss=0.271, reward_mean=0.470, reward_bound=0.254, batch=216\n",
      "6221: loss=0.269, reward_mean=0.340, reward_bound=0.256, batch=221\n",
      "6222: loss=0.267, reward_mean=0.440, reward_bound=0.282, batch=205\n",
      "6223: loss=0.269, reward_mean=0.400, reward_bound=0.260, batch=213\n",
      "6224: loss=0.270, reward_mean=0.380, reward_bound=0.282, batch=215\n",
      "6225: loss=0.269, reward_mean=0.420, reward_bound=0.289, batch=220\n",
      "6226: loss=0.272, reward_mean=0.340, reward_bound=0.288, batch=224\n",
      "6227: loss=0.274, reward_mean=0.350, reward_bound=0.314, batch=202\n",
      "6228: loss=0.277, reward_mean=0.420, reward_bound=0.185, batch=210\n",
      "6229: loss=0.275, reward_mean=0.380, reward_bound=0.229, batch=216\n",
      "6230: loss=0.278, reward_mean=0.420, reward_bound=0.282, batch=220\n",
      "6231: loss=0.274, reward_mean=0.470, reward_bound=0.314, batch=219\n",
      "6232: loss=0.276, reward_mean=0.400, reward_bound=0.309, batch=223\n",
      "6233: loss=0.272, reward_mean=0.510, reward_bound=0.349, batch=193\n",
      "6234: loss=0.270, reward_mean=0.360, reward_bound=0.160, batch=205\n",
      "6235: loss=0.267, reward_mean=0.460, reward_bound=0.194, batch=213\n",
      "6236: loss=0.272, reward_mean=0.430, reward_bound=0.254, batch=216\n",
      "6237: loss=0.270, reward_mean=0.430, reward_bound=0.282, batch=218\n",
      "6238: loss=0.269, reward_mean=0.330, reward_bound=0.231, batch=222\n",
      "6239: loss=0.269, reward_mean=0.360, reward_bound=0.282, batch=224\n",
      "6240: loss=0.268, reward_mean=0.430, reward_bound=0.311, batch=227\n",
      "6241: loss=0.266, reward_mean=0.380, reward_bound=0.314, batch=224\n",
      "6242: loss=0.266, reward_mean=0.370, reward_bound=0.314, batch=226\n",
      "6243: loss=0.267, reward_mean=0.390, reward_bound=0.331, batch=228\n",
      "6244: loss=0.272, reward_mean=0.390, reward_bound=0.349, batch=223\n",
      "6245: loss=0.279, reward_mean=0.500, reward_bound=0.387, batch=189\n",
      "6246: loss=0.274, reward_mean=0.440, reward_bound=0.135, batch=201\n",
      "6247: loss=0.280, reward_mean=0.450, reward_bound=0.150, batch=209\n",
      "6248: loss=0.276, reward_mean=0.480, reward_bound=0.215, batch=216\n",
      "6249: loss=0.283, reward_mean=0.430, reward_bound=0.254, batch=214\n",
      "6250: loss=0.282, reward_mean=0.460, reward_bound=0.204, batch=220\n",
      "6251: loss=0.279, reward_mean=0.470, reward_bound=0.229, batch=223\n",
      "6252: loss=0.281, reward_mean=0.420, reward_bound=0.282, batch=222\n",
      "6253: loss=0.280, reward_mean=0.390, reward_bound=0.236, batch=225\n",
      "6254: loss=0.283, reward_mean=0.440, reward_bound=0.314, batch=218\n",
      "6255: loss=0.283, reward_mean=0.390, reward_bound=0.349, batch=208\n",
      "6256: loss=0.280, reward_mean=0.390, reward_bound=0.257, batch=215\n",
      "6257: loss=0.283, reward_mean=0.370, reward_bound=0.314, batch=219\n",
      "6258: loss=0.284, reward_mean=0.420, reward_bound=0.349, batch=221\n",
      "6259: loss=0.287, reward_mean=0.430, reward_bound=0.387, batch=207\n",
      "6260: loss=0.286, reward_mean=0.510, reward_bound=0.308, batch=215\n",
      "6261: loss=0.287, reward_mean=0.440, reward_bound=0.314, batch=217\n",
      "6262: loss=0.287, reward_mean=0.390, reward_bound=0.249, batch=222\n",
      "6263: loss=0.287, reward_mean=0.460, reward_bound=0.314, batch=224\n",
      "6264: loss=0.288, reward_mean=0.460, reward_bound=0.349, batch=223\n",
      "6265: loss=0.290, reward_mean=0.500, reward_bound=0.334, batch=226\n",
      "6266: loss=0.295, reward_mean=0.510, reward_bound=0.387, batch=223\n",
      "6267: loss=0.294, reward_mean=0.470, reward_bound=0.349, batch=224\n",
      "6268: loss=0.293, reward_mean=0.420, reward_bound=0.349, batch=226\n",
      "6269: loss=0.292, reward_mean=0.350, reward_bound=0.372, batch=228\n",
      "6270: loss=0.281, reward_mean=0.380, reward_bound=0.430, batch=177\n",
      "6271: loss=0.273, reward_mean=0.440, reward_bound=0.097, batch=194\n",
      "6272: loss=0.279, reward_mean=0.370, reward_bound=0.122, batch=203\n",
      "6273: loss=0.284, reward_mean=0.470, reward_bound=0.167, batch=210\n",
      "6274: loss=0.285, reward_mean=0.300, reward_bound=0.185, batch=216\n",
      "6275: loss=0.283, reward_mean=0.470, reward_bound=0.206, batch=220\n",
      "6276: loss=0.283, reward_mean=0.360, reward_bound=0.229, batch=219\n",
      "6277: loss=0.285, reward_mean=0.480, reward_bound=0.254, batch=219\n",
      "6278: loss=0.278, reward_mean=0.400, reward_bound=0.282, batch=218\n",
      "6279: loss=0.280, reward_mean=0.500, reward_bound=0.254, batch=221\n",
      "6280: loss=0.281, reward_mean=0.380, reward_bound=0.254, batch=224\n",
      "6281: loss=0.284, reward_mean=0.510, reward_bound=0.314, batch=218\n",
      "6282: loss=0.282, reward_mean=0.370, reward_bound=0.289, batch=222\n",
      "6283: loss=0.286, reward_mean=0.420, reward_bound=0.349, batch=211\n",
      "6284: loss=0.285, reward_mean=0.400, reward_bound=0.254, batch=217\n",
      "6285: loss=0.288, reward_mean=0.480, reward_bound=0.314, batch=220\n",
      "6286: loss=0.281, reward_mean=0.460, reward_bound=0.376, batch=224\n",
      "6287: loss=0.281, reward_mean=0.490, reward_bound=0.282, batch=226\n",
      "6288: loss=0.276, reward_mean=0.470, reward_bound=0.387, batch=218\n",
      "6289: loss=0.276, reward_mean=0.400, reward_bound=0.286, batch=222\n",
      "6290: loss=0.277, reward_mean=0.410, reward_bound=0.314, batch=222\n",
      "6291: loss=0.275, reward_mean=0.440, reward_bound=0.400, batch=225\n",
      "6292: loss=0.274, reward_mean=0.340, reward_bound=0.303, batch=227\n",
      "6293: loss=0.273, reward_mean=0.480, reward_bound=0.422, batch=229\n",
      "6294: loss=0.273, reward_mean=0.420, reward_bound=0.405, batch=230\n",
      "6295: loss=0.279, reward_mean=0.420, reward_bound=0.430, batch=199\n",
      "6296: loss=0.277, reward_mean=0.410, reward_bound=0.282, batch=207\n",
      "6297: loss=0.278, reward_mean=0.380, reward_bound=0.277, batch=215\n",
      "6298: loss=0.275, reward_mean=0.450, reward_bound=0.314, batch=219\n",
      "6299: loss=0.275, reward_mean=0.450, reward_bound=0.282, batch=222\n",
      "6300: loss=0.276, reward_mean=0.470, reward_bound=0.283, batch=225\n",
      "6301: loss=0.275, reward_mean=0.350, reward_bound=0.349, batch=220\n",
      "6302: loss=0.275, reward_mean=0.520, reward_bound=0.376, batch=224\n",
      "6303: loss=0.275, reward_mean=0.470, reward_bound=0.384, batch=227\n",
      "6304: loss=0.276, reward_mean=0.460, reward_bound=0.387, batch=221\n",
      "6305: loss=0.273, reward_mean=0.370, reward_bound=0.254, batch=224\n",
      "6306: loss=0.273, reward_mean=0.440, reward_bound=0.280, batch=227\n",
      "6307: loss=0.275, reward_mean=0.500, reward_bound=0.282, batch=228\n",
      "6308: loss=0.275, reward_mean=0.520, reward_bound=0.392, batch=229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6309: loss=0.277, reward_mean=0.450, reward_bound=0.430, batch=220\n",
      "6310: loss=0.277, reward_mean=0.390, reward_bound=0.282, batch=222\n",
      "6311: loss=0.276, reward_mean=0.350, reward_bound=0.360, batch=225\n",
      "6312: loss=0.279, reward_mean=0.520, reward_bound=0.396, batch=227\n",
      "6313: loss=0.284, reward_mean=0.360, reward_bound=0.380, batch=229\n",
      "6314: loss=0.283, reward_mean=0.360, reward_bound=0.364, batch=230\n",
      "6315: loss=0.281, reward_mean=0.460, reward_bound=0.387, batch=229\n",
      "6316: loss=0.281, reward_mean=0.410, reward_bound=0.405, batch=230\n",
      "6317: loss=0.277, reward_mean=0.470, reward_bound=0.430, batch=224\n",
      "6318: loss=0.277, reward_mean=0.500, reward_bound=0.349, batch=225\n",
      "6319: loss=0.277, reward_mean=0.430, reward_bound=0.430, batch=225\n",
      "6320: loss=0.278, reward_mean=0.420, reward_bound=0.356, batch=227\n",
      "6321: loss=0.279, reward_mean=0.410, reward_bound=0.380, batch=229\n",
      "6322: loss=0.277, reward_mean=0.420, reward_bound=0.387, batch=229\n",
      "6323: loss=0.277, reward_mean=0.430, reward_bound=0.478, batch=232\n",
      "6324: loss=0.277, reward_mean=0.390, reward_bound=0.445, batch=232\n",
      "6325: loss=0.279, reward_mean=0.470, reward_bound=0.478, batch=99\n",
      "6326: loss=0.237, reward_mean=0.370, reward_bound=0.000, batch=136\n",
      "6327: loss=0.237, reward_mean=0.440, reward_bound=0.016, batch=165\n",
      "6328: loss=0.254, reward_mean=0.490, reward_bound=0.043, batch=185\n",
      "6329: loss=0.256, reward_mean=0.390, reward_bound=0.053, batch=199\n",
      "6330: loss=0.265, reward_mean=0.420, reward_bound=0.080, batch=206\n",
      "6331: loss=0.269, reward_mean=0.400, reward_bound=0.098, batch=211\n",
      "6332: loss=0.275, reward_mean=0.420, reward_bound=0.109, batch=216\n",
      "6333: loss=0.268, reward_mean=0.440, reward_bound=0.150, batch=220\n",
      "6334: loss=0.268, reward_mean=0.470, reward_bound=0.167, batch=221\n",
      "6335: loss=0.278, reward_mean=0.440, reward_bound=0.185, batch=217\n",
      "6336: loss=0.285, reward_mean=0.380, reward_bound=0.206, batch=209\n",
      "6337: loss=0.283, reward_mean=0.520, reward_bound=0.229, batch=208\n",
      "6338: loss=0.279, reward_mean=0.450, reward_bound=0.231, batch=215\n",
      "6339: loss=0.279, reward_mean=0.430, reward_bound=0.206, batch=219\n",
      "6340: loss=0.289, reward_mean=0.420, reward_bound=0.254, batch=196\n",
      "6341: loss=0.288, reward_mean=0.390, reward_bound=0.282, batch=184\n",
      "6342: loss=0.279, reward_mean=0.400, reward_bound=0.120, batch=199\n",
      "6343: loss=0.282, reward_mean=0.380, reward_bound=0.135, batch=208\n",
      "6344: loss=0.282, reward_mean=0.500, reward_bound=0.206, batch=214\n",
      "6345: loss=0.284, reward_mean=0.450, reward_bound=0.226, batch=220\n",
      "6346: loss=0.285, reward_mean=0.420, reward_bound=0.247, batch=224\n",
      "6347: loss=0.284, reward_mean=0.430, reward_bound=0.280, batch=227\n",
      "6348: loss=0.284, reward_mean=0.370, reward_bound=0.254, batch=228\n",
      "6349: loss=0.281, reward_mean=0.400, reward_bound=0.282, batch=226\n",
      "6350: loss=0.283, reward_mean=0.470, reward_bound=0.314, batch=203\n",
      "6351: loss=0.280, reward_mean=0.380, reward_bound=0.220, batch=212\n",
      "6352: loss=0.281, reward_mean=0.460, reward_bound=0.254, batch=216\n",
      "6353: loss=0.285, reward_mean=0.370, reward_bound=0.282, batch=219\n",
      "6354: loss=0.287, reward_mean=0.510, reward_bound=0.328, batch=223\n",
      "6355: loss=0.288, reward_mean=0.470, reward_bound=0.301, batch=226\n",
      "6356: loss=0.287, reward_mean=0.450, reward_bound=0.314, batch=227\n",
      "6357: loss=0.283, reward_mean=0.520, reward_bound=0.349, batch=193\n",
      "6358: loss=0.278, reward_mean=0.430, reward_bound=0.122, batch=204\n",
      "6359: loss=0.277, reward_mean=0.450, reward_bound=0.135, batch=212\n",
      "6360: loss=0.280, reward_mean=0.510, reward_bound=0.229, batch=214\n",
      "6361: loss=0.279, reward_mean=0.500, reward_bound=0.252, batch=220\n",
      "6362: loss=0.276, reward_mean=0.430, reward_bound=0.254, batch=223\n",
      "6363: loss=0.275, reward_mean=0.430, reward_bound=0.282, batch=222\n",
      "6364: loss=0.275, reward_mean=0.410, reward_bound=0.292, batch=225\n",
      "6365: loss=0.280, reward_mean=0.450, reward_bound=0.314, batch=219\n",
      "6366: loss=0.280, reward_mean=0.420, reward_bound=0.328, batch=223\n",
      "6367: loss=0.279, reward_mean=0.470, reward_bound=0.314, batch=225\n",
      "6368: loss=0.278, reward_mean=0.430, reward_bound=0.282, batch=226\n",
      "6369: loss=0.275, reward_mean=0.500, reward_bound=0.349, batch=221\n",
      "6370: loss=0.275, reward_mean=0.470, reward_bound=0.282, batch=224\n",
      "6371: loss=0.273, reward_mean=0.440, reward_bound=0.345, batch=227\n",
      "6372: loss=0.282, reward_mean=0.390, reward_bound=0.387, batch=187\n",
      "6373: loss=0.274, reward_mean=0.380, reward_bound=0.167, batch=199\n",
      "6374: loss=0.275, reward_mean=0.430, reward_bound=0.206, batch=208\n",
      "6375: loss=0.277, reward_mean=0.440, reward_bound=0.229, batch=212\n",
      "6376: loss=0.279, reward_mean=0.470, reward_bound=0.254, batch=217\n",
      "6377: loss=0.278, reward_mean=0.330, reward_bound=0.282, batch=214\n",
      "6378: loss=0.277, reward_mean=0.540, reward_bound=0.282, batch=219\n",
      "6379: loss=0.283, reward_mean=0.450, reward_bound=0.314, batch=219\n",
      "6380: loss=0.281, reward_mean=0.430, reward_bound=0.328, batch=223\n",
      "6381: loss=0.280, reward_mean=0.430, reward_bound=0.314, batch=225\n",
      "6382: loss=0.280, reward_mean=0.410, reward_bound=0.314, batch=226\n",
      "6383: loss=0.285, reward_mean=0.470, reward_bound=0.349, batch=221\n",
      "6384: loss=0.283, reward_mean=0.360, reward_bound=0.314, batch=224\n",
      "6385: loss=0.282, reward_mean=0.330, reward_bound=0.380, batch=227\n",
      "6386: loss=0.281, reward_mean=0.420, reward_bound=0.342, batch=229\n",
      "6387: loss=0.281, reward_mean=0.520, reward_bound=0.349, batch=229\n",
      "6388: loss=0.278, reward_mean=0.340, reward_bound=0.387, batch=216\n",
      "6389: loss=0.280, reward_mean=0.390, reward_bound=0.254, batch=220\n",
      "6390: loss=0.281, reward_mean=0.360, reward_bound=0.338, batch=224\n",
      "6391: loss=0.281, reward_mean=0.450, reward_bound=0.345, batch=227\n",
      "6392: loss=0.280, reward_mean=0.380, reward_bound=0.387, batch=223\n",
      "6393: loss=0.278, reward_mean=0.500, reward_bound=0.372, batch=226\n",
      "6394: loss=0.281, reward_mean=0.420, reward_bound=0.368, batch=228\n",
      "6395: loss=0.278, reward_mean=0.470, reward_bound=0.430, batch=178\n",
      "6396: loss=0.274, reward_mean=0.490, reward_bound=0.208, batch=194\n",
      "6397: loss=0.272, reward_mean=0.420, reward_bound=0.223, batch=206\n",
      "6398: loss=0.271, reward_mean=0.400, reward_bound=0.207, batch=214\n",
      "6399: loss=0.269, reward_mean=0.390, reward_bound=0.165, batch=220\n",
      "6400: loss=0.273, reward_mean=0.550, reward_bound=0.229, batch=221\n",
      "6401: loss=0.274, reward_mean=0.390, reward_bound=0.254, batch=222\n",
      "6402: loss=0.275, reward_mean=0.400, reward_bound=0.282, batch=217\n",
      "6403: loss=0.277, reward_mean=0.420, reward_bound=0.314, batch=208\n",
      "6404: loss=0.279, reward_mean=0.410, reward_bound=0.231, batch=215\n",
      "6405: loss=0.272, reward_mean=0.410, reward_bound=0.282, batch=216\n",
      "6406: loss=0.270, reward_mean=0.340, reward_bound=0.254, batch=220\n",
      "6407: loss=0.272, reward_mean=0.500, reward_bound=0.314, batch=220\n",
      "6408: loss=0.271, reward_mean=0.510, reward_bound=0.247, batch=224\n",
      "6409: loss=0.273, reward_mean=0.500, reward_bound=0.314, batch=226\n",
      "6410: loss=0.279, reward_mean=0.400, reward_bound=0.349, batch=212\n",
      "6411: loss=0.276, reward_mean=0.380, reward_bound=0.236, batch=218\n",
      "6412: loss=0.278, reward_mean=0.470, reward_bound=0.314, batch=219\n",
      "6413: loss=0.283, reward_mean=0.470, reward_bound=0.349, batch=222\n",
      "6414: loss=0.282, reward_mean=0.460, reward_bound=0.360, batch=225\n",
      "6415: loss=0.281, reward_mean=0.390, reward_bound=0.329, batch=227\n",
      "6416: loss=0.281, reward_mean=0.350, reward_bound=0.342, batch=229\n",
      "6417: loss=0.280, reward_mean=0.430, reward_bound=0.364, batch=230\n",
      "6418: loss=0.276, reward_mean=0.350, reward_bound=0.387, batch=208\n",
      "6419: loss=0.275, reward_mean=0.460, reward_bound=0.171, batch=215\n",
      "6420: loss=0.276, reward_mean=0.380, reward_bound=0.206, batch=218\n",
      "6421: loss=0.274, reward_mean=0.290, reward_bound=0.190, batch=222\n",
      "6422: loss=0.274, reward_mean=0.460, reward_bound=0.263, batch=225\n",
      "6423: loss=0.277, reward_mean=0.450, reward_bound=0.282, batch=221\n",
      "6424: loss=0.274, reward_mean=0.370, reward_bound=0.314, batch=220\n",
      "6425: loss=0.274, reward_mean=0.390, reward_bound=0.314, batch=223\n",
      "6426: loss=0.275, reward_mean=0.430, reward_bound=0.349, batch=219\n",
      "6427: loss=0.275, reward_mean=0.460, reward_bound=0.387, batch=220\n",
      "6428: loss=0.274, reward_mean=0.460, reward_bound=0.314, batch=223\n",
      "6429: loss=0.274, reward_mean=0.390, reward_bound=0.301, batch=226\n",
      "6430: loss=0.274, reward_mean=0.390, reward_bound=0.331, batch=228\n",
      "6431: loss=0.275, reward_mean=0.320, reward_bound=0.349, batch=228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6432: loss=0.275, reward_mean=0.420, reward_bound=0.387, batch=228\n",
      "6433: loss=0.274, reward_mean=0.450, reward_bound=0.430, batch=203\n",
      "6434: loss=0.268, reward_mean=0.380, reward_bound=0.091, batch=212\n",
      "6435: loss=0.271, reward_mean=0.550, reward_bound=0.213, batch=218\n",
      "6436: loss=0.273, reward_mean=0.420, reward_bound=0.254, batch=220\n",
      "6437: loss=0.274, reward_mean=0.390, reward_bound=0.314, batch=223\n",
      "6438: loss=0.274, reward_mean=0.320, reward_bound=0.349, batch=222\n",
      "6439: loss=0.272, reward_mean=0.510, reward_bound=0.360, batch=225\n",
      "6440: loss=0.267, reward_mean=0.500, reward_bound=0.387, batch=222\n",
      "6441: loss=0.267, reward_mean=0.520, reward_bound=0.373, batch=225\n",
      "6442: loss=0.270, reward_mean=0.390, reward_bound=0.337, batch=227\n",
      "6443: loss=0.269, reward_mean=0.380, reward_bound=0.335, batch=229\n",
      "6444: loss=0.271, reward_mean=0.460, reward_bound=0.364, batch=230\n",
      "6445: loss=0.272, reward_mean=0.400, reward_bound=0.387, batch=229\n",
      "6446: loss=0.273, reward_mean=0.520, reward_bound=0.430, batch=223\n",
      "6447: loss=0.275, reward_mean=0.470, reward_bound=0.398, batch=226\n",
      "6448: loss=0.276, reward_mean=0.390, reward_bound=0.430, batch=225\n",
      "6449: loss=0.275, reward_mean=0.460, reward_bound=0.396, batch=227\n",
      "6450: loss=0.275, reward_mean=0.450, reward_bound=0.430, batch=227\n",
      "6451: loss=0.274, reward_mean=0.470, reward_bound=0.308, batch=229\n",
      "6452: loss=0.275, reward_mean=0.360, reward_bound=0.387, batch=228\n",
      "6453: loss=0.274, reward_mean=0.470, reward_bound=0.478, batch=230\n",
      "6454: loss=0.270, reward_mean=0.480, reward_bound=0.478, batch=145\n",
      "6455: loss=0.251, reward_mean=0.540, reward_bound=0.075, batch=171\n",
      "6456: loss=0.246, reward_mean=0.460, reward_bound=0.098, batch=187\n",
      "6457: loss=0.241, reward_mean=0.420, reward_bound=0.089, batch=203\n",
      "6458: loss=0.255, reward_mean=0.460, reward_bound=0.135, batch=210\n",
      "6459: loss=0.260, reward_mean=0.500, reward_bound=0.150, batch=216\n",
      "6460: loss=0.250, reward_mean=0.420, reward_bound=0.167, batch=216\n",
      "6461: loss=0.255, reward_mean=0.480, reward_bound=0.206, batch=214\n",
      "6462: loss=0.258, reward_mean=0.400, reward_bound=0.229, batch=207\n",
      "6463: loss=0.259, reward_mean=0.480, reward_bound=0.206, batch=214\n",
      "6464: loss=0.259, reward_mean=0.380, reward_bound=0.252, batch=220\n",
      "6465: loss=0.257, reward_mean=0.450, reward_bound=0.254, batch=212\n",
      "6466: loss=0.255, reward_mean=0.400, reward_bound=0.263, batch=218\n",
      "6467: loss=0.258, reward_mean=0.500, reward_bound=0.282, batch=206\n",
      "6468: loss=0.257, reward_mean=0.480, reward_bound=0.256, batch=214\n",
      "6469: loss=0.267, reward_mean=0.500, reward_bound=0.314, batch=205\n",
      "6470: loss=0.265, reward_mean=0.480, reward_bound=0.229, batch=212\n",
      "6471: loss=0.260, reward_mean=0.400, reward_bound=0.126, batch=218\n",
      "6472: loss=0.263, reward_mean=0.480, reward_bound=0.211, batch=222\n",
      "6473: loss=0.260, reward_mean=0.420, reward_bound=0.254, batch=223\n",
      "6474: loss=0.261, reward_mean=0.390, reward_bound=0.282, batch=222\n",
      "6475: loss=0.262, reward_mean=0.430, reward_bound=0.349, batch=205\n",
      "6476: loss=0.267, reward_mean=0.430, reward_bound=0.229, batch=211\n",
      "6477: loss=0.265, reward_mean=0.410, reward_bound=0.229, batch=217\n",
      "6478: loss=0.263, reward_mean=0.450, reward_bound=0.282, batch=221\n",
      "6479: loss=0.262, reward_mean=0.350, reward_bound=0.229, batch=223\n",
      "6480: loss=0.261, reward_mean=0.370, reward_bound=0.301, batch=226\n",
      "6481: loss=0.259, reward_mean=0.410, reward_bound=0.314, batch=226\n",
      "6482: loss=0.257, reward_mean=0.360, reward_bound=0.368, batch=228\n",
      "6483: loss=0.257, reward_mean=0.490, reward_bound=0.387, batch=200\n",
      "6484: loss=0.255, reward_mean=0.410, reward_bound=0.247, batch=210\n",
      "6485: loss=0.253, reward_mean=0.460, reward_bound=0.254, batch=214\n",
      "6486: loss=0.257, reward_mean=0.480, reward_bound=0.311, batch=220\n",
      "6487: loss=0.256, reward_mean=0.420, reward_bound=0.314, batch=220\n",
      "6488: loss=0.260, reward_mean=0.420, reward_bound=0.314, batch=223\n",
      "6489: loss=0.258, reward_mean=0.490, reward_bound=0.335, batch=226\n",
      "6490: loss=0.257, reward_mean=0.400, reward_bound=0.331, batch=228\n",
      "6491: loss=0.257, reward_mean=0.420, reward_bound=0.289, batch=229\n",
      "6492: loss=0.256, reward_mean=0.500, reward_bound=0.349, batch=223\n",
      "6493: loss=0.255, reward_mean=0.400, reward_bound=0.335, batch=226\n",
      "6494: loss=0.255, reward_mean=0.430, reward_bound=0.331, batch=228\n",
      "6495: loss=0.251, reward_mean=0.460, reward_bound=0.387, batch=223\n",
      "6496: loss=0.251, reward_mean=0.500, reward_bound=0.372, batch=226\n",
      "6497: loss=0.250, reward_mean=0.300, reward_bound=0.368, batch=228\n",
      "6498: loss=0.250, reward_mean=0.400, reward_bound=0.282, batch=228\n",
      "6499: loss=0.250, reward_mean=0.460, reward_bound=0.387, batch=226\n",
      "6500: loss=0.251, reward_mean=0.490, reward_bound=0.387, batch=227\n",
      "6501: loss=0.250, reward_mean=0.450, reward_bound=0.387, batch=228\n",
      "6502: loss=0.261, reward_mean=0.420, reward_bound=0.430, batch=191\n",
      "6503: loss=0.256, reward_mean=0.360, reward_bound=0.150, batch=203\n",
      "6504: loss=0.256, reward_mean=0.500, reward_bound=0.150, batch=210\n",
      "6505: loss=0.254, reward_mean=0.430, reward_bound=0.200, batch=217\n",
      "6506: loss=0.259, reward_mean=0.360, reward_bound=0.206, batch=221\n",
      "6507: loss=0.257, reward_mean=0.380, reward_bound=0.229, batch=222\n",
      "6508: loss=0.257, reward_mean=0.440, reward_bound=0.282, batch=222\n",
      "6509: loss=0.260, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "6510: loss=0.264, reward_mean=0.490, reward_bound=0.349, batch=211\n",
      "6511: loss=0.262, reward_mean=0.510, reward_bound=0.314, batch=217\n",
      "6512: loss=0.261, reward_mean=0.540, reward_bound=0.349, batch=220\n",
      "6513: loss=0.261, reward_mean=0.420, reward_bound=0.349, batch=223\n",
      "6514: loss=0.261, reward_mean=0.550, reward_bound=0.372, batch=226\n",
      "6515: loss=0.262, reward_mean=0.330, reward_bound=0.387, batch=216\n",
      "6516: loss=0.261, reward_mean=0.420, reward_bound=0.331, batch=221\n",
      "6517: loss=0.261, reward_mean=0.380, reward_bound=0.314, batch=223\n",
      "6518: loss=0.261, reward_mean=0.350, reward_bound=0.349, batch=225\n",
      "6519: loss=0.267, reward_mean=0.490, reward_bound=0.387, batch=224\n",
      "6520: loss=0.266, reward_mean=0.430, reward_bound=0.342, batch=227\n",
      "6521: loss=0.266, reward_mean=0.360, reward_bound=0.380, batch=229\n",
      "6522: loss=0.266, reward_mean=0.450, reward_bound=0.343, batch=230\n",
      "6523: loss=0.267, reward_mean=0.380, reward_bound=0.418, batch=231\n",
      "6524: loss=0.265, reward_mean=0.520, reward_bound=0.430, batch=212\n",
      "6525: loss=0.263, reward_mean=0.510, reward_bound=0.263, batch=218\n",
      "6526: loss=0.263, reward_mean=0.470, reward_bound=0.317, batch=222\n",
      "6527: loss=0.267, reward_mean=0.490, reward_bound=0.349, batch=222\n",
      "6528: loss=0.265, reward_mean=0.440, reward_bound=0.360, batch=225\n",
      "6529: loss=0.265, reward_mean=0.370, reward_bound=0.356, batch=227\n",
      "6530: loss=0.264, reward_mean=0.480, reward_bound=0.387, batch=224\n",
      "6531: loss=0.264, reward_mean=0.490, reward_bound=0.387, batch=225\n",
      "6532: loss=0.264, reward_mean=0.440, reward_bound=0.430, batch=224\n",
      "6533: loss=0.264, reward_mean=0.480, reward_bound=0.426, batch=227\n",
      "6534: loss=0.263, reward_mean=0.390, reward_bound=0.414, batch=229\n",
      "6535: loss=0.263, reward_mean=0.490, reward_bound=0.381, batch=230\n",
      "6536: loss=0.263, reward_mean=0.490, reward_bound=0.418, batch=231\n",
      "6537: loss=0.263, reward_mean=0.460, reward_bound=0.349, batch=231\n",
      "6538: loss=0.263, reward_mean=0.440, reward_bound=0.349, batch=231\n",
      "6539: loss=0.263, reward_mean=0.510, reward_bound=0.387, batch=231\n",
      "6540: loss=0.263, reward_mean=0.390, reward_bound=0.430, batch=228\n",
      "6541: loss=0.262, reward_mean=0.470, reward_bound=0.478, batch=230\n",
      "6542: loss=0.262, reward_mean=0.430, reward_bound=0.320, batch=231\n",
      "6543: loss=0.263, reward_mean=0.350, reward_bound=0.387, batch=231\n",
      "6544: loss=0.262, reward_mean=0.360, reward_bound=0.430, batch=231\n",
      "6545: loss=0.265, reward_mean=0.520, reward_bound=0.478, batch=177\n",
      "6546: loss=0.267, reward_mean=0.400, reward_bound=0.117, batch=194\n",
      "6547: loss=0.262, reward_mean=0.300, reward_bound=0.122, batch=203\n",
      "6548: loss=0.262, reward_mean=0.470, reward_bound=0.185, batch=211\n",
      "6549: loss=0.271, reward_mean=0.440, reward_bound=0.206, batch=216\n",
      "6550: loss=0.267, reward_mean=0.460, reward_bound=0.229, batch=219\n",
      "6551: loss=0.265, reward_mean=0.420, reward_bound=0.254, batch=214\n",
      "6552: loss=0.269, reward_mean=0.360, reward_bound=0.249, batch=220\n",
      "6553: loss=0.267, reward_mean=0.440, reward_bound=0.282, batch=216\n",
      "6554: loss=0.264, reward_mean=0.490, reward_bound=0.314, batch=212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6555: loss=0.262, reward_mean=0.450, reward_bound=0.254, batch=217\n",
      "6556: loss=0.258, reward_mean=0.440, reward_bound=0.282, batch=220\n",
      "6557: loss=0.260, reward_mean=0.490, reward_bound=0.296, batch=224\n",
      "6558: loss=0.257, reward_mean=0.430, reward_bound=0.314, batch=224\n",
      "6559: loss=0.264, reward_mean=0.450, reward_bound=0.349, batch=210\n",
      "6560: loss=0.269, reward_mean=0.450, reward_bound=0.266, batch=217\n",
      "6561: loss=0.268, reward_mean=0.360, reward_bound=0.249, batch=222\n",
      "6562: loss=0.266, reward_mean=0.420, reward_bound=0.292, batch=225\n",
      "6563: loss=0.266, reward_mean=0.480, reward_bound=0.314, batch=224\n",
      "6564: loss=0.269, reward_mean=0.350, reward_bound=0.254, batch=226\n",
      "6565: loss=0.264, reward_mean=0.410, reward_bound=0.331, batch=228\n",
      "6566: loss=0.267, reward_mean=0.470, reward_bound=0.353, batch=229\n",
      "6567: loss=0.268, reward_mean=0.380, reward_bound=0.387, batch=210\n",
      "6568: loss=0.261, reward_mean=0.420, reward_bound=0.146, batch=217\n",
      "6569: loss=0.265, reward_mean=0.440, reward_bound=0.282, batch=220\n",
      "6570: loss=0.264, reward_mean=0.480, reward_bound=0.349, batch=223\n",
      "6571: loss=0.263, reward_mean=0.440, reward_bound=0.345, batch=226\n",
      "6572: loss=0.263, reward_mean=0.360, reward_bound=0.387, batch=225\n",
      "6573: loss=0.263, reward_mean=0.460, reward_bound=0.396, batch=227\n",
      "6574: loss=0.262, reward_mean=0.510, reward_bound=0.282, batch=228\n",
      "6575: loss=0.262, reward_mean=0.420, reward_bound=0.349, batch=228\n",
      "6576: loss=0.262, reward_mean=0.400, reward_bound=0.357, batch=229\n",
      "6577: loss=0.264, reward_mean=0.470, reward_bound=0.430, batch=210\n",
      "6578: loss=0.263, reward_mean=0.420, reward_bound=0.282, batch=216\n",
      "6579: loss=0.262, reward_mean=0.410, reward_bound=0.351, batch=221\n",
      "6580: loss=0.262, reward_mean=0.440, reward_bound=0.314, batch=224\n",
      "6581: loss=0.261, reward_mean=0.470, reward_bound=0.387, batch=222\n",
      "6582: loss=0.260, reward_mean=0.400, reward_bound=0.349, batch=223\n",
      "6583: loss=0.259, reward_mean=0.460, reward_bound=0.290, batch=226\n",
      "6584: loss=0.258, reward_mean=0.450, reward_bound=0.176, batch=228\n",
      "6585: loss=0.258, reward_mean=0.510, reward_bound=0.321, batch=229\n",
      "6586: loss=0.263, reward_mean=0.400, reward_bound=0.430, batch=219\n",
      "6587: loss=0.266, reward_mean=0.460, reward_bound=0.349, batch=221\n",
      "6588: loss=0.268, reward_mean=0.460, reward_bound=0.387, batch=222\n",
      "6589: loss=0.269, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "6590: loss=0.268, reward_mean=0.460, reward_bound=0.345, batch=227\n",
      "6591: loss=0.267, reward_mean=0.490, reward_bound=0.349, batch=227\n",
      "6592: loss=0.266, reward_mean=0.470, reward_bound=0.308, batch=229\n",
      "6593: loss=0.267, reward_mean=0.530, reward_bound=0.387, batch=225\n",
      "6594: loss=0.266, reward_mean=0.450, reward_bound=0.266, batch=227\n",
      "6595: loss=0.269, reward_mean=0.410, reward_bound=0.314, batch=228\n",
      "6596: loss=0.267, reward_mean=0.390, reward_bound=0.349, batch=227\n",
      "6597: loss=0.267, reward_mean=0.390, reward_bound=0.380, batch=229\n",
      "6598: loss=0.267, reward_mean=0.500, reward_bound=0.405, batch=230\n",
      "6599: loss=0.266, reward_mean=0.460, reward_bound=0.418, batch=231\n",
      "6600: loss=0.267, reward_mean=0.520, reward_bound=0.430, batch=229\n",
      "6601: loss=0.266, reward_mean=0.380, reward_bound=0.478, batch=231\n",
      "6602: loss=0.271, reward_mean=0.440, reward_bound=0.478, batch=200\n",
      "6603: loss=0.269, reward_mean=0.450, reward_bound=0.247, batch=210\n",
      "6604: loss=0.266, reward_mean=0.440, reward_bound=0.254, batch=216\n",
      "6605: loss=0.263, reward_mean=0.400, reward_bound=0.282, batch=217\n",
      "6606: loss=0.263, reward_mean=0.420, reward_bound=0.267, batch=222\n",
      "6607: loss=0.266, reward_mean=0.440, reward_bound=0.349, batch=220\n",
      "6608: loss=0.263, reward_mean=0.450, reward_bound=0.304, batch=224\n",
      "6609: loss=0.264, reward_mean=0.400, reward_bound=0.349, batch=223\n",
      "6610: loss=0.267, reward_mean=0.500, reward_bound=0.282, batch=225\n",
      "6611: loss=0.267, reward_mean=0.410, reward_bound=0.282, batch=226\n",
      "6612: loss=0.269, reward_mean=0.470, reward_bound=0.387, batch=218\n",
      "6613: loss=0.269, reward_mean=0.420, reward_bound=0.282, batch=220\n",
      "6614: loss=0.271, reward_mean=0.420, reward_bound=0.376, batch=224\n",
      "6615: loss=0.270, reward_mean=0.410, reward_bound=0.384, batch=227\n",
      "6616: loss=0.271, reward_mean=0.570, reward_bound=0.387, batch=228\n",
      "6617: loss=0.269, reward_mean=0.460, reward_bound=0.430, batch=214\n",
      "6618: loss=0.268, reward_mean=0.480, reward_bound=0.249, batch=220\n",
      "6619: loss=0.267, reward_mean=0.480, reward_bound=0.304, batch=224\n",
      "6620: loss=0.266, reward_mean=0.420, reward_bound=0.314, batch=224\n",
      "6621: loss=0.267, reward_mean=0.420, reward_bound=0.349, batch=226\n",
      "6622: loss=0.266, reward_mean=0.420, reward_bound=0.387, batch=224\n",
      "6623: loss=0.265, reward_mean=0.380, reward_bound=0.345, batch=227\n",
      "6624: loss=0.267, reward_mean=0.480, reward_bound=0.342, batch=229\n",
      "6625: loss=0.268, reward_mean=0.320, reward_bound=0.405, batch=230\n",
      "6626: loss=0.268, reward_mean=0.390, reward_bound=0.314, batch=230\n",
      "6627: loss=0.268, reward_mean=0.410, reward_bound=0.430, batch=229\n",
      "6628: loss=0.268, reward_mean=0.530, reward_bound=0.478, batch=231\n",
      "6629: loss=0.268, reward_mean=0.540, reward_bound=0.430, batch=231\n",
      "6630: loss=0.270, reward_mean=0.440, reward_bound=0.478, batch=210\n",
      "6631: loss=0.269, reward_mean=0.410, reward_bound=0.314, batch=215\n",
      "6632: loss=0.267, reward_mean=0.470, reward_bound=0.282, batch=218\n",
      "6633: loss=0.271, reward_mean=0.440, reward_bound=0.349, batch=219\n",
      "6634: loss=0.269, reward_mean=0.420, reward_bound=0.265, batch=223\n",
      "6635: loss=0.270, reward_mean=0.500, reward_bound=0.314, batch=225\n",
      "6636: loss=0.270, reward_mean=0.470, reward_bound=0.349, batch=226\n",
      "6637: loss=0.270, reward_mean=0.430, reward_bound=0.368, batch=228\n",
      "6638: loss=0.268, reward_mean=0.420, reward_bound=0.387, batch=223\n",
      "6639: loss=0.267, reward_mean=0.450, reward_bound=0.387, batch=225\n",
      "6640: loss=0.268, reward_mean=0.390, reward_bound=0.329, batch=227\n",
      "6641: loss=0.272, reward_mean=0.510, reward_bound=0.373, batch=229\n",
      "6642: loss=0.272, reward_mean=0.410, reward_bound=0.387, batch=227\n",
      "6643: loss=0.269, reward_mean=0.430, reward_bound=0.430, batch=220\n",
      "6644: loss=0.266, reward_mean=0.430, reward_bound=0.200, batch=224\n",
      "6645: loss=0.269, reward_mean=0.430, reward_bound=0.282, batch=225\n",
      "6646: loss=0.269, reward_mean=0.520, reward_bound=0.356, batch=227\n",
      "6647: loss=0.268, reward_mean=0.480, reward_bound=0.430, batch=221\n",
      "6648: loss=0.267, reward_mean=0.390, reward_bound=0.314, batch=224\n",
      "6649: loss=0.268, reward_mean=0.490, reward_bound=0.349, batch=226\n",
      "6650: loss=0.268, reward_mean=0.510, reward_bound=0.368, batch=228\n",
      "6651: loss=0.267, reward_mean=0.410, reward_bound=0.392, batch=229\n",
      "6652: loss=0.268, reward_mean=0.370, reward_bound=0.430, batch=225\n",
      "6653: loss=0.267, reward_mean=0.490, reward_bound=0.387, batch=226\n",
      "6654: loss=0.268, reward_mean=0.420, reward_bound=0.368, batch=228\n",
      "6655: loss=0.268, reward_mean=0.350, reward_bound=0.353, batch=229\n",
      "6656: loss=0.267, reward_mean=0.410, reward_bound=0.430, batch=226\n",
      "6657: loss=0.268, reward_mean=0.480, reward_bound=0.430, batch=227\n",
      "6658: loss=0.267, reward_mean=0.440, reward_bound=0.387, batch=228\n",
      "6659: loss=0.267, reward_mean=0.410, reward_bound=0.387, batch=228\n",
      "6660: loss=0.267, reward_mean=0.480, reward_bound=0.387, batch=228\n",
      "6661: loss=0.267, reward_mean=0.440, reward_bound=0.435, batch=229\n",
      "6662: loss=0.267, reward_mean=0.480, reward_bound=0.450, batch=230\n",
      "6663: loss=0.267, reward_mean=0.500, reward_bound=0.464, batch=231\n",
      "6664: loss=0.268, reward_mean=0.470, reward_bound=0.478, batch=222\n",
      "6665: loss=0.266, reward_mean=0.490, reward_bound=0.292, batch=225\n",
      "6666: loss=0.267, reward_mean=0.460, reward_bound=0.396, batch=227\n",
      "6667: loss=0.268, reward_mean=0.460, reward_bound=0.430, batch=225\n",
      "6668: loss=0.267, reward_mean=0.480, reward_bound=0.321, batch=227\n",
      "6669: loss=0.268, reward_mean=0.500, reward_bound=0.349, batch=228\n",
      "6670: loss=0.268, reward_mean=0.480, reward_bound=0.321, batch=229\n",
      "6671: loss=0.268, reward_mean=0.420, reward_bound=0.328, batch=230\n",
      "6672: loss=0.267, reward_mean=0.510, reward_bound=0.376, batch=231\n",
      "6673: loss=0.270, reward_mean=0.430, reward_bound=0.430, batch=228\n",
      "6674: loss=0.269, reward_mean=0.450, reward_bound=0.353, batch=229\n",
      "6675: loss=0.269, reward_mean=0.470, reward_bound=0.478, batch=231\n",
      "6676: loss=0.270, reward_mean=0.440, reward_bound=0.478, batch=226\n",
      "6677: loss=0.271, reward_mean=0.360, reward_bound=0.314, batch=227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6678: loss=0.270, reward_mean=0.540, reward_bound=0.349, batch=228\n",
      "6679: loss=0.270, reward_mean=0.360, reward_bound=0.430, batch=227\n",
      "6680: loss=0.269, reward_mean=0.400, reward_bound=0.414, batch=229\n",
      "6681: loss=0.269, reward_mean=0.490, reward_bound=0.430, batch=229\n",
      "6682: loss=0.269, reward_mean=0.420, reward_bound=0.387, batch=229\n",
      "6683: loss=0.268, reward_mean=0.400, reward_bound=0.450, batch=230\n",
      "6684: loss=0.270, reward_mean=0.450, reward_bound=0.478, batch=227\n",
      "6685: loss=0.270, reward_mean=0.400, reward_bound=0.387, batch=228\n",
      "6686: loss=0.269, reward_mean=0.510, reward_bound=0.484, batch=229\n",
      "6687: loss=0.270, reward_mean=0.380, reward_bound=0.405, batch=230\n",
      "6688: loss=0.270, reward_mean=0.440, reward_bound=0.376, batch=231\n",
      "6689: loss=0.269, reward_mean=0.520, reward_bound=0.430, batch=230\n",
      "6690: loss=0.269, reward_mean=0.440, reward_bound=0.478, batch=229\n",
      "6691: loss=0.268, reward_mean=0.360, reward_bound=0.343, batch=230\n",
      "6692: loss=0.268, reward_mean=0.300, reward_bound=0.338, batch=231\n",
      "6693: loss=0.268, reward_mean=0.480, reward_bound=0.349, batch=231\n",
      "6694: loss=0.271, reward_mean=0.400, reward_bound=0.387, batch=231\n",
      "6695: loss=0.269, reward_mean=0.530, reward_bound=0.430, batch=231\n",
      "6696: loss=0.269, reward_mean=0.460, reward_bound=0.430, batch=231\n",
      "6697: loss=0.269, reward_mean=0.490, reward_bound=0.478, batch=230\n",
      "6698: loss=0.269, reward_mean=0.430, reward_bound=0.451, batch=231\n",
      "6699: loss=0.269, reward_mean=0.380, reward_bound=0.430, batch=231\n",
      "6700: loss=0.269, reward_mean=0.370, reward_bound=0.478, batch=230\n",
      "6701: loss=0.269, reward_mean=0.440, reward_bound=0.430, batch=230\n",
      "6702: loss=0.269, reward_mean=0.410, reward_bound=0.501, batch=231\n",
      "6704: loss=0.220, reward_mean=0.480, reward_bound=0.000, batch=48\n",
      "6705: loss=0.222, reward_mean=0.440, reward_bound=0.000, batch=92\n",
      "6706: loss=0.226, reward_mean=0.400, reward_bound=0.000, batch=132\n",
      "6707: loss=0.230, reward_mean=0.460, reward_bound=0.002, batch=162\n",
      "6708: loss=0.237, reward_mean=0.490, reward_bound=0.011, batch=180\n",
      "6709: loss=0.237, reward_mean=0.440, reward_bound=0.024, batch=196\n",
      "6710: loss=0.243, reward_mean=0.500, reward_bound=0.038, batch=205\n",
      "6711: loss=0.240, reward_mean=0.420, reward_bound=0.047, batch=210\n",
      "6712: loss=0.238, reward_mean=0.450, reward_bound=0.072, batch=209\n",
      "6713: loss=0.238, reward_mean=0.460, reward_bound=0.080, batch=215\n",
      "6714: loss=0.244, reward_mean=0.530, reward_bound=0.089, batch=231\n",
      "6715: loss=0.240, reward_mean=0.470, reward_bound=0.098, batch=225\n",
      "6716: loss=0.243, reward_mean=0.410, reward_bound=0.122, batch=213\n",
      "6717: loss=0.247, reward_mean=0.500, reward_bound=0.135, batch=210\n",
      "6718: loss=0.249, reward_mean=0.490, reward_bound=0.150, batch=203\n",
      "6719: loss=0.247, reward_mean=0.400, reward_bound=0.167, batch=195\n",
      "6720: loss=0.244, reward_mean=0.460, reward_bound=0.141, batch=206\n",
      "6721: loss=0.248, reward_mean=0.480, reward_bound=0.185, batch=198\n",
      "6722: loss=0.256, reward_mean=0.420, reward_bound=0.206, batch=181\n",
      "6723: loss=0.248, reward_mean=0.420, reward_bound=0.098, batch=196\n",
      "6724: loss=0.249, reward_mean=0.490, reward_bound=0.167, batch=205\n",
      "6725: loss=0.251, reward_mean=0.490, reward_bound=0.229, batch=181\n",
      "6726: loss=0.245, reward_mean=0.450, reward_bound=0.135, batch=196\n",
      "6727: loss=0.243, reward_mean=0.460, reward_bound=0.150, batch=205\n",
      "6728: loss=0.240, reward_mean=0.490, reward_bound=0.189, batch=213\n",
      "6729: loss=0.240, reward_mean=0.440, reward_bound=0.206, batch=218\n",
      "6730: loss=0.244, reward_mean=0.490, reward_bound=0.254, batch=183\n",
      "6731: loss=0.244, reward_mean=0.460, reward_bound=0.211, batch=198\n",
      "6732: loss=0.240, reward_mean=0.450, reward_bound=0.152, batch=208\n",
      "6733: loss=0.242, reward_mean=0.520, reward_bound=0.171, batch=215\n",
      "6734: loss=0.247, reward_mean=0.480, reward_bound=0.229, batch=218\n",
      "6735: loss=0.250, reward_mean=0.460, reward_bound=0.254, batch=215\n",
      "6736: loss=0.255, reward_mean=0.480, reward_bound=0.282, batch=182\n",
      "6737: loss=0.256, reward_mean=0.460, reward_bound=0.145, batch=197\n",
      "6738: loss=0.255, reward_mean=0.400, reward_bound=0.113, batch=208\n",
      "6739: loss=0.254, reward_mean=0.460, reward_bound=0.167, batch=214\n",
      "6740: loss=0.254, reward_mean=0.470, reward_bound=0.206, batch=218\n",
      "6741: loss=0.257, reward_mean=0.480, reward_bound=0.231, batch=222\n",
      "6742: loss=0.257, reward_mean=0.410, reward_bound=0.254, batch=217\n",
      "6743: loss=0.254, reward_mean=0.560, reward_bound=0.314, batch=179\n",
      "6744: loss=0.252, reward_mean=0.430, reward_bound=0.127, batch=195\n",
      "6745: loss=0.250, reward_mean=0.500, reward_bound=0.141, batch=206\n",
      "6746: loss=0.251, reward_mean=0.460, reward_bound=0.167, batch=213\n",
      "6747: loss=0.251, reward_mean=0.460, reward_bound=0.220, batch=219\n",
      "6748: loss=0.250, reward_mean=0.530, reward_bound=0.282, batch=212\n",
      "6749: loss=0.251, reward_mean=0.580, reward_bound=0.314, batch=212\n",
      "6750: loss=0.250, reward_mean=0.440, reward_bound=0.236, batch=218\n",
      "6751: loss=0.250, reward_mean=0.470, reward_bound=0.282, batch=220\n",
      "6752: loss=0.251, reward_mean=0.450, reward_bound=0.304, batch=224\n",
      "6753: loss=0.251, reward_mean=0.480, reward_bound=0.314, batch=224\n",
      "6754: loss=0.250, reward_mean=0.490, reward_bound=0.314, batch=226\n",
      "6755: loss=0.257, reward_mean=0.590, reward_bound=0.349, batch=162\n",
      "6756: loss=0.257, reward_mean=0.440, reward_bound=0.072, batch=182\n",
      "6757: loss=0.253, reward_mean=0.460, reward_bound=0.113, batch=197\n",
      "6758: loss=0.256, reward_mean=0.480, reward_bound=0.150, batch=204\n",
      "6759: loss=0.252, reward_mean=0.430, reward_bound=0.167, batch=212\n",
      "6760: loss=0.253, reward_mean=0.510, reward_bound=0.185, batch=217\n",
      "6761: loss=0.255, reward_mean=0.450, reward_bound=0.206, batch=214\n",
      "6762: loss=0.256, reward_mean=0.390, reward_bound=0.229, batch=216\n",
      "6763: loss=0.256, reward_mean=0.420, reward_bound=0.254, batch=219\n",
      "6764: loss=0.255, reward_mean=0.430, reward_bound=0.265, batch=223\n",
      "6765: loss=0.255, reward_mean=0.460, reward_bound=0.282, batch=212\n",
      "6766: loss=0.256, reward_mean=0.510, reward_bound=0.263, batch=218\n",
      "6767: loss=0.261, reward_mean=0.340, reward_bound=0.282, batch=221\n",
      "6768: loss=0.265, reward_mean=0.490, reward_bound=0.314, batch=213\n",
      "6769: loss=0.262, reward_mean=0.510, reward_bound=0.229, batch=218\n",
      "6770: loss=0.263, reward_mean=0.410, reward_bound=0.286, batch=222\n",
      "6771: loss=0.264, reward_mean=0.430, reward_bound=0.314, batch=221\n",
      "6772: loss=0.264, reward_mean=0.500, reward_bound=0.314, batch=224\n",
      "6773: loss=0.265, reward_mean=0.430, reward_bound=0.349, batch=211\n",
      "6774: loss=0.264, reward_mean=0.430, reward_bound=0.282, batch=216\n",
      "6775: loss=0.265, reward_mean=0.410, reward_bound=0.271, batch=221\n",
      "6776: loss=0.264, reward_mean=0.490, reward_bound=0.314, batch=223\n",
      "6777: loss=0.263, reward_mean=0.430, reward_bound=0.349, batch=224\n",
      "6778: loss=0.260, reward_mean=0.460, reward_bound=0.387, batch=145\n",
      "6779: loss=0.246, reward_mean=0.420, reward_bound=0.032, batch=171\n",
      "6780: loss=0.260, reward_mean=0.400, reward_bound=0.058, batch=187\n",
      "6781: loss=0.247, reward_mean=0.470, reward_bound=0.098, batch=200\n",
      "6782: loss=0.247, reward_mean=0.470, reward_bound=0.122, batch=208\n",
      "6783: loss=0.248, reward_mean=0.520, reward_bound=0.150, batch=212\n",
      "6784: loss=0.254, reward_mean=0.380, reward_bound=0.172, batch=218\n",
      "6785: loss=0.248, reward_mean=0.430, reward_bound=0.185, batch=218\n",
      "6786: loss=0.250, reward_mean=0.430, reward_bound=0.206, batch=214\n",
      "6787: loss=0.249, reward_mean=0.450, reward_bound=0.229, batch=212\n",
      "6788: loss=0.250, reward_mean=0.480, reward_bound=0.254, batch=215\n",
      "6789: loss=0.249, reward_mean=0.410, reward_bound=0.282, batch=212\n",
      "6790: loss=0.251, reward_mean=0.530, reward_bound=0.314, batch=202\n",
      "6791: loss=0.253, reward_mean=0.500, reward_bound=0.150, batch=210\n",
      "6792: loss=0.251, reward_mean=0.440, reward_bound=0.206, batch=219\n",
      "6793: loss=0.247, reward_mean=0.510, reward_bound=0.215, batch=223\n",
      "6794: loss=0.249, reward_mean=0.430, reward_bound=0.254, batch=224\n",
      "6795: loss=0.250, reward_mean=0.430, reward_bound=0.314, batch=224\n",
      "6796: loss=0.250, reward_mean=0.500, reward_bound=0.254, batch=226\n",
      "6797: loss=0.254, reward_mean=0.460, reward_bound=0.349, batch=207\n",
      "6798: loss=0.254, reward_mean=0.440, reward_bound=0.282, batch=213\n",
      "6799: loss=0.254, reward_mean=0.440, reward_bound=0.282, batch=217\n",
      "6800: loss=0.252, reward_mean=0.450, reward_bound=0.308, batch=222\n",
      "6801: loss=0.256, reward_mean=0.480, reward_bound=0.282, batch=224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6802: loss=0.255, reward_mean=0.380, reward_bound=0.314, batch=225\n",
      "6803: loss=0.254, reward_mean=0.470, reward_bound=0.282, batch=226\n",
      "6804: loss=0.253, reward_mean=0.540, reward_bound=0.349, batch=222\n",
      "6805: loss=0.254, reward_mean=0.440, reward_bound=0.324, batch=225\n",
      "6806: loss=0.252, reward_mean=0.430, reward_bound=0.356, batch=227\n",
      "6807: loss=0.254, reward_mean=0.480, reward_bound=0.380, batch=229\n",
      "6808: loss=0.257, reward_mean=0.510, reward_bound=0.387, batch=200\n",
      "6809: loss=0.255, reward_mean=0.540, reward_bound=0.314, batch=208\n",
      "6810: loss=0.253, reward_mean=0.480, reward_bound=0.314, batch=214\n",
      "6811: loss=0.252, reward_mean=0.410, reward_bound=0.311, batch=220\n",
      "6812: loss=0.253, reward_mean=0.380, reward_bound=0.314, batch=223\n",
      "6813: loss=0.255, reward_mean=0.430, reward_bound=0.349, batch=218\n",
      "6814: loss=0.257, reward_mean=0.460, reward_bound=0.387, batch=215\n",
      "6815: loss=0.256, reward_mean=0.400, reward_bound=0.289, batch=220\n",
      "6816: loss=0.257, reward_mean=0.540, reward_bound=0.254, batch=222\n",
      "6817: loss=0.256, reward_mean=0.440, reward_bound=0.292, batch=225\n",
      "6818: loss=0.255, reward_mean=0.410, reward_bound=0.289, batch=227\n",
      "6819: loss=0.258, reward_mean=0.390, reward_bound=0.349, batch=224\n",
      "6820: loss=0.257, reward_mean=0.420, reward_bound=0.387, batch=225\n",
      "6821: loss=0.257, reward_mean=0.500, reward_bound=0.387, batch=226\n",
      "6822: loss=0.257, reward_mean=0.520, reward_bound=0.387, batch=227\n",
      "6823: loss=0.256, reward_mean=0.410, reward_bound=0.342, batch=229\n",
      "6824: loss=0.256, reward_mean=0.400, reward_bound=0.381, batch=230\n",
      "6825: loss=0.256, reward_mean=0.430, reward_bound=0.406, batch=231\n",
      "6826: loss=0.250, reward_mean=0.410, reward_bound=0.430, batch=122\n",
      "6827: loss=0.212, reward_mean=0.320, reward_bound=0.000, batch=154\n",
      "6828: loss=0.232, reward_mean=0.500, reward_bound=0.020, batch=177\n",
      "6829: loss=0.226, reward_mean=0.380, reward_bound=0.037, batch=194\n",
      "6830: loss=0.230, reward_mean=0.510, reward_bound=0.071, batch=206\n",
      "6831: loss=0.232, reward_mean=0.470, reward_bound=0.080, batch=210\n",
      "6832: loss=0.233, reward_mean=0.560, reward_bound=0.122, batch=215\n",
      "6833: loss=0.236, reward_mean=0.440, reward_bound=0.150, batch=211\n",
      "6834: loss=0.235, reward_mean=0.460, reward_bound=0.185, batch=209\n",
      "6835: loss=0.237, reward_mean=0.460, reward_bound=0.164, batch=216\n",
      "6836: loss=0.237, reward_mean=0.440, reward_bound=0.196, batch=221\n",
      "6837: loss=0.236, reward_mean=0.440, reward_bound=0.206, batch=218\n",
      "6838: loss=0.242, reward_mean=0.390, reward_bound=0.229, batch=211\n",
      "6839: loss=0.242, reward_mean=0.580, reward_bound=0.254, batch=205\n",
      "6840: loss=0.241, reward_mean=0.490, reward_bound=0.206, batch=211\n",
      "6841: loss=0.239, reward_mean=0.500, reward_bound=0.229, batch=216\n",
      "6842: loss=0.238, reward_mean=0.450, reward_bound=0.282, batch=210\n",
      "6843: loss=0.235, reward_mean=0.510, reward_bound=0.247, batch=217\n",
      "6844: loss=0.235, reward_mean=0.510, reward_bound=0.277, batch=222\n",
      "6845: loss=0.236, reward_mean=0.530, reward_bound=0.263, batch=225\n",
      "6846: loss=0.238, reward_mean=0.500, reward_bound=0.282, batch=225\n",
      "6847: loss=0.238, reward_mean=0.540, reward_bound=0.314, batch=202\n",
      "6848: loss=0.238, reward_mean=0.530, reward_bound=0.263, batch=211\n",
      "6849: loss=0.238, reward_mean=0.490, reward_bound=0.282, batch=215\n",
      "6850: loss=0.238, reward_mean=0.450, reward_bound=0.289, batch=220\n",
      "6851: loss=0.240, reward_mean=0.470, reward_bound=0.338, batch=224\n",
      "6852: loss=0.245, reward_mean=0.520, reward_bound=0.349, batch=196\n",
      "6853: loss=0.244, reward_mean=0.400, reward_bound=0.122, batch=206\n",
      "6854: loss=0.245, reward_mean=0.450, reward_bound=0.196, batch=214\n",
      "6855: loss=0.249, reward_mean=0.500, reward_bound=0.183, batch=220\n",
      "6856: loss=0.243, reward_mean=0.460, reward_bound=0.229, batch=222\n",
      "6857: loss=0.243, reward_mean=0.500, reward_bound=0.282, batch=223\n",
      "6858: loss=0.242, reward_mean=0.510, reward_bound=0.335, batch=226\n",
      "6859: loss=0.242, reward_mean=0.470, reward_bound=0.349, batch=217\n",
      "6860: loss=0.243, reward_mean=0.450, reward_bound=0.249, batch=222\n",
      "6861: loss=0.243, reward_mean=0.490, reward_bound=0.349, batch=223\n",
      "6862: loss=0.243, reward_mean=0.510, reward_bound=0.387, batch=188\n",
      "6863: loss=0.237, reward_mean=0.480, reward_bound=0.169, batch=201\n",
      "6864: loss=0.239, reward_mean=0.510, reward_bound=0.206, batch=205\n",
      "6865: loss=0.235, reward_mean=0.350, reward_bound=0.167, batch=212\n",
      "6866: loss=0.238, reward_mean=0.540, reward_bound=0.254, batch=214\n",
      "6867: loss=0.240, reward_mean=0.430, reward_bound=0.280, batch=220\n",
      "6868: loss=0.241, reward_mean=0.440, reward_bound=0.282, batch=219\n",
      "6869: loss=0.241, reward_mean=0.360, reward_bound=0.314, batch=221\n",
      "6870: loss=0.240, reward_mean=0.500, reward_bound=0.349, batch=219\n",
      "6871: loss=0.239, reward_mean=0.440, reward_bound=0.265, batch=223\n",
      "6872: loss=0.239, reward_mean=0.520, reward_bound=0.254, batch=225\n",
      "6873: loss=0.239, reward_mean=0.510, reward_bound=0.289, batch=227\n",
      "6874: loss=0.240, reward_mean=0.460, reward_bound=0.349, batch=227\n",
      "6875: loss=0.239, reward_mean=0.440, reward_bound=0.380, batch=229\n",
      "6876: loss=0.240, reward_mean=0.490, reward_bound=0.364, batch=230\n",
      "6877: loss=0.241, reward_mean=0.500, reward_bound=0.387, batch=214\n",
      "6878: loss=0.241, reward_mean=0.490, reward_bound=0.254, batch=219\n",
      "6879: loss=0.239, reward_mean=0.400, reward_bound=0.328, batch=223\n",
      "6880: loss=0.238, reward_mean=0.440, reward_bound=0.311, batch=226\n",
      "6881: loss=0.238, reward_mean=0.440, reward_bound=0.298, batch=228\n",
      "6882: loss=0.242, reward_mean=0.510, reward_bound=0.387, batch=222\n",
      "6883: loss=0.244, reward_mean=0.540, reward_bound=0.430, batch=183\n",
      "6884: loss=0.242, reward_mean=0.440, reward_bound=0.185, batch=196\n",
      "6885: loss=0.242, reward_mean=0.560, reward_bound=0.229, batch=205\n",
      "6886: loss=0.239, reward_mean=0.410, reward_bound=0.170, batch=213\n",
      "6887: loss=0.242, reward_mean=0.430, reward_bound=0.254, batch=215\n",
      "6888: loss=0.242, reward_mean=0.560, reward_bound=0.260, batch=220\n",
      "6889: loss=0.242, reward_mean=0.560, reward_bound=0.282, batch=214\n",
      "6890: loss=0.244, reward_mean=0.520, reward_bound=0.314, batch=210\n",
      "6891: loss=0.246, reward_mean=0.490, reward_bound=0.240, batch=217\n",
      "6892: loss=0.245, reward_mean=0.470, reward_bound=0.314, batch=219\n",
      "6893: loss=0.249, reward_mean=0.440, reward_bound=0.278, batch=223\n",
      "6894: loss=0.248, reward_mean=0.480, reward_bound=0.301, batch=226\n",
      "6895: loss=0.244, reward_mean=0.390, reward_bound=0.314, batch=226\n",
      "6896: loss=0.241, reward_mean=0.440, reward_bound=0.349, batch=220\n",
      "6897: loss=0.240, reward_mean=0.570, reward_bound=0.304, batch=224\n",
      "6898: loss=0.240, reward_mean=0.480, reward_bound=0.345, batch=227\n",
      "6899: loss=0.240, reward_mean=0.530, reward_bound=0.387, batch=207\n",
      "6900: loss=0.238, reward_mean=0.500, reward_bound=0.314, batch=214\n",
      "6901: loss=0.237, reward_mean=0.400, reward_bound=0.183, batch=220\n",
      "6902: loss=0.238, reward_mean=0.410, reward_bound=0.247, batch=224\n",
      "6903: loss=0.237, reward_mean=0.500, reward_bound=0.314, batch=224\n",
      "6904: loss=0.240, reward_mean=0.510, reward_bound=0.349, batch=218\n",
      "6905: loss=0.240, reward_mean=0.490, reward_bound=0.349, batch=221\n",
      "6906: loss=0.240, reward_mean=0.430, reward_bound=0.229, batch=224\n",
      "6907: loss=0.240, reward_mean=0.590, reward_bound=0.387, batch=222\n",
      "6908: loss=0.241, reward_mean=0.470, reward_bound=0.292, batch=225\n",
      "6909: loss=0.243, reward_mean=0.490, reward_bound=0.430, batch=209\n",
      "6910: loss=0.244, reward_mean=0.470, reward_bound=0.282, batch=215\n",
      "6911: loss=0.244, reward_mean=0.440, reward_bound=0.210, batch=220\n",
      "6912: loss=0.242, reward_mean=0.490, reward_bound=0.314, batch=219\n",
      "6913: loss=0.242, reward_mean=0.560, reward_bound=0.364, batch=223\n",
      "6914: loss=0.240, reward_mean=0.570, reward_bound=0.335, batch=226\n",
      "6915: loss=0.240, reward_mean=0.490, reward_bound=0.301, batch=228\n",
      "6916: loss=0.243, reward_mean=0.500, reward_bound=0.387, batch=225\n",
      "6917: loss=0.242, reward_mean=0.390, reward_bound=0.430, batch=218\n",
      "6918: loss=0.243, reward_mean=0.560, reward_bound=0.257, batch=222\n",
      "6919: loss=0.243, reward_mean=0.510, reward_bound=0.282, batch=223\n",
      "6920: loss=0.243, reward_mean=0.490, reward_bound=0.314, batch=224\n",
      "6921: loss=0.242, reward_mean=0.550, reward_bound=0.349, batch=225\n",
      "6922: loss=0.241, reward_mean=0.440, reward_bound=0.387, batch=225\n",
      "6923: loss=0.240, reward_mean=0.470, reward_bound=0.365, batch=227\n",
      "6924: loss=0.242, reward_mean=0.560, reward_bound=0.430, batch=223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6925: loss=0.242, reward_mean=0.450, reward_bound=0.387, batch=223\n",
      "6926: loss=0.242, reward_mean=0.480, reward_bound=0.244, batch=226\n",
      "6927: loss=0.242, reward_mean=0.440, reward_bound=0.409, batch=228\n",
      "6928: loss=0.242, reward_mean=0.380, reward_bound=0.286, batch=229\n",
      "6929: loss=0.241, reward_mean=0.440, reward_bound=0.328, batch=230\n",
      "6930: loss=0.242, reward_mean=0.410, reward_bound=0.338, batch=231\n",
      "6931: loss=0.243, reward_mean=0.430, reward_bound=0.349, batch=230\n",
      "6932: loss=0.242, reward_mean=0.460, reward_bound=0.430, batch=228\n",
      "6933: loss=0.242, reward_mean=0.480, reward_bound=0.435, batch=229\n",
      "6934: loss=0.243, reward_mean=0.480, reward_bound=0.405, batch=230\n",
      "6935: loss=0.243, reward_mean=0.420, reward_bound=0.395, batch=231\n",
      "6936: loss=0.242, reward_mean=0.390, reward_bound=0.430, batch=229\n",
      "6937: loss=0.242, reward_mean=0.410, reward_bound=0.450, batch=230\n",
      "6938: loss=0.237, reward_mean=0.480, reward_bound=0.478, batch=84\n",
      "6939: loss=0.220, reward_mean=0.420, reward_bound=0.000, batch=126\n",
      "6940: loss=0.236, reward_mean=0.520, reward_bound=0.019, batch=158\n",
      "6941: loss=0.233, reward_mean=0.440, reward_bound=0.031, batch=178\n",
      "6942: loss=0.226, reward_mean=0.460, reward_bound=0.047, batch=197\n",
      "6943: loss=0.229, reward_mean=0.440, reward_bound=0.058, batch=209\n",
      "6944: loss=0.222, reward_mean=0.510, reward_bound=0.089, batch=209\n",
      "6945: loss=0.216, reward_mean=0.480, reward_bound=0.122, batch=210\n",
      "6946: loss=0.215, reward_mean=0.430, reward_bound=0.150, batch=210\n",
      "6947: loss=0.215, reward_mean=0.470, reward_bound=0.167, batch=212\n",
      "6948: loss=0.215, reward_mean=0.420, reward_bound=0.185, batch=204\n",
      "6949: loss=0.219, reward_mean=0.370, reward_bound=0.206, batch=201\n",
      "6950: loss=0.226, reward_mean=0.490, reward_bound=0.229, batch=187\n",
      "6951: loss=0.227, reward_mean=0.430, reward_bound=0.107, batch=201\n",
      "6952: loss=0.229, reward_mean=0.460, reward_bound=0.122, batch=210\n",
      "6953: loss=0.226, reward_mean=0.590, reward_bound=0.167, batch=215\n",
      "6954: loss=0.226, reward_mean=0.490, reward_bound=0.189, batch=220\n",
      "6955: loss=0.230, reward_mean=0.490, reward_bound=0.229, batch=222\n",
      "6956: loss=0.228, reward_mean=0.430, reward_bound=0.254, batch=204\n",
      "6957: loss=0.231, reward_mean=0.470, reward_bound=0.202, batch=213\n",
      "6958: loss=0.235, reward_mean=0.500, reward_bound=0.271, batch=219\n",
      "6959: loss=0.235, reward_mean=0.550, reward_bound=0.265, batch=223\n",
      "6960: loss=0.236, reward_mean=0.450, reward_bound=0.282, batch=196\n",
      "6961: loss=0.233, reward_mean=0.470, reward_bound=0.229, batch=205\n",
      "6962: loss=0.232, reward_mean=0.580, reward_bound=0.234, batch=213\n",
      "6963: loss=0.232, reward_mean=0.530, reward_bound=0.254, batch=216\n",
      "6964: loss=0.233, reward_mean=0.420, reward_bound=0.282, batch=220\n",
      "6965: loss=0.233, reward_mean=0.490, reward_bound=0.314, batch=195\n",
      "6966: loss=0.235, reward_mean=0.490, reward_bound=0.153, batch=206\n",
      "6967: loss=0.233, reward_mean=0.600, reward_bound=0.217, batch=214\n",
      "6968: loss=0.233, reward_mean=0.460, reward_bound=0.226, batch=220\n",
      "6969: loss=0.235, reward_mean=0.470, reward_bound=0.247, batch=224\n",
      "6970: loss=0.235, reward_mean=0.500, reward_bound=0.254, batch=225\n",
      "6971: loss=0.235, reward_mean=0.440, reward_bound=0.282, batch=225\n",
      "6972: loss=0.235, reward_mean=0.540, reward_bound=0.314, batch=219\n",
      "6973: loss=0.236, reward_mean=0.410, reward_bound=0.314, batch=222\n",
      "6974: loss=0.235, reward_mean=0.580, reward_bound=0.314, batch=224\n",
      "6975: loss=0.235, reward_mean=0.520, reward_bound=0.282, batch=226\n",
      "6976: loss=0.235, reward_mean=0.580, reward_bound=0.331, batch=228\n",
      "6977: loss=0.226, reward_mean=0.510, reward_bound=0.349, batch=188\n",
      "6978: loss=0.225, reward_mean=0.500, reward_bound=0.206, batch=200\n",
      "6979: loss=0.225, reward_mean=0.480, reward_bound=0.229, batch=209\n",
      "6980: loss=0.221, reward_mean=0.500, reward_bound=0.150, batch=215\n",
      "6981: loss=0.223, reward_mean=0.480, reward_bound=0.254, batch=217\n",
      "6982: loss=0.222, reward_mean=0.510, reward_bound=0.277, batch=222\n",
      "6983: loss=0.227, reward_mean=0.440, reward_bound=0.282, batch=221\n",
      "6984: loss=0.226, reward_mean=0.450, reward_bound=0.314, batch=222\n",
      "6985: loss=0.225, reward_mean=0.420, reward_bound=0.282, batch=224\n",
      "6986: loss=0.229, reward_mean=0.490, reward_bound=0.349, batch=218\n",
      "6987: loss=0.229, reward_mean=0.450, reward_bound=0.314, batch=220\n",
      "6988: loss=0.229, reward_mean=0.460, reward_bound=0.304, batch=224\n",
      "6989: loss=0.228, reward_mean=0.520, reward_bound=0.314, batch=225\n",
      "6990: loss=0.227, reward_mean=0.490, reward_bound=0.349, batch=225\n",
      "6991: loss=0.227, reward_mean=0.470, reward_bound=0.387, batch=175\n",
      "6992: loss=0.220, reward_mean=0.420, reward_bound=0.075, batch=192\n",
      "6993: loss=0.227, reward_mean=0.530, reward_bound=0.167, batch=203\n",
      "6994: loss=0.225, reward_mean=0.410, reward_bound=0.185, batch=211\n",
      "6995: loss=0.227, reward_mean=0.510, reward_bound=0.206, batch=217\n",
      "6996: loss=0.226, reward_mean=0.450, reward_bound=0.229, batch=221\n",
      "6997: loss=0.224, reward_mean=0.500, reward_bound=0.254, batch=222\n",
      "6998: loss=0.222, reward_mean=0.470, reward_bound=0.282, batch=216\n",
      "6999: loss=0.221, reward_mean=0.460, reward_bound=0.298, batch=221\n",
      "7000: loss=0.224, reward_mean=0.480, reward_bound=0.314, batch=216\n",
      "7001: loss=0.226, reward_mean=0.460, reward_bound=0.349, batch=210\n",
      "7002: loss=0.226, reward_mean=0.490, reward_bound=0.247, batch=217\n",
      "7003: loss=0.226, reward_mean=0.500, reward_bound=0.254, batch=219\n",
      "7004: loss=0.224, reward_mean=0.510, reward_bound=0.282, batch=220\n",
      "7005: loss=0.224, reward_mean=0.550, reward_bound=0.314, batch=223\n",
      "7006: loss=0.226, reward_mean=0.540, reward_bound=0.349, batch=218\n",
      "7007: loss=0.226, reward_mean=0.500, reward_bound=0.286, batch=222\n",
      "7008: loss=0.231, reward_mean=0.430, reward_bound=0.387, batch=207\n",
      "7009: loss=0.230, reward_mean=0.440, reward_bound=0.224, batch=215\n",
      "7010: loss=0.230, reward_mean=0.500, reward_bound=0.194, batch=220\n",
      "7011: loss=0.230, reward_mean=0.500, reward_bound=0.247, batch=224\n",
      "7012: loss=0.232, reward_mean=0.430, reward_bound=0.280, batch=227\n",
      "7013: loss=0.231, reward_mean=0.560, reward_bound=0.282, batch=228\n",
      "7014: loss=0.231, reward_mean=0.480, reward_bound=0.314, batch=228\n",
      "7015: loss=0.230, reward_mean=0.380, reward_bound=0.349, batch=226\n",
      "7016: loss=0.229, reward_mean=0.420, reward_bound=0.351, batch=228\n",
      "7017: loss=0.228, reward_mean=0.520, reward_bound=0.387, batch=223\n",
      "7018: loss=0.228, reward_mean=0.470, reward_bound=0.314, batch=224\n",
      "7019: loss=0.229, reward_mean=0.550, reward_bound=0.380, batch=227\n",
      "7020: loss=0.229, reward_mean=0.480, reward_bound=0.422, batch=229\n",
      "7021: loss=0.229, reward_mean=0.530, reward_bound=0.360, batch=230\n",
      "7022: loss=0.235, reward_mean=0.460, reward_bound=0.430, batch=152\n",
      "7023: loss=0.220, reward_mean=0.460, reward_bound=0.058, batch=175\n",
      "7024: loss=0.221, reward_mean=0.460, reward_bound=0.089, batch=195\n",
      "7025: loss=0.215, reward_mean=0.490, reward_bound=0.109, batch=207\n",
      "7026: loss=0.218, reward_mean=0.570, reward_bound=0.147, batch=215\n",
      "7027: loss=0.220, reward_mean=0.490, reward_bound=0.170, batch=220\n",
      "7028: loss=0.218, reward_mean=0.420, reward_bound=0.185, batch=222\n",
      "7029: loss=0.219, reward_mean=0.490, reward_bound=0.206, batch=233\n",
      "7030: loss=0.222, reward_mean=0.420, reward_bound=0.206, batch=230\n",
      "7031: loss=0.229, reward_mean=0.530, reward_bound=0.254, batch=216\n",
      "7032: loss=0.229, reward_mean=0.370, reward_bound=0.282, batch=209\n",
      "7033: loss=0.227, reward_mean=0.410, reward_bound=0.295, batch=216\n",
      "7034: loss=0.226, reward_mean=0.410, reward_bound=0.284, batch=221\n",
      "7035: loss=0.229, reward_mean=0.470, reward_bound=0.314, batch=207\n",
      "7036: loss=0.229, reward_mean=0.550, reward_bound=0.229, batch=214\n",
      "7037: loss=0.228, reward_mean=0.540, reward_bound=0.349, batch=203\n",
      "7038: loss=0.233, reward_mean=0.520, reward_bound=0.229, batch=211\n",
      "7039: loss=0.228, reward_mean=0.550, reward_bound=0.282, batch=214\n",
      "7040: loss=0.227, reward_mean=0.440, reward_bound=0.308, batch=220\n",
      "7041: loss=0.228, reward_mean=0.480, reward_bound=0.296, batch=224\n",
      "7042: loss=0.226, reward_mean=0.470, reward_bound=0.311, batch=227\n",
      "7043: loss=0.226, reward_mean=0.480, reward_bound=0.314, batch=224\n",
      "7044: loss=0.228, reward_mean=0.540, reward_bound=0.349, batch=217\n",
      "7045: loss=0.226, reward_mean=0.430, reward_bound=0.380, batch=222\n",
      "7046: loss=0.227, reward_mean=0.560, reward_bound=0.336, batch=225\n",
      "7047: loss=0.232, reward_mean=0.520, reward_bound=0.387, batch=198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048: loss=0.228, reward_mean=0.450, reward_bound=0.229, batch=207\n",
      "7049: loss=0.229, reward_mean=0.560, reward_bound=0.254, batch=213\n",
      "7050: loss=0.228, reward_mean=0.390, reward_bound=0.271, batch=219\n",
      "7051: loss=0.229, reward_mean=0.610, reward_bound=0.282, batch=221\n",
      "7052: loss=0.230, reward_mean=0.560, reward_bound=0.314, batch=224\n",
      "7053: loss=0.231, reward_mean=0.460, reward_bound=0.349, batch=222\n",
      "7054: loss=0.231, reward_mean=0.510, reward_bound=0.360, batch=225\n",
      "7055: loss=0.232, reward_mean=0.500, reward_bound=0.387, batch=224\n",
      "7056: loss=0.232, reward_mean=0.480, reward_bound=0.426, batch=227\n",
      "7057: loss=0.232, reward_mean=0.450, reward_bound=0.387, batch=228\n",
      "7058: loss=0.233, reward_mean=0.470, reward_bound=0.430, batch=190\n",
      "7059: loss=0.229, reward_mean=0.450, reward_bound=0.127, batch=203\n",
      "7060: loss=0.239, reward_mean=0.470, reward_bound=0.167, batch=210\n",
      "7061: loss=0.237, reward_mean=0.460, reward_bound=0.222, batch=217\n",
      "7062: loss=0.237, reward_mean=0.470, reward_bound=0.229, batch=219\n",
      "7063: loss=0.239, reward_mean=0.480, reward_bound=0.265, batch=223\n",
      "7064: loss=0.242, reward_mean=0.500, reward_bound=0.282, batch=225\n",
      "7065: loss=0.239, reward_mean=0.500, reward_bound=0.321, batch=227\n",
      "7066: loss=0.233, reward_mean=0.420, reward_bound=0.349, batch=216\n",
      "7067: loss=0.231, reward_mean=0.470, reward_bound=0.256, batch=221\n",
      "7068: loss=0.234, reward_mean=0.470, reward_bound=0.387, batch=211\n",
      "7069: loss=0.233, reward_mean=0.570, reward_bound=0.349, batch=216\n",
      "7070: loss=0.233, reward_mean=0.480, reward_bound=0.298, batch=221\n",
      "7071: loss=0.237, reward_mean=0.560, reward_bound=0.314, batch=223\n",
      "7072: loss=0.234, reward_mean=0.540, reward_bound=0.387, batch=224\n",
      "7073: loss=0.235, reward_mean=0.440, reward_bound=0.430, batch=214\n",
      "7074: loss=0.233, reward_mean=0.520, reward_bound=0.280, batch=220\n",
      "7075: loss=0.234, reward_mean=0.350, reward_bound=0.282, batch=222\n",
      "7076: loss=0.234, reward_mean=0.550, reward_bound=0.349, batch=224\n",
      "7077: loss=0.234, reward_mean=0.580, reward_bound=0.349, batch=225\n",
      "7078: loss=0.235, reward_mean=0.530, reward_bound=0.314, batch=226\n",
      "7079: loss=0.235, reward_mean=0.450, reward_bound=0.316, batch=228\n",
      "7080: loss=0.235, reward_mean=0.540, reward_bound=0.387, batch=224\n",
      "7081: loss=0.235, reward_mean=0.490, reward_bound=0.345, batch=227\n",
      "7082: loss=0.236, reward_mean=0.480, reward_bound=0.349, batch=228\n",
      "7083: loss=0.236, reward_mean=0.470, reward_bound=0.353, batch=229\n",
      "7084: loss=0.235, reward_mean=0.490, reward_bound=0.387, batch=229\n",
      "7085: loss=0.235, reward_mean=0.500, reward_bound=0.381, batch=230\n",
      "7086: loss=0.235, reward_mean=0.460, reward_bound=0.430, batch=222\n",
      "7087: loss=0.236, reward_mean=0.510, reward_bound=0.292, batch=225\n",
      "7088: loss=0.234, reward_mean=0.470, reward_bound=0.321, batch=227\n",
      "7089: loss=0.233, reward_mean=0.480, reward_bound=0.349, batch=227\n",
      "7090: loss=0.233, reward_mean=0.520, reward_bound=0.342, batch=229\n",
      "7091: loss=0.233, reward_mean=0.420, reward_bound=0.364, batch=230\n",
      "7092: loss=0.234, reward_mean=0.480, reward_bound=0.387, batch=230\n",
      "7093: loss=0.234, reward_mean=0.510, reward_bound=0.430, batch=227\n",
      "7094: loss=0.234, reward_mean=0.510, reward_bound=0.422, batch=229\n",
      "7095: loss=0.235, reward_mean=0.520, reward_bound=0.360, batch=230\n",
      "7096: loss=0.235, reward_mean=0.490, reward_bound=0.418, batch=231\n",
      "7097: loss=0.235, reward_mean=0.440, reward_bound=0.387, batch=231\n",
      "7098: loss=0.235, reward_mean=0.440, reward_bound=0.387, batch=231\n",
      "7099: loss=0.232, reward_mean=0.500, reward_bound=0.478, batch=133\n",
      "7100: loss=0.225, reward_mean=0.460, reward_bound=0.022, batch=163\n",
      "7101: loss=0.204, reward_mean=0.450, reward_bound=0.042, batch=181\n",
      "7102: loss=0.211, reward_mean=0.480, reward_bound=0.072, batch=196\n",
      "7103: loss=0.216, reward_mean=0.480, reward_bound=0.098, batch=205\n",
      "7104: loss=0.219, reward_mean=0.450, reward_bound=0.122, batch=209\n",
      "7105: loss=0.216, reward_mean=0.520, reward_bound=0.167, batch=207\n",
      "7106: loss=0.216, reward_mean=0.490, reward_bound=0.167, batch=213\n",
      "7107: loss=0.217, reward_mean=0.510, reward_bound=0.185, batch=213\n",
      "7108: loss=0.225, reward_mean=0.510, reward_bound=0.206, batch=212\n",
      "7109: loss=0.222, reward_mean=0.440, reward_bound=0.229, batch=213\n",
      "7110: loss=0.230, reward_mean=0.540, reward_bound=0.254, batch=205\n",
      "7111: loss=0.234, reward_mean=0.480, reward_bound=0.282, batch=207\n",
      "7112: loss=0.234, reward_mean=0.590, reward_bound=0.254, batch=214\n",
      "7113: loss=0.234, reward_mean=0.470, reward_bound=0.206, batch=219\n",
      "7114: loss=0.231, reward_mean=0.560, reward_bound=0.229, batch=222\n",
      "7115: loss=0.231, reward_mean=0.460, reward_bound=0.282, batch=224\n",
      "7116: loss=0.229, reward_mean=0.510, reward_bound=0.314, batch=214\n",
      "7117: loss=0.228, reward_mean=0.440, reward_bound=0.254, batch=219\n",
      "7118: loss=0.228, reward_mean=0.410, reward_bound=0.314, batch=222\n",
      "7119: loss=0.233, reward_mean=0.580, reward_bound=0.349, batch=199\n",
      "7120: loss=0.232, reward_mean=0.480, reward_bound=0.254, batch=205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d57449fb0da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfull_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mreward_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfull_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-558ccbfb6b68>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mepisode_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEpisodeStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/standard_env/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/standard_env/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TimeLimit.truncated'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(12345)\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "\n",
    "full_batch = []\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "    if not full_batch:\n",
    "        continue\n",
    "    obs_v = torch.FloatTensor(obs)\n",
    "    acts_v = torch.LongTensor(acts)\n",
    "    full_batch = full_batch[-500:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "        iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        \n",
    "    if reward_mean > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
