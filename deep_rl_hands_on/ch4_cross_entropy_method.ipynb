{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method in RL\n",
    "The cross-entropy method falls into the model-free and policy-based category\n",
    "of methods.\n",
    "\n",
    "The term \"model-free\" means that the method doesn't build a model of the\n",
    "environment or reward.\n",
    "\n",
    "Policy-based methods are directly approximating the policy of the agent, that is, what actions the agent should carry out at every step. Policy is usually represented by probability distribution over the available actions.\n",
    "\n",
    "As our cross-entropy method is policy-based, our nonlinear function (neural network) produces policy, which basically says for every observation which action the agent should take.\n",
    "\n",
    "During the agent's lifetime, its experience is present as episodes. Every episode is a sequence of observations that the agent has got from the environment, actions it has issued, and rewards for these actions. Imagine that our agent has played several such episodes. For every episode, we can calculate the total reward that the agent has claimed. This total reward shows how good this episode was for the agent.\n",
    "\n",
    "Due to randomness in the environment and the way that the agent selects actions to take, some episodes will be better than others. The core of the cross-entropy method is to throw away bad\n",
    "episodes and train on better ones.\n",
    "\n",
    "### Algorithm\n",
    "the steps of the method are as follows:\n",
    "\n",
    "1. Play ```N``` number of episodes using our current model and environment.\n",
    "2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tf.keras.backend.clear_session() # Reset TF notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neural Net\n",
    "The neural network takes in an observation and returns an action. In this case, we have a discrete number of potential actions, so the network returns a \"probability\" for each action, which a higher probability indicating that the action is more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset() # Reset environment and obtain first observation.\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v)) # Get action probabilities from observation.\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) # Action is an integer.\n",
    "        next_obs, reward, is_done, _ = env.step(action) # Perform an action and move to the next state.\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "        \n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.692, reward_mean=25.6, reward_bound=26.5\n",
      "1: loss=0.698, reward_mean=19.8, reward_bound=22.0\n",
      "2: loss=0.685, reward_mean=21.7, reward_bound=25.0\n",
      "3: loss=0.675, reward_mean=21.5, reward_bound=25.0\n",
      "4: loss=0.657, reward_mean=44.4, reward_bound=43.0\n",
      "5: loss=0.647, reward_mean=36.4, reward_bound=26.0\n",
      "6: loss=0.638, reward_mean=58.0, reward_bound=50.0\n",
      "7: loss=0.625, reward_mean=60.1, reward_bound=65.0\n",
      "8: loss=0.616, reward_mean=58.4, reward_bound=63.5\n",
      "9: loss=0.602, reward_mean=71.5, reward_bound=80.0\n",
      "10: loss=0.601, reward_mean=58.3, reward_bound=78.5\n",
      "11: loss=0.581, reward_mean=72.8, reward_bound=105.0\n",
      "12: loss=0.579, reward_mean=65.7, reward_bound=82.0\n",
      "13: loss=0.567, reward_mean=80.4, reward_bound=102.5\n",
      "14: loss=0.563, reward_mean=76.6, reward_bound=81.0\n",
      "15: loss=0.570, reward_mean=95.1, reward_bound=108.5\n",
      "16: loss=0.569, reward_mean=91.1, reward_bound=104.0\n",
      "17: loss=0.531, reward_mean=77.9, reward_bound=85.5\n",
      "18: loss=0.538, reward_mean=89.0, reward_bound=95.5\n",
      "19: loss=0.529, reward_mean=92.8, reward_bound=93.5\n",
      "20: loss=0.544, reward_mean=85.6, reward_bound=94.5\n",
      "21: loss=0.543, reward_mean=87.2, reward_bound=98.5\n",
      "22: loss=0.554, reward_mean=94.6, reward_bound=109.0\n",
      "23: loss=0.513, reward_mean=83.5, reward_bound=88.0\n",
      "24: loss=0.503, reward_mean=127.5, reward_bound=147.0\n",
      "25: loss=0.524, reward_mean=110.2, reward_bound=117.5\n",
      "26: loss=0.517, reward_mean=132.5, reward_bound=156.0\n",
      "27: loss=0.521, reward_mean=173.1, reward_bound=200.0\n",
      "28: loss=0.502, reward_mean=173.1, reward_bound=200.0\n",
      "29: loss=0.505, reward_mean=178.2, reward_bound=200.0\n",
      "30: loss=0.518, reward_mean=187.5, reward_bound=200.0\n",
      "31: loss=0.499, reward_mean=176.7, reward_bound=200.0\n",
      "32: loss=0.513, reward_mean=191.3, reward_bound=200.0\n",
      "33: loss=0.508, reward_mean=193.5, reward_bound=200.0\n",
      "34: loss=0.504, reward_mean=191.5, reward_bound=200.0\n",
      "35: loss=0.500, reward_mean=192.5, reward_bound=200.0\n",
      "36: loss=0.499, reward_mean=190.4, reward_bound=200.0\n",
      "37: loss=0.490, reward_mean=181.6, reward_bound=200.0\n",
      "38: loss=0.492, reward_mean=192.8, reward_bound=200.0\n",
      "39: loss=0.499, reward_mean=188.9, reward_bound=200.0\n",
      "40: loss=0.503, reward_mean=196.3, reward_bound=200.0\n",
      "41: loss=0.492, reward_mean=199.9, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs_size = env.observation_space.shape[0] # Number of possible observations.\n",
    "n_actions = env.action_space.n # Number of possible agent actions.\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    \n",
    "    # Train neural net.\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Solution\n",
    "This has slow convergence...possibly because TensorFlow has to build a new graph each time we fit a Keras model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_mlp():\n",
    "    \n",
    "    inputs = keras.Input(shape=(obs_size,))\n",
    "    dense = layers.Dense(256, activation='relu')(inputs)\n",
    "    outputs = layers.Dense(n_actions, activation='softmax')(dense)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches_keras(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset() # Reset environment and obtain first observation.\n",
    "    while True:\n",
    "        obs_v = np.array([obs])\n",
    "        act_probs = net.predict(obs_v).reshape(-1)\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action) # Perform an action and move to the next state.\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "        \n",
    "def filter_batch_keras(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    return train_obs, train_act, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "obs_size = env.observation_space.shape[0] # Number of possible observations.\n",
    "n_actions = env.action_space.n # Number of possible agent actions.\n",
    "net = keras_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: reward_mean=19.1, reward_bound=19.5\n",
      "1: reward_mean=22.4, reward_bound=22.5\n",
      "2: reward_mean=30.4, reward_bound=40.0\n",
      "3: reward_mean=39.6, reward_bound=48.5\n",
      "4: reward_mean=36.3, reward_bound=33.5\n",
      "5: reward_mean=31.1, reward_bound=40.0\n",
      "6: reward_mean=35.4, reward_bound=41.0\n",
      "7: reward_mean=35.9, reward_bound=52.0\n",
      "8: reward_mean=43.6, reward_bound=47.5\n",
      "9: reward_mean=49.9, reward_bound=56.5\n",
      "10: reward_mean=53.0, reward_bound=66.0\n",
      "11: reward_mean=50.3, reward_bound=65.5\n",
      "12: reward_mean=65.3, reward_bound=80.5\n",
      "13: reward_mean=93.8, reward_bound=111.0\n",
      "14: reward_mean=74.8, reward_bound=77.5\n",
      "15: reward_mean=76.2, reward_bound=87.0\n",
      "16: reward_mean=111.7, reward_bound=123.5\n",
      "17: reward_mean=128.3, reward_bound=156.5\n",
      "18: reward_mean=97.5, reward_bound=127.0\n",
      "19: reward_mean=136.5, reward_bound=152.5\n",
      "20: reward_mean=90.6, reward_bound=125.0\n",
      "21: reward_mean=131.9, reward_bound=165.5\n",
      "22: reward_mean=159.6, reward_bound=200.0\n",
      "23: reward_mean=151.1, reward_bound=200.0\n"
     ]
    }
   ],
   "source": [
    "for iter_no, batch in enumerate(iterate_batches_keras(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch_keras(batch, PERCENTILE)\n",
    "    \n",
    "    obs_v, acts_v_binary = np.array(obs_v), to_categorical(acts_v)\n",
    "    net.fit(obs_v, acts_v_binary, batch_size = BATCH_SIZE, verbose = 0)\n",
    "    \n",
    "    print(\"%d: reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "        iter_no, reward_m, reward_b))\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
